---
title: "Introduction to trace estimation with `primate`"
bibliography: ../references.bib
---

`primate` contains a variety of methods for estimating quantities from _matrix functions_. One popular such quantity is the [trace](https://en.wikipedia.org/wiki/Trace_(linear_algebra)) of $f(A)$, for some generic $f: [a,b] \to \mathbb{R}$ defined on the spectrum of $A$:
$$ \mathrm{tr}(f(A)) \triangleq \mathrm{tr}(U f(\Lambda) U^{\intercal}), \quad \quad f : [a,b] \to \mathbb{R}$$

Many quantities of common interest can be expressed in as traces of matrix functions with the right choice of $f$. A few such function specializations include: 

$$
\begin{align*}
f &= \mathrm{id} \quad &\Longleftrightarrow& \quad &\mathrm{tr}(A) \\
f &= f^{-1} \quad &\Longleftrightarrow& \quad &\mathrm{tr}(A^{-1}) \\
f &= \log \quad &\Longleftrightarrow& \quad  &\mathrm{logdet}(A) \\
f &= \mathrm{exp} \quad &\Longleftrightarrow& \quad &\mathrm{exp}(A) \\
f &= \mathrm{sign} \quad &\Longleftrightarrow& \quad &\mathrm{rank}(A)  \\
&\vdots \quad &\hphantom{\Longleftrightarrow}& \quad & \vdots & 
\end{align*}
$$ 

In this introduction, the basics of randomized trace estimation are introduced, with a focus on how to use matrix-free algorithms to estimate traces of _matrix functions_.

<!-- One such method, often used in trace estimation, is the "stochastic Lanczos quadrature" (SLQ) method. Unfortunately, SLQ refer not to one but a host of methods in the literature; though each is typically related, they are often used in different contexts.  -->

```{python}
#| echo: false
#| output: true
from bokeh.plotting import figure, show
from bokeh.io import output_notebook
from bokeh.models import Range1d
output_notebook(hide_banner=True)
import numpy as np
np.random.seed(1234)
```

## Basics: Trace computation 

<!-- Whether used for spectral sum estimation, quadrature approximation, or many techniques 
 -->
Suppose we wanted to compute the trace of some matrix $A$. By the very definition of the trace, we have a variety of means to do it: 
$$\mathrm{tr}(A) \triangleq \sum\limits_{i=1}^n A_{ii} = \sum\limits_{i=1}^n \lambda_i = \sum\limits_{i=1}^n e_i^T A e_i $$

where, in the right-most sum, we're using $e_i$ to represent the $i$th [identity vector](https://en.wikipedia.org/wiki/Indicator_vector): 
$$ A_{ii} = e_i^T A e_i, \quad e_i = [0, \dots, 0, \underbrace{1}_{i}, 0, \dots, 0] $$

Let's see some code. First, we start with a simple (random) positive-definite matrix $A \in \mathbb{R}^{n \times n}$. 

```{python}
from primate.random import symmetric
A = symmetric(150, pd = True)
```

By default, `symmetric` normalizes $A$ such that the eigenvalues are uniformly distributed in the interval $[0, 1]$. For reference, the spectrum of $A$ looks as follows:

```{python}
#| echo: false
#| output: true
#| fig-align: center
p = figure(width=700, height=200, title="Spectrum of A")
p.toolbar_location = None
p.multi_line(np.split(np.repeat(np.linalg.eigvalsh(A), 2), 150), [(-0.10, 0.10)]*150)
p.y_range = Range1d(-0.35, 0.35)
p.yaxis.visible = False
p.title.align = 'center'
show(p)
```

Now, using numpy, we can verify all of these trace definitions are indeed equivalent:  
  <!-- 1. Sum the diagonal entries (or use numpy's built-in `.trace()`)
  2. Sum the eigenvalues
  3. Sum $\mathrm{e}_i^T A \mathrm{e}_i$ for all $i \in [1, n]$ -->

```{python}
import numpy as np
eye = lambda i: np.ravel(np.eye(1, 150, k=i))
print(f"Direct: {np.sum(A.diagonal()):.8f}")
print(f"Eigen:  {np.sum(np.linalg.eigvalsh(A)):.8f}")
print(f"Matvec: {sum([eye(i) @ A @ eye(i) for i in range(150)]):.8f}")
```

While (1) trivially yields $\mathrm{tr}(A)$ in (optimal) $O(n)$ time, there exist situations where the diagonal entries of `A` are not available---particularly in the large-scale regime. In contrast, though both (2) and (3) are less efficient, they are nonetheless viable alternatives to obtain $\mathrm{tr}(A)$.

## _Implicit_ trace estimation

The _implicit_ trace estimation problem can be stated as follows: 

> Given access to a square matrix $A \in \mathbb{R}^{n \times n}$ via its matrix-vector product operator $x \mapsto Ax$, estimate its traceÂ $\mathrm{tr}(A) = \sum\limits_{i=1}^n A_{ii}$

Note we no longer assume we have direct access to the diagonal here, leaving us with the `Eigen` or `Matvec` methods. The `Eigen` method is expensive, and though the `Matvec` method solves the problem exactly, its pretty inefficient from a computational standpoint---most of the entries of $e_i$ are zero, thus most inner product computations do not contribute to the trace estimate at all. 

One idea, attributed to [A. Girard](https://link.springer.com/article/10.1007/BF01395775), is to replace $e_i$ with random _zero-mean_ vectors, e.g. random sign vectors $v \in \{-1, +1\}^{n}$ or random normal vectors $v \sim \mathcal{N}(\mu=0, \sigma=1)$: 

$$\mathtt{tr}(A) \approx \frac{1}{n_v}\sum\limits_{i=1}^{n_v} v_i^\top A v_i $$

It was shown more formally later by [M.F. Huchinson](https://www.tandfonline.com/doi/abs/10.1080/03610918908812806) that if these random vectors $v \in \mathbb{R}^n$ satisfy $\mathbb{E}[vv^T] = I$ then this approach indeed forms an unbiased estimator of $\mathrm{tr}(A)$. To see this, its sufficient to combine the linearity of expectation, the cyclic-property of the trace function, and the aforementioned [isotropy conditions](https://en.wikipedia.org/wiki/Isotropic_position):
$$\mathtt{tr}(A) = \mathtt{tr}(A I) = \mathtt{tr}(A \mathbb{E}[v v^T]) = \mathbb{E}[\mathtt{tr}(Avv^T)] = \mathbb{E}[\mathtt{tr}(v^T A v)] = \mathbb{E}[v^T A v]$$

Naturally we expect the approximation to gradually improve as more vectors are sampled, converging as $n_v \to \infty$. Let's see if we can check this: we start by collecting a few sample quadratic forms 

```{python}
def isotropic(n):
  yield from [np.random.choice([-1,+1], size=A.shape[1]) for i in range(n)]
estimates = np.array([v @ A @ v for v in isotropic(500)])
```

Plotting both the estimator formed by averaging the first $k$ samples along with its absolute error, we get the following figures for $k = 50$ and $k = 500$, respectively: 
```{python}
#| echo: false
#| output: true
#| fig-align: center

from bokeh.models import Legend, Span
from bokeh.layouts import column, row
k = 50
sample_indices = np.arange(len(estimates)) + 1
estimator = np.cumsum(estimates) / sample_indices
error = np.abs(A.trace() - estimator)

p = figure(width=350, height=200, title=f"Trace estimates (k = {k})")
p.scatter(sample_indices[:k], estimates[:k], color="blue", size=3, legend_label="Estimates")
p.line(sample_indices[:k], estimator[:k], color="red", line_width=2.5, legend_label="Estimator")
# p.add_layout(Legend(), 'above')
p.add_layout(Span(location=A.trace(), dimension='width', line_color='black', line_dash='dotted', line_width=1.5))

q = figure(width=350, height=200, title=f"Estimator error (k = {k})")
q.line(sample_indices[:k], error[:k], color="black")

p2 = figure(width=350, height=200, title=f"Trace estimates (k = {len(estimates)})")
p2.scatter(sample_indices, estimates, color="blue", size=3, legend_label="Estimates")
p2.line(sample_indices, estimator, color="red", line_width=2.5, legend_label="Estimator")
p2.add_layout(Span(location=A.trace(), dimension='width', line_color='black', line_dash='dotted', line_width=1.5))

error = np.abs(A.trace() - estimator)
q2 = figure(width=350, height=200, title=f"Estimator error (k = {len(estimates)})")
q2.line(sample_indices, error, color="black")

p.toolbar_location = None
q.toolbar_location = None
p2.toolbar_location = None
q2.toolbar_location = None
# print(f"Trace estimate: { estimates.mean() }")

for fig in [p,p2]:
  fig.legend.label_text_font_size = '7pt'
  fig.legend.label_height = 5
  fig.legend.glyph_height = 5
  fig.legend.spacing = 1
  # p.legend.location = "above"
  fig.legend.orientation = "horizontal"
  fig.legend.background_fill_alpha = 0.5

show(row(column(p,q), column(p2,q2)))
```

Sure enough, the estimator quickly gets within an error rate of about 1-5% away from the actual trace, getting generally closer as more samples are collected. On the other hand, improvement is not gaurenteed, and securing additional precision much less than 1% becomes increasingly difficult due to the randomness and variance of the sample estimates. 

## Estimator Implementation

Rather than manually averaging samples, trace estimates can be obtained via the `hutch` function: 

```{python}
from primate.trace import hutch
print(f"Trace estimate: {hutch(A, maxiter=50)}")
```

Though its estimation is identical to averaging quadratic forms $v^T A v$ of isotropic vectors as above, `hutch` comes with a few extra features to simplify its usage. In particular, all $v \mapsto v^T A v$ evaluations are done in parallel, there are a variety of isotropic distributions and random number generators (RNGs) to choose from, and the estimator may be suppleid various convergence criteria to allow for early-stopping; see the [hutch documentation page](../reference/trace.hutch.qmd) for more details.  

<!-- 
Let's see how this fares using, let's say, $\frac{1}{4}$ the number of _matvecs_: 

```{python}
n: int = A.shape[0]
trace_estimate = 0.0
for j in range(n // 4):
  v = np.random.choice([-1, +1], size=n)
  trace_estimate += v @ A @ v
print(trace_estimate / (n // 4))
```

Not bad! Though it's fairly intuitive, why does this work?  -->

## Extending to matrix functions

While there are some real-world applications for estimating $\mathrm{tr}(A)$, in many setting the trace of a given matrix is not a very informative quantity. On the other hand, as shown in the beginning, there are many situations where we might be interested in estimating the trace of a _matrix function_ $f(A)$. It is natural to consider extending the Hutchinson estimator above with something like: 

$$ \mathrm{tr}(f(A)) \approx \frac{n}{n_v} \sum\limits_{i=1}^{n_v} v_i^T f(A) v_i $$

Of course, the remaining difficulty lies in computing quadratic forms $v \mapsto v^T f(A) v$ efficiently. The approach taken by @ubaru2017fast is to notice that sums of these quantities can transformed into a _Riemann-Stieltjes integral_:

$$ v^T f(A) v = v^T U f(\Lambda) U^T v = \sum\limits_{i}^n f(\lambda_i) \mu_i^2 = \int\limits_{a}^b f(t) \mu(t)$$

where scalars $\mu_i \in \mathbb{R}$ constitute a cumulative, piecewise constant function $\mu : \mathbb{R} \to \mathbb{R}_+$: 

$$
\mu_i = (U^T v)_i, \quad \mu(t) = \begin{cases}
0, & \text{if } t < a = \lambda_1 \\
\sum_{j=1}^{i-1} \mu_j^2, & \text{if } \lambda_{i-1} \leq t < \lambda_i, i = 2, \dots, n \\
\sum_{j=1}^n \mu_j^2, & \text{if } b = \lambda_n \leq t
\end{cases}
$$

As with any definite integral, one would ideally like to approximate its value with a [quadrature rule](https://en.wikipedia.org/wiki/Numerical_integration#Methods_for_one-dimensional_integrals). An exemplary such approximation is the $m$-point Gaussian quadrature rule: 

$$ \int\limits_{a}^b f(t) \mu(t) \approx \sum\limits_{k=1}^m \omega_k f(\eta_k) $$

with weights $\{\omega_k\}$ and nodes $\{ \eta_k\}$. On one hand, Gaussian Quadrature (GQ) is just one of many quadrature rules that could be used to approximate $v^T A v$. On the other hand, GQ is often preferred over other rules due to its _exactness_ on polynomials up to degree $2m - 1$.

Of course, both the weights and nodes of GQ rule are unknown for the integral above. This contrasts the typical quadrature setting, wherein $\omega$ is assumed uniform or is otherwise known ahead-of-time. 

A surprising and powerful fact is that both the weights $\{\omega_k\}$ and nodes $\{ \eta_k\}$ of the $m$-point GQ above are directly computable the degree $m$ matrix $T_m$ produced by the _Lanczos method_:

$$ Q_m^T A Q_m = T_m = Y \Theta Y^T \quad \Leftrightarrow \quad (\eta_i, \omega_i) = (\theta_i, \tau_i), \; \tau_i = (e_1^T y_i)^2 $$

In other words, the Ritz values $\{\theta_i\}$ of $T_m$ are exactly the nodes of GQ quadrature rule applied to $\mu(t)$, while the first components of the Ritz vectors $\tau_i$ (squared) are exactly the weights. For more information about this non-trivial fact, see @golub2009matrices.

## Stochastic Lanczos Quadrature

By using Lanczos quadrature estimates, the theory above enables the Hutchinson estimator to extend to be used as an estimator for the trace of a matrix function:

$$\mathrm{tr}(f(A)) \approx \frac{n}{n_v} \sum\limits_{i=1}^{n_v} e_1^T f(T_m) e_1  = \frac{n}{n_v} \sum\limits_{j=1}^{n_v} \left ( \sum\limits_{i=1}^m \tau_i^{(j)} f(\theta_i)^{(j)} \right )$$

Under relatively mild assumptions, if the function of interest $f: [a,b] \to \mathbb{R}$ is analytic on $[\lambda_{\text{min}}, \lambda_{\text{max}}]$, then for constants $\epsilon, \eta \in (0, 1)$ the output $\Gamma$ of the Hutchinson estimator satisfies: 

$$
\mathrm{Pr}\Bigg[ \lvert \mathrm{tr}(f(A)) - \Gamma \rvert \leq \epsilon \lvert \mathrm{tr}(f(A)) \rvert \Bigg] \geq 1 - \eta 
$$

...if the number of sample vectors $n_v$ satisfies $n_v \geq (24/\epsilon^2) \log(2/\eta)$ and the degree of the Lanczos expansion is sufficiently large. 
In other words, we can achieve an arbitrary $(1 \pm \epsilon)$-approximation of $\mathrm{tr}(f(A))$ with success probability $\eta$ using 
on the order of $\sim O(\epsilon^{-2} \log(\eta^{-1}))$ evaluations of $e_1^T f(T_m) e_1$. This probablistic guarantee is most useful when $\epsilon$ is not too small, i.e. only a relatively coarse approximation of $\mathrm{tr}(f(A))$ is needed.

## Example: Logdet computation

Back to the computation, let's see how the above theory translate to code with `primate`. 

To parameterize a Girard-Hutchinson estimator for use with matrix-functions, it suffices to simply set the `fun` argument in `hutch` to either the name of a built-in function or an arbitrary `Callable`. 

For example, to approximate the log-determinant of `A`:

```{python}
## Log-determinant
logdet_test = hutch(A, fun="log", maxiter=25)
logdet_true = np.sum(np.log(np.linalg.eigvalsh(A)))

print(f"Logdet (exact):  {logdet_true}")
print(f"Logdet (approx): {logdet_test}")
```

Here we see that using only $n / 6$ evaluations of $e_1^T f(T_m) e_1$, we get a decent approximation of `logdet(A)`. To get a better idea of the quality of the estimator, set `verbose=True`:

```{python}
est = hutch(A, fun="log", maxiter=25, seed=5, verbose=True)
```

The first line of the statement contains fixed information about the estimator, including the type of estimator (`Girard-Hutchinson`), the function applied to the spectrum (`log`), the degree of the Lanczos quadrature approximation (`20`), and the quadrature method used (`golub_welsch`). 

The second line prints the runtime information about the samples, such as the final trace estimate, its [margin of error](https://en.wikipedia.org/wiki/Margin_of_error) and [coefficient of variation](https://en.wikipedia.org/wiki/Coefficient_of_variation) (a.k.a relative std. deviation), the number of samples vectors $n_v$, and the isotropic distribution of choice ('R' for rademacher). 

Another way to get an idea of summarize the convergence behavior of the estimator is to pass the `plot=True` option. Here, we pass the same `seed` for reproducibility with the above call. 

```{python}
est = hutch(A, fun="log", maxiter=100, seed=5, plot=True)
```

Similar information as reported by the `verbose=True` flag is shown cumulatively applied to every sample. The left plot shows the sample point estimates alongside yellow bands denoting the margin of error associated with the 95% confidence interval (CI) at every sample index. The right plot show convergence information, e.g. the running coefficient of variation is shown on the top-right and an upper bound on the absolute error is derived from the CI is shown on the bottom right. 

While the margin of error is typically a valid, conservative estimate of estimators error, in some situations these bounds may not be valid as they are based solely on the [CLT](https://en.wikipedia.org/wiki/Central_limit_theorem) implications for normally distributed random variables. In particular, there is an inherent amount of error associated with the Lanczos-based quadrature approximations that is not taken into account in the error bound, which may invalidate the corresponding CI's. 

<!-- Moreover, though `primate` provides a variety of "good" random number generators for vector generation, every such generator is inherently imperfect. -->

[^1]: Technically, we could solve a system of equations via either Gaussian elimination or, when $A$ is rank-deficient, via least squares: $(A + \epsilon I)^{-1}v \simeq u \Leftrightarrow \mathrm{argmin}_{u \in \mathbb{R}^n} \lVert v - (A + \epsilon I)u \rVert^2$. Then again, this could  be just as expensive---if not more so---than just executing the Lanczos iteration!