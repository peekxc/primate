---
title: "Introduction to `primate`"
draft: true
---

`primate` contains an extensible implementation of a variety of methods for estimating quantities from matrix functions.

One such method, often used in trace estimation, is the "stochastic Lanczos quadrature" (SLQ) method. Unfortunately, SLQ refer not to one but a host of methods in the literature; though each is typically related, they are often used in different contexts. 

## Trace estimation 
<!-- Whether used for spectral sum estimation, quadrature approximation, or many techniques 
 -->
```{python}
#| echo: false
#| output: false
from bokeh.plotting import figure, show
from bokeh.io import output_notebook
output_notebook()
import numpy as np
np.random.seed(1234)
```

To illustrate the functionality of `primate`, we need some kind of matrix and some spectral quantity of interest. To sole the former, let's start with a simple (random) positive semi-definite matrix $A \in \mathbb{R}^{n \times n}$. 

```{python}
import numpy as np
from primate.random import symmetric
A = symmetric(150, psd = True)
```

By default, `symmetric`normalizes $A$ such that the eigenvalues are uniformly distributed in the interval $[0, 1]$. 

```{python}
print(np.histogram(np.linalg.eigvalsh(A)))
```

Now, the problem to solve. Suppose, for starters, we wanted to compute the trace of $A$:
$$\mathrm{tr}(A) \triangleq \sum\limits_{i=1}^n A_{ii} = \sum\limits_{i=1}^n \lambda_i $$

Obviously, there are many ways to obtain this... here's three: 
  
  1. Use the built-in `.trace()` call
  2. Sum the diagonal entries directly 
  3. Sum the eigenvalues

```{python}
print(A.trace()) 
print(np.sum(A.diagonal()))
print(np.sum(np.linalg.eigvalsh(A)))
```

Yet another way to compute $\mathrm{tr}(A)$ exactly is to multiply $A$ by a sequence of $n$ identity vectors:

$$\mathrm{tr}(A) = \sum\limits_{i=1}^n e_i^T A e_i, \quad e_i = [0, \dots, 0, \underset{i}{1}, 0, \dots, 0] $$

```{python}
n = A.shape[0]
trace = 0.0 
for k in range(n): 
  v = np.eye(1, n, k=k).T
  trace += np.take(v.T @ A @ v, 0)
print(trace)
```

Putting aside the fact that each $v \mapsto Av$ takes $O(n^2)$ here, observe this last approach is pretty inefficient in general as most of the entries of $v$ are zero. One idea, accredited first to A. Girard and then studied by M.F. Huchinson, is to use random sign vector $v \in \{-1, +1\}^{n}$. 

$$\mathtt{tr}(A) = \mathbb{E}[v^T A v] \approx \frac{1}{n_v}\sum\limits_{i=1}^{n_v} v_i^\top A v_i = n_v^{-1} \cdot V^\top A V$$

Let's see how this fares using, let's say, $\frac{1}{2}$ the number of _matvecs_: 

```{python}
trace_estimate = 0.0
for j in range(n // 4):
  v = np.random.choice([-1, +1], size=n)
  trace_estimate += v @ A @ v
print(trace_estimate / (n // 4))
```

Not bad! 

## The implicit trace estimation problem

> Given access to a square matrix $A \in \mathbb{R}^{n \times n}$ via its matrix-vector product operator $x \mapsto Ax$, estimate its traceÂ $\mathrm{tr}(A) = \sum\limits_{i=1}^n A_{ii}$




<!-- ```{python}
c = np.reciprocal(np.sqrt(n))
trace_estimates = np.empty(n)
for j in range(n):
  v = np.random.choice([-1, +1], size=n)
  trace_estimates[j] = v @ A @ v


est_index = np.arange(1, (n*10)+1)
p = figure(width=200, height=150)
p.line(est_index, (np.cumsum(trace_estimates) / est_index))
p.line(est_index, np.repeat(A.trace(), n*10), color='red')
show(p)
``` -->

<!-- ```{python}
from primate.trace import sl_trace
trace_estimate = sl_trace(A)
print(trace_estimate)
``` -->
<!-- 
```{python}
# tr_est = np.mean(estimates)
# print(f"Error: {abs(tr_est - A.trace()):.5}")
# print(f"Samples std. deviation: {estimates.std(ddof=1)}")
# print(f"Estimator standard error: {estimates.std(ddof=1)/np.sqrt(len(estimates))}")
``` -->
