---
title: "Introduction to trace estimation with `primate`"
---

`primate` contains an extensible implementations of a variety of methods for estimating quantities from matrix functions. One popular application of such estimators is the approximation of certain [trace](https://en.wikipedia.org/wiki/Trace_(linear_algebra)) quantities.

<!-- One such method, often used in trace estimation, is the "stochastic Lanczos quadrature" (SLQ) method. Unfortunately, SLQ refer not to one but a host of methods in the literature; though each is typically related, they are often used in different contexts.  -->

```{python}
#| echo: false
#| output: true
from bokeh.plotting import figure, show
from bokeh.io import output_notebook
from bokeh.models import Range1d
output_notebook(hide_banner=True)
import numpy as np
np.random.seed(1234)
```

In the examples below, I'll use a simple (random) positive-definite matrix $A \in \mathbb{R}^{n \times n}$. 

```{python}
from primate.random import symmetric
A = symmetric(150, pd = True)
```

By default, `symmetric` normalizes $A$ such that the eigenvalues are uniformly distributed in the interval $[0, 1]$. For reference, the spectrum of $A$ looks as follows:

```{python}
#| echo: false
#| output: true
#| fig-align: center
p = figure(width=700, height=20 0, title="Spectrum of A")
p.toolbar_location = None
p.multi_line(np.split(np.repeat(np.linalg.eigvalsh(A), 2), 150), [(-0.10, 0.10)]*150)
p.y_range = Range1d(-0.35, 0.35)
p.yaxis.visible = False
p.title.align = 'center'
show(p)
```

## Basics: Trace estimation 

<!-- Whether used for spectral sum estimation, quadrature approximation, or many techniques 
 -->
Suppose we wanted to compute the trace of $A$. From its definition, we have a variety of means to do it: 
$$\mathrm{tr}(A) \triangleq \sum\limits_{i=1}^n A_{ii} = \sum\limits_{i=1}^n \lambda_i = \sum\limits_{i=1}^n e_i^T A e_i $$

where, in the right-most sum, we're using $e_i$ to represent the $i$th [identity vector](https://en.wikipedia.org/wiki/Indicator_vector): 
$$ A_{ii} = e_i^T A e_i, \quad e_i = [0, \dots, 0, \underbrace{1}_{i}, 0, \dots, 0] $$

Using numpy, we can verify all of these give the exact trace of $A$:  
  <!-- 1. Sum the diagonal entries (or use numpy's built-in `.trace()`)
  2. Sum the eigenvalues
  3. Sum $\mathrm{e}_i^T A \mathrm{e}_i$ for all $i \in [1, n]$ -->

```{python}
import numpy as np
eye = lambda i: np.ravel(np.eye(1, 150, k=i))
print(f"Direct: {np.sum(A.diagonal()):.8f}")
print(f"Eigen:  {np.sum(np.linalg.eigvalsh(A)):.8f}")
print(f"Matvec: {sum([eye(i) @ A @ eye(i) for i in range(150)]):.8f}")
```

While (1) trivially yields $\mathrm{tr}(A)$ in (optimal) $O(n)$ time, there exist situations where the diagonal entries of `A` are not available---particularly in the large-scale regime. In contrast, though both (2) and (3) are less efficient, they yield alternative ways nonetheless of obtaining $\mathrm{tr}(A)$.

## The implicit trace estimation problem

The _implicit_ trace estimation problem can be stated as follows: 

> Given access to a square matrix $A \in \mathbb{R}^{n \times n}$ via its matrix-vector product operator $x \mapsto Ax$, estimate its traceÂ $\mathrm{tr}(A) = \sum\limits_{i=1}^n A_{ii}$

Putting aside the fact that each $v \mapsto Av$ takes $O(n^2)$ here, observe this last approach is pretty inefficient in general as most of the entries of $v$ are zero. The zero components in `v` still yield inner product computations, despite not contributing to the trace estimate at all. One idea, accredited first to A. Girard and then studied more in-depth by M.F. Huchinson, is to use random sign vector $v \in \{-1, +1\}^{n}$. 

$$\mathtt{tr}(A) = \mathbb{E}[v^T A v] \approx \frac{1}{n_v}\sum\limits_{i=1}^{n_v} v_i^\top A v_i = n_v^{-1} \cdot V^\top A V$$

Let's see how this fares using, let's say, $\frac{1}{4}$ the number of _matvecs_: 

```{python}
n: int = A.shape[0]
trace_estimate = 0.0
for j in range(n // 4):
  v = np.random.choice([-1, +1], size=n)
  trace_estimate += v @ A @ v
print(trace_estimate / (n // 4))
```

Not bad! 

## The statistical side 


This forms an unbiased estimator of $\mathrm{tr}(A)$; to see this, its sufficient to combine the linearity of expectation, the cyclic-property of the trace function, and the aforementioned isotropy conditions
$$\mathtt{tr}(A) = \mathtt{tr}(A I) = \mathtt{tr}(A \mathbb{E}[v v^T]) = \mathbb{E}[\mathtt{tr}(Avv^T)] = \mathbb{E}[\mathtt{tr}(v^T A v)] = \mathbb{E}[v^T A v]$$

# Matrix functions 


## Ex: Logdet computation

The basic way to approximate the trace of a matrix function is to simply set `fun` to the name the spectral function. For example, to approximate the log-determinant:

```{python}
from primate.trace import hutch

## Log-determinant
logdet_test = hutch(A, fun="log", maxiter=25)
logdet_true = np.sum(np.log(np.linalg.eigvalsh(A)))

print(f"Logdet (exact):  {logdet_true}")
print(f"Logdet (approx): {logdet_test}")
```

Even using $n / 6$ matvecs, we get a decent approximation. But how good is this estimate? 

To get a slightly better idea, you can set `verbose=True`:

```{python}
est = hutch(A, fun="log", maxiter=25, verbose=True)
```

The first line of the statement contains fixed about the estimator:

1. The type of estimator (Girard-Hutchinson)
2. The spectral function being approximated (log)
2. The degree of the polynomial approximation (20)
3. The quadrature method used ('fttr')

The second line prints the runtime information about the samples:

1. The final trace estimate 
2. Its [margin of error](https://en.wikipedia.org/wiki/Margin_of_error) 
2. The [coefficient of variation](https://en.wikipedia.org/wiki/Coefficient_of_variation) (aka the relative std. deviation)
3. The number of samples used + their distribution prefix ('R' for rademacher)

```{python}
est = hutch(A, fun="log", maxiter=100, plot=True)
```


<!-- ```{python}
import timeit 
## Based on https://www.mathworks.com/matlabcentral/fileexchange/8548-toeplitzmult
from scipy.linalg import toeplitz
# toeplitz([1,2,3], [1,4,5,6])
T = toeplitz(c)
# x = np.array([0,1,2,3])
T @ c

hutch(A, maxiter=20, deg=5, fun="log")

timeit.timeit(lambda: hutch(A, maxiter=20, deg=5, fun="log", quad="fttr"), number = 1000)
timeit.timeit(lambda: np.sum(np.log(np.linalg.eigvalsh(A))), number = 1000)

``` -->


<!-- ```{python}



M.quad(np.ones(M.shape[0]))
``` -->

<!-- ```{python}
c = np.reciprocal(np.sqrt(n))
trace_estimates = np.empty(n)
for j in range(n):
  v = np.random.choice([-1, +1], size=n)
  trace_estimates[j] = v @ A @ v


est_index = np.arange(1, (n*10)+1)
p = figure(width=200, height=150)
p.line(est_index, (np.cumsum(trace_estimates) / est_index))
p.line(est_index, np.repeat(A.trace(), n*10), color='red')
show(p)
``` -->

<!-- ```{python}
from primate.trace import sl_trace
trace_estimate = sl_trace(A)
print(trace_estimate)
``` -->
<!-- 
```{python}
# tr_est = np.mean(estimates)
# print(f"Error: {abs(tr_est - A.trace()):.5}")
# print(f"Samples std. deviation: {estimates.std(ddof=1)}")
# print(f"Estimator standard error: {estimates.std(ddof=1)/np.sqrt(len(estimates))}")
``` -->
