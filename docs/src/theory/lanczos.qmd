---
title: "The Lanczos method"
editor: 
    rendor-on-save: true
bibliography: ../references.bib
---

Whether for simplifying the representation of complicated systems, characterizing the asymptotic behavior of differential equations, or even just fitting polynomials to data via least-squares, decomposing linear operators has had significant use in many areas of sciences and engineering. 

Central to operator theory is the [spectral theorem](https://en.wikipedia.org/wiki/Spectral_theorem), which provides conditions under which a linear operator $A : \mathbb{R}^n \to \mathbb{R}^n$ can be _diagonalized_ in terms of its eigenvalues and eigenvectors:
$$ A = U \Lambda U^{-1} $$ 

In the case where $A$ is symmetric, the eigen-decomposition is not only guarenteed to exist, but its [canonical form](https://en.wikipedia.org/wiki/Canonical_form) may be obtained via _orthogonal diagonalization_. Such matrices are among the most commonly encountered matrices in applications.
<!-- i.e. by orthogonal $U$ and diagonal $\Lambda$ -->
<!-- the spectral theorem guarentees _orthogonally diagonalizability_, i.e. $U$ is orthogonal and its spectrum $\Lambda(A)$ is real-valued. Though this may seem like a "special case", symmetric matrices are an important class of matrices, as  -->
<!-- it is an important class of matrices as the decomposition is guarenteed to exist and to be canonical such matrices are among the most commonly encountered matrices in applications. -->

In 1950, Cornelius Lanczos studied an alternative means of decomposition via *tridiagonalization*:
$$  AQ = Q T \quad \Leftrightarrow \quad Q^T A Q = T $$
The algorithm by which one produces such a $T$ is known as the *Lanczos method*.
Despite its age, it remains the standard algorithm[^1] both for computing eigensets and solving linear systems in the large-scale regime. Having intrinsic connections to the _conjugate gradient_ method, the _theory of orthogonal polynomials_, and _Gaussian quadrature_, it is one of the most important numerical methods of all time---indeed, an [IEEE guest editorial](https://www.computer.org/csdl/magazine/cs/2000/01/c1022/13rRUxBJhBm) places it among the **top 10 most influential algorithms of the 20th century**.

As the Lanczos method lies at the heart of `primate`'s design, this post introduces it, with a focus on its motivating principles and computational details. For its API usage, see the [lanczos](../reference/diagonalize.lanczos.qmd) page.

<!-- `primate` implements Paige's[@paige] A1 variant of the _Lanczos method_, which is at the heart of most of the packages functionality. In this post, I'll describe the basic theory behind the Lanczos method, with a focus on its time and space complexity.  -->
<!-- In general, the Lanczos method is intrinsically connected to other mathematical constructions, e.g. orthogonal polynomials, gaussian quadrature, conjugate gradient---none of these will be discussed for now.  -->

## Lanczos on a bumper sticker

Given any non-zero $v \in \mathbb{R}^n$, Lanczos generates a _Krylov subspace_ via successive powers of $A$:

$$
\mathcal{K}(A, v) \triangleq \{ \, A^{0} v, A^{1}v, A^{2}v, \dots, A^{n}v \, \}
$$

These vectors are [independent](https://en.wikipedia.org/wiki/Linear_independence), so orthogonalizing them not only yields an orthonormal basis for $\mathcal{K}(A, v)$ but also a _change-of-basis_ matrix $Q$, allowing $A$ to be represented by a new matrix $T$:

$$ 
\begin{align*}
K &= [\, v \mid Av \mid A^2 v \mid \dots \mid A^{n-1}v \,] && \\
Q &= [\, q_1, q_2, \dots, q \,] \gets \mathrm{qr}(K) &&  \\
T &= Q^T A Q &&
\end{align*}
$$

It turns out that since $A$ is symmetric, $T$ is guaranteed to have a _symmetric tridiagonal structure_:

$$
T = \mathrm{tridiag}\Bigg(
\begin{array}{ccccccccc} 
& \beta_2 & & \beta_3 & & \cdot & & \beta_n & \\
\alpha_1 & & \alpha_2 & & \cdot & & \cdot & & \alpha_n \\
& \beta_2 & & \beta_3 & & \cdot & & \beta_n &
\end{array}
\Bigg)
$$

<!-- That's pretty fortunate, because computing the eigen-sets of _any_ tridiagonal matrix $T$ takes just $O(n^2)$ time[^2]! -->
<!-- $Q$ is $A$-invariant[^5] ( -->
Since $\mathrm{range}(Q) = \mathcal{K}(A, v)$, the change-of-basis $A \mapsto Q^{-1} A Q$ is in fact a [similarity transform](https://en.wikipedia.org/wiki/Matrix_similarity), which are known to be equivalence relations on $\mathcal{S}^n$---thus we can obtain $\Lambda$ by _diagonalizing_ $T$:

$$ T = Y \Lambda Y^T $$
<!-- $$ T = Y \Theta Y^T, \mathrm{diag}(Y) = (\theta_1, \theta_2, \dots, \theta_n)$$ -->

As $T$ is quite structured, it can be easily diagonalized, thus we have effectively solved the eigenvalue problem. 
To quote the [Lanczos introduction from Parlett](https://apps.dtic.mil/sti/tr/pdf/ADA289614.pdf), _could anything be more simple?_

## The "iteration" part

Lanczos originally referred to his algorithm as the _method of minimized iterations_, and indeed nowadays it is often called an _iterative_ method. Where's the iterative component?

If you squint hard enough, you can deduce that for every $j \in [1, n)$: 
$$A Q_j = Q_j T_j + \beta_{j+1} q_{j+1} e_{j}^T$$
Equating the $j$-th columns on each side of the equation and rearranging yields a *three-term recurrence*: 
$$ 
\begin{align*}
A q_j &= \beta_{j\text{-}1} q_{j\text{-}1} + \alpha_j q_j + \beta_j q_{j+1} \\
\Leftrightarrow \beta_{j} \, q_{j+1} &= A q_j - \alpha_j \, q_j - \beta_{j\text{-}1} \, q_{j\text{-}1}  
\end{align*}
$$

The equation above is a variable-coefficient second-order linear difference equation, and it is known such equations have unique solutions; they are given below: 
$$
\alpha_j = q_j^T A q_j, \;\; \beta_j = \lVert r_j \rVert_2, \;\; q_{j+1} = r_j / \beta_j
$$

$$
\text{where  } r_j = (A - \alpha_j I)q_j - \beta_{j\text{-}1} q_j
$$

In other words, if ($q_{j\text{-}1}, \beta_j, q_j$) are known, then ($\alpha_j$, $\beta_{j+1}, q_{j+1}$) are completely determined. In theory, this means we can _iteratively_ generate both $Q$ and $T$ using just a couple vectors at a time---no need to explicitly call to the QR algorithm as shown above. Pretty nifty, eh!

## Wait, isn't $T$ arbitrary? 

Unfortunately---and unlike the spectral decomposition[^6]---there is no canonical choice of $T$. Indeed, as $T$ is a family with $n - 1$ degrees of freedom and $v \in \mathbb{R}^n$ was chosen arbitrarily, there are infinitely many _essentially distinct_ such decompositions.

Not all hope is lost though, as it turns out that $T$ is actually fully characterized by $v$. To see this, notice that since $Q$ is an orthogonal matrix, we have: 

$$ Q Q^T = I_n = [e_1, e_2, \dots, e_n] $$

By extension, given an initial pair $(A, q_1)$ satisfying $\lVert q_1 \rVert = 1$, the following holds:

$$
K_n(A, q_1) = Q Q^T K_n(A, q_1) = Q[ \, e_1 \mid T e_1 \mid T^2 e_1 \mid \dots \mid T^{n-1} e_1 \, ]
$$

...this is actually a _QR_ factorization, which is [essentially unique](https://www.math.purdue.edu/~kkloste/cs515fa14/qr-uniqueness.pdf)! Indeed, the Implicit Q Theorem asserts that if an upper Hessenburg matrix $T \in \mathbb{R}^{n \times n}$ has only positive elements on its first subdiagonal and there exists an orthogonal matrix $Q$ such that $Q^T A Q = T$, then $Q$ and $T$ are _uniquely determined_[^6] by $(A, q_1)$. 

<!-- Moreover, so long as $T$ has non-zero subdiagonal entries, its the eigenvalues must be distinct -->

## Beating the complexity bounds

Elegant and as theoretically founded as the Lanczos method may be, is it efficient in practice? 

Let's start by establishing a baseline on its complexity: 

::: {#thm-line style="background-color: #efefef;"}

## Parlett 1994

Given a symmetric rank-$r$ matrix $A \in \mathbb{R}^{n \times n}$ whose operator $x \mapsto A x$ requires $O(\eta)$ time and $O(\nu)$ space, the Lanczos method computes $\Lambda(A)$ in $O(\max\{\eta, n\}\cdot r)$ time and $O(\max\{\nu, n\})$ space, when computation is done in exact arithmetic

:::

As its clear from the theorem, if we specialize it such that $r = n$ and $\eta = \nu = n$, then the Lanczos method requires just $O(n^2)$ time and $O(n)$ space to execute. In other words, the Lanczos method drops _both_ the time and space complexity[^4] of obtaining spectral information by **order of magnitude** over similar eigen-algorithms that decompose $A$ directly.

To see why this is true, note that a symmetric tridiagonal matrix is fully characterized by its diagonal and subdiagonal terms, which requires just $O(n)$ space. If we assume that $v \mapsto Av \sim O(n)$, then carrying out the recurrence clearly takes at most $O(n^2)$ time, since there are most $n$ such vectors $\{q_i\}_{i=1}^n$ to generate! 

Now, if we need to store all of $Y$ or $Q$ explicitly, we clearly need $O(n^2)$ space to do so. However, if we only need the eigen-_values_ $\Lambda(A)$ (and not their eigen-vectors $U$), then we may execute the recurrence keeping at most three vectors $\{q_{j-1}, q_{j}, q_{j+1}\}$ in memory at any given time. Since each of these is $O(n)$ is size, the claim of $O(n)$ space is justified!

<!-- ## ... but how do I code it up? 

{{< include slq_pseudo.qmd >}} -->


<!-- ## Rayleigh-Ritz approximations

Suppose instead of constructing the full $T \in \mathbb{R}^{n \times n}$, we stop at the $j^{\text{th}}$ iteration, where $1 \leq j < n$. 

$$
T_j = \mathrm{tridiag}\Bigg(
\begin{array}{ccccccccc} 
& \beta_2 & & \beta_3 & & \cdot & & \beta_j & \\
\alpha_1 & & \alpha_2 & & \cdot & & \cdot & & \alpha_j \\
& \beta_2 & & \beta_3 & & \cdot & & \beta_j &
\end{array}
\Bigg)
$$

It is natural to assume that the eigenvalues of $T_j$ approximate some of eigenvalues of $\Lambda(A)$. This intuition not only turns out to be true, but in fact the eigenvalues of $T_j$ are known to be _optimal_ approximations of $\Lambda(A)$ under many appealing notions of optimality. Thus, if we need only to approximate the spectrum of $A$, we may potentially do so using just $j << n$ iterations; indeed, this is the hallmark of the _iterative_ approach to obtaining eigenvalues: 


## Finite-precision

Though elegant as the Lanczos method is, the complexity statements and much of the theory holds only in exact arith

## Pseudocode 

There are several ways to implement the Lanczos method, some of which are "better" than others. Below is pseudocode equivalent to Paige's A27 variant, which has been shown to have a variety of attractive properties. 


There are many extensions that modify the Lanczos method to make it more robust, more computationally efficient, etc., though many of these have non-trivial implications on the space and time complexities. 
 -->



<!-- 
## The Lanczos Iteration

The Lanczos method exposes the spectrum of $A$ by successively projecting onto *Krylov subspaces*. That is, given a symmetric $A \in \mathbb{R}^{n \times n}$ with eigenvalues $\lambda_1 \geq \lambda_2 > \dots \geq \lambda_r > 0$ and a vector $v \in \mathbb{R} \setminus \{0\}$, the order-$j$ Krylov subspaces / Krylov matrices of the pair $(A, v)$ are given by: 

$$
\mathcal{K}_j(A, v) := \mathrm{span}\{ v, Av, A^2 v, \dots, A^{j-1}v \}, \quad \quad K_j(A, v) = [ v \mid Av \mid A^2 v \mid \dots \mid A^{j-1}]
$$ 

Krylov subspaces arise naturally from using the minimal polynomial of $A$ to express $A^{-1}$ in terms of powers of $A$: if $A$ is nonsingular and its minimal polynomial has degree $m$, then $A^{-1}v \in K_m(A, v)$ and $K_m(A, v)$ is an invariant subspace.

The spectral theorem implies that since $A$ is symmetric, it is orthogonally diagonalizable: thus, $\Lambda(A)$ may be obtained by generating an orthonormal basis for $\mathcal{K}_n(A, v)$. To do this, the Lanczos method constructs successive QR factorizations of $K_j(A,v) = Q_j R_j$ for each $j = 1, 2, \dots, n$. Due to $A$'s symmetry and the orthogonality of $Q_j$, we have $q_k^T A q_l = q_l^T A^T q_k = 0$ for $k > l + 1$, implying $T_j = Q_j^T A Q_j$ has a tridiagonal structure: 

$$\begin{equation}
    T_j = \begin{bmatrix} 
    \alpha_1 & \beta_2 & & & \\
    \beta_2 & \alpha_2 & \beta_3 & & \\
     & \beta_3 & \alpha_3 & \ddots & \\
    & & \ddots & \ddots & \beta_{j} \\
    & & & \beta_{j} & \alpha_{j} 
    \end{bmatrix}, \; \beta_j > 0, \; j = 1, 2, \dots, n
\end{equation}
$$ 

Given an initial pair $(A, q_1)$ satisfying $\lVert q_1 \rVert = 1$, one can restrict and project $A$ to its $j$-th Krylov subspace $T_j$ via: $$
\begin{equation}
    A Q_j = Q_j T_j + \beta_{j+1} q_{j+1} e_{j}^T \quad\quad (\beta_j > 0)
\end{equation}
$$ where $Q_j = [\, q_1 \mid q_2 \mid \dots \mid q_j \,]$ is an orthonormal set of vectors mutually orthogonal to $q_{j+1}$. Equating the $j$-th columns on each side of the above and rearranging the terms yields the famed *three-term recurrence*: $$\begin{equation}
     \beta_{j} \, q_{j+1} = A q_j - \alpha_j \, q_j - \beta_{j\text{-}1} \, q_{j\text{-}1}  
\end{equation}
$$ where $\alpha_j = q_j^T A q_j$, $\beta_j = \lVert r_j \rVert_2$, $r_j = (A - \alpha_j I)q_j - \beta_{j\text{-}1} q_j$, and $q_{j+1} = r_j / \beta_j$. The equation above is a variable-coefficient second-order linear difference equation, and such equations have unique solutions: if ($q_{j\text{-}1}, \beta_j, q_j$) are known, then ($\alpha_j$, $\beta_{j+1}, q_{j+1}$) are completely determined. This sequential process which iteratively builds $T_j$ via this three-term recurrence is what is known as the *Lanczos iteration*.

## Uniqueness of $T$

Unfortunately, unlike the spectral decomposition $A = V \Lambda V^T$---which identifies a diagonalizable $A$ with its spectrum $\Lambda(A)$ up to a change of basis $A \mapsto M^{-1} A M$---there is no canonical choice of $T_j$ due to the arbitrary choice of $v$. However, there is a connection between the iterates $K_j(A,v)$ and the full tridiagonalization of $A$: if $Q^T A Q = T$ is tridiagonal and $Q= [\, q_1 \mid q_2 \mid \dots \mid q_n \,]$ is an $n \times n$ orthogonal matrix $Q Q^T = I_n = [e_1, e_2, \dots, e_n]$, then we have: $$
\begin{equation}
    K_n(A, q_1) = Q Q^T K_n(A, q_1) = Q[ \, e_1 \mid T e_1 \mid T^2 e_1 \mid \dots \mid T^{n-1} e_1 \, ]
\end{equation}
$$ *is* the QR factorization of $K_n(A, q_1)$. Thus, tridiagonalizing $A$ with respect to a unit-norm $q_1$ determines $Q$. Indeed, the Implicit Q Theorem asserts that if an upper Hessenburg matrix $T \in \mathbb{R}^{n \times n}$ has only positive elements on its first subdiagonal and there exists an orthogonal matrix $Q$ such that $Q^T A Q = T$, then $Q$ and $T$ are *uniquely* determined by $(A, q_1)$.
 -->

<!-- In some applications, the eigenvectors are not needed all at once (or at all, even). One of the main draws to the Lanczos method is its efficiency: if one can perform $v \mapsto Av$ quickly---say, in $\approx O(n)$ time---then the Lanczos method can construct $\Lambda(A)$ in _just_ [$O(n^2)$ time]{style="color: red;"} and [$O(n)$ space]{style="color: red;"}! 
Moreover, entire method is *matrix free* as the only input to the algorithm is a (fast) matrix-vector product $v \mapsto Av$: one need not store $A$ explicitly to do this for many special types of linear operators.  -->

<!-- Among the main insights of the Lanczos method is that  -->
<!-- This fact is fantastic from a computational point of view: no explicit call to the QR algorithm necessary[^3]! -->

<!-- Thus, tridiagonalizing $A$ with respect to an arbitrary $q_1 \in \mathbb{R}^n$ satisfying $\lVert q_1\rVert = 1$ _determines_ $Q$.  -->


[^1]: A variant of the Lanczos method is actually at the heart `scipy.sparse.linalg`'s default `eigsh` solver (which is a port of [ARPACK](https://en.wikipedia.org/wiki/ARPACK)). 
[^2]: tridiag ref 
[^3]: In fact, there's a strnger 
[^4]: For general $A \in \mathbb{R}^{n \times n}$, computing the spectral-decomposition is essentially bounded by the matrix-multiplication time: $\Theta(n^\omega)$ time and $\Theta(n^2)$ space, where $\omega \approx 2.37\dots$ is the matrix multiplication constant. If we exclude the Strassen model for computation, we get effectively a $\Omega(n^3)$ time and $\Omega(n^2)$ space bound.
[^5]: Recall that if $S \subseteq \mathbb{R}^n$, then $S$ is called an _invariant subspace_ of $A$ or $A$-\emph{invariant} iff $x \in A \implies Ax \in S$ for all $x \in S$
[^6]: The spectral decomposition $A = U \Lambda U^T$ identifies a diagonalizable $A$ with its spectrum $\Lambda(A)$ up to a change of basis $A \mapsto M^{-1} A M$