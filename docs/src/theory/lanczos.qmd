---
title: "The Lanczos method"
editor: 
    rendor-on-save: true
bibliography: ../references.bib
---

Whether for simplifying the representation of complicated systems, characterizing the asymptotic behavior of differential equations---or even just least-squares data-fitting---decomposing linear operators over function spaces has had significant use in many areas of sciences and engineering. 

Central to operator theory is the [spectral theorem](), which provides conditions under which a linear operator $A : \mathbb{R}^n \to \mathbb{R}^n$ can be _diagonalized_ in terms of its eigenvalues and eigenvectors:
$$ A = U \Lambda U^{-1} $$ 

When $A$ is symmetric, it is _orthogonally diagonalizable_, i.e. $U^{-1} = U^T$ and $\Lambda$ is real-valued.
Moreover, as Cornelius Lanczos showed in 1950, its spectrum can be **iteratively** revealed via *tridiagonalization*:
$$  AQ = Q T \quad \Leftrightarrow \quad Q^T A Q = T $$
Lanczos called this approach the *method of minimized iterations*---it is now known as the *Lanczos method*, in his honor.
Despite its age, the Lanczos method remains the standard algorithm[^1] both for computing eigensets and solving linear systems in the large-scale regime. Having intrinsic connections to the _conjugate gradient_ method, the _theory of orthogonal polynomials_, and _Gaussian quadrature_, it is one of the most important numerical methods of all time---indeed, an [IEEE guest editorial](https://www.computer.org/csdl/magazine/cs/2000/01/c1022/13rRUxBJhBm) places it among the **top 10 most influential algorithms of the 20th century**.

As the Lanczos method lies at the heart of `primate`'s design, this post introduces it, focusing in particular on its motivating principles and complexity details. For its API usage, see the [lanczos](../reference/diagonalize.lanczos.qmd) page.

<!-- `primate` implements Paige's[@paige] A1 variant of the _Lanczos method_, which is at the heart of most of the packages functionality. In this post, I'll describe the basic theory behind the Lanczos method, with a focus on its time and space complexity.  -->
<!-- In general, the Lanczos method is intrinsically connected to other mathematical constructions, e.g. orthogonal polynomials, gaussian quadrature, conjugate gradient---none of these will be discussed for now.  -->

## Lanczos on a bumper sticker

Given any non-zero $v \in \mathbb{R}^n$, Lanczos generates a _Krylov subspace_ via successive powers of $A$:

$$
\mathcal{K}(A, v) \triangleq \{ \, A^{0} v, A^{1}v, A^{2}v, \dots, A^{n}v \, \}
$$

Orthogonalizing these [linearly independent](https://en.wikipedia.org/wiki/Linear_independence) vectors not only yields an orthonormal basis for $\mathcal{K}(A, v)$, but also a _change-of-basis_ matrix $Q$, allowing $A$ to be represented by a new matrix $T$:

$$ 
\begin{align*}
K &= [\, v \mid Av \mid A^2 v \mid \dots \mid A^{n-1}v \,] && \\
Q &= [\, q_1, q_2, \dots, q \,] \gets \mathrm{qr}(K) &&  \\
T &= Q^T A Q &&
\end{align*}
$$

It turns out that since $A$ is symmetric, $T$ is guaranteed to have a _symmetric tridiagonal structure_:

$$
T = \mathrm{tridiag}\Bigg(
\begin{array}{ccccccccc} 
& \beta_2 & & \beta_3 & & \cdot & & \beta_n & \\
\alpha_1 & & \alpha_2 & & \cdot & & \cdot & & \alpha_n \\
& \beta_2 & & \beta_3 & & \cdot & & \beta_n &
\end{array}
\Bigg)
$$

<!-- That's pretty fortunate, because computing the eigen-sets of _any_ tridiagonal matrix $T$ takes just $O(n^2)$ time[^2]! -->
<!-- $Q$ is $A$-invariant[^5] ( -->
Now, since $\mathrm{range}(Q) = \mathcal{K}(A, v)$, the transform $A \mapsto Q^{-1} A Q$ is a _similarity transform_. [Similarity](https://en.wikipedia.org/wiki/Matrix_similarity) is an equivalence relation on $\mathcal{S}^n$, thus we can obtain $\Lambda$ by _orthogonally diagonalizing_ $T$:

$$ T = Y \Lambda(A) Y^T $$
<!-- $$ T = Y \Theta Y^T, \mathrm{diag}(Y) = (\theta_1, \theta_2, \dots, \theta_n)$$ -->

As $T$ is _much_ more structured and easier to work with than $A$, we have effectively solved the eigenvalue problem. 
To quote the [Lanczos introduction from Parlett](https://apps.dtic.mil/sti/tr/pdf/ADA289614.pdf), _could anything be more simple?_

## Complexities

Elegant as the Lanczos method may be, what does it net us? Well, for starters, the Lanczos method can drop the complexity of obtaining the spectral decomposition by **order of magnitude**.

::: {#thm-line style="background-color: #efefef;"}

## Parlett 1994, Simon 1984

Given a symmetric rank-$r$ matrix $A \in \mathbb{R}^{n \times n}$ whose operator $x \mapsto A x$ requires $O(\eta)$ time and $O(\nu)$ space, the Lanczos method computes $\Lambda(A)$ in $O(\max\{\eta, n\}\cdot r)$ time and $O(\max\{\nu, n\})$ space, when computation is done in exact arithmetic

:::

For general $A \in \mathbb{R}^{n \times n}$, computing the spectral-decomposition is essentially bounded by the matrix-multiplication time: $\Theta(n^\omega)$ time and $\Theta(n^2)$ space, where $\omega \approx 2.37\dots$ is the matrix multiplication constant. If we exclude the Strassen model for computation, we get effectively a $\Omega(n^3)$ time and $\Omega(n^2)$ space bound.
In contrast, when both $r = n$ and $\eta = \nu = n$, the Theorem above implies the spectral decomposition takes just $O(n^2)$ time and $O(n)$ space to obtain. Unless you know the tricks, this feels like a contradiction. So... how does the complexity argument play out?

### Solving the right problem

<!-- In some applications, the eigenvectors are not needed all at once (or at all, even). One of the main draws to the Lanczos method is its efficiency: if one can perform $v \mapsto Av$ quickly---say, in $\approx O(n)$ time---then the Lanczos method can construct $\Lambda(A)$ in _just_ [$O(n^2)$ time]{style="color: red;"} and [$O(n)$ space]{style="color: red;"}! 
Moreover, entire method is *matrix free* as the only input to the algorithm is a (fast) matrix-vector product $v \mapsto Av$: one need not store $A$ explicitly to do this for many special types of linear operators.  -->

<!-- Among the main insights of the Lanczos method is that  -->

If you squint hard enough, you can deduce that since $A Q_j = Q_j T_j + \beta_{j+1} q_{j+1} e_{j}^T$, every symmetric $A \in \mathbb{R}^{n \times n}$ expanded this way admits a *three-term recurrence*: 
$$ 
\begin{align*}
A q_j &= \beta_{j\text{-}1} q_{j\text{-}1} + \alpha_j q_j + \beta_j q_{j+1} \\
\Leftrightarrow \beta_{j} \, q_{j+1} &= A q_j - \alpha_j \, q_j - \beta_{j\text{-}1} \, q_{j\text{-}1}  
\end{align*}
$$

The equation above is a variable-coefficient second-order linear difference equation, and it is known such equations have unique solutions, which are given by: 
$$
\alpha_j = q_j^T A q_j, \;\; \beta_j = \lVert r_j \rVert_2, \;\; q_{j+1} = r_j / \beta_j
$$

$$
\text{where  } r_j = (A - \alpha_j I)q_j - \beta_{j\text{-}1} q_j
$$

In other words, if ($q_{j\text{-}1}, \beta_j, q_j$) are known, then ($\alpha_j$, $\beta_{j+1}, q_{j+1}$) are completely determined. This fact is fantastic from a computational point of view: no explicit call to the QR algorithm necessary[^3]!

Note that a symmetric tridiagonal matrix is fully characterized by its diagonal and subdiagonal terms, which requires just $O(n)$ space. If we assume that $v \mapsto Av \sim O(n)$, then the above procedure clearly takes at most $O(n^2)$ time, since there are most $n$ such vectors $\{q_i\}_{i=1}^n$ to generate! 

Moreover, if we only need the eigen-_values_ $\Lambda(A)$ ( and not their vectors $U$), then we may execute the recurrence keeping at most three vectors $\{q_{j-1}, q_{j}, q_{j+1}\}$ in memory at any given time. Since each of these is $O(n)$ is size, the claim of $O(n)$ space is justified!

### But wait, isn't $T$ arbitrary? 

Unfortunately, there is no canonical choice of $T_j$. Indeed, as $T_n$ is a family with $n - 1$ degrees of freedom and $v \in \mathbb{R}^n$ was chosen arbitrarily, there are infinitely many _essentially distinct_ such decompositions. In contrast, the spectral decomposition $A = U \Lambda U^T$ identifies a diagonalizable $A$ with its spectrum $\Lambda(A)$ up to a change of basis $A \mapsto M^{-1} A M$.

Not all hope is lost though. Notice that since $Q$ is an orthogonal matrix, thus we have: 

$$ Q Q^T = I_n = [e_1, e_2, \dots, e_n] $$

By extension, given an initial pair $(A, q_1)$ satisfying $\lVert q_1 \rVert = 1$, we have:

$$
K_n(A, q_1) = Q Q^T K_n(A, q_1) = Q[ \, e_1 \mid T e_1 \mid T^2 e_1 \mid \dots \mid T^{n-1} e_1 \, ]
$$

This is actually QR factorization! Indeed, the Implicit Q Theorem asserts that if an upper Hessenburg matrix $T \in \mathbb{R}^{n \times n}$ has only positive elements on its first subdiagonal and there exists an orthogonal matrix $Q$ such that $Q^T A Q = T$, then $Q$ and $T$ are _uniquely determined_[^6] by $(A, q_1)$. 

<!-- Thus, tridiagonalizing $A$ with respect to an arbitrary $q_1 \in \mathbb{R}^n$ satisfying $\lVert q_1\rVert = 1$ _determines_ $Q$.  -->

## Approximation

TODO

## Pseudocode 

TODO

<!-- Alright, enough of the  -->


<!-- 
## The Lanczos Iteration

The Lanczos method exposes the spectrum of $A$ by successively projecting onto *Krylov subspaces*. That is, given a symmetric $A \in \mathbb{R}^{n \times n}$ with eigenvalues $\lambda_1 \geq \lambda_2 > \dots \geq \lambda_r > 0$ and a vector $v \in \mathbb{R} \setminus \{0\}$, the order-$j$ Krylov subspaces / Krylov matrices of the pair $(A, v)$ are given by: 

$$
\mathcal{K}_j(A, v) := \mathrm{span}\{ v, Av, A^2 v, \dots, A^{j-1}v \}, \quad \quad K_j(A, v) = [ v \mid Av \mid A^2 v \mid \dots \mid A^{j-1}]
$$ 

Krylov subspaces arise naturally from using the minimal polynomial of $A$ to express $A^{-1}$ in terms of powers of $A$: if $A$ is nonsingular and its minimal polynomial has degree $m$, then $A^{-1}v \in K_m(A, v)$ and $K_m(A, v)$ is an invariant subspace.

The spectral theorem implies that since $A$ is symmetric, it is orthogonally diagonalizable: thus, $\Lambda(A)$ may be obtained by generating an orthonormal basis for $\mathcal{K}_n(A, v)$. To do this, the Lanczos method constructs successive QR factorizations of $K_j(A,v) = Q_j R_j$ for each $j = 1, 2, \dots, n$. Due to $A$'s symmetry and the orthogonality of $Q_j$, we have $q_k^T A q_l = q_l^T A^T q_k = 0$ for $k > l + 1$, implying $T_j = Q_j^T A Q_j$ has a tridiagonal structure: 

$$\begin{equation}
    T_j = \begin{bmatrix} 
    \alpha_1 & \beta_2 & & & \\
    \beta_2 & \alpha_2 & \beta_3 & & \\
     & \beta_3 & \alpha_3 & \ddots & \\
    & & \ddots & \ddots & \beta_{j} \\
    & & & \beta_{j} & \alpha_{j} 
    \end{bmatrix}, \; \beta_j > 0, \; j = 1, 2, \dots, n
\end{equation}
$$ 

Given an initial pair $(A, q_1)$ satisfying $\lVert q_1 \rVert = 1$, one can restrict and project $A$ to its $j$-th Krylov subspace $T_j$ via: $$
\begin{equation}
    A Q_j = Q_j T_j + \beta_{j+1} q_{j+1} e_{j}^T \quad\quad (\beta_j > 0)
\end{equation}
$$ where $Q_j = [\, q_1 \mid q_2 \mid \dots \mid q_j \,]$ is an orthonormal set of vectors mutually orthogonal to $q_{j+1}$. Equating the $j$-th columns on each side of the above and rearranging the terms yields the famed *three-term recurrence*: $$\begin{equation}
     \beta_{j} \, q_{j+1} = A q_j - \alpha_j \, q_j - \beta_{j\text{-}1} \, q_{j\text{-}1}  
\end{equation}
$$ where $\alpha_j = q_j^T A q_j$, $\beta_j = \lVert r_j \rVert_2$, $r_j = (A - \alpha_j I)q_j - \beta_{j\text{-}1} q_j$, and $q_{j+1} = r_j / \beta_j$. The equation above is a variable-coefficient second-order linear difference equation, and such equations have unique solutions: if ($q_{j\text{-}1}, \beta_j, q_j$) are known, then ($\alpha_j$, $\beta_{j+1}, q_{j+1}$) are completely determined. This sequential process which iteratively builds $T_j$ via this three-term recurrence is what is known as the *Lanczos iteration*.

## Uniqueness of $T$

Unfortunately, unlike the spectral decomposition $A = V \Lambda V^T$---which identifies a diagonalizable $A$ with its spectrum $\Lambda(A)$ up to a change of basis $A \mapsto M^{-1} A M$---there is no canonical choice of $T_j$ due to the arbitrary choice of $v$. However, there is a connection between the iterates $K_j(A,v)$ and the full tridiagonalization of $A$: if $Q^T A Q = T$ is tridiagonal and $Q= [\, q_1 \mid q_2 \mid \dots \mid q_n \,]$ is an $n \times n$ orthogonal matrix $Q Q^T = I_n = [e_1, e_2, \dots, e_n]$, then we have: $$
\begin{equation}
    K_n(A, q_1) = Q Q^T K_n(A, q_1) = Q[ \, e_1 \mid T e_1 \mid T^2 e_1 \mid \dots \mid T^{n-1} e_1 \, ]
\end{equation}
$$ *is* the QR factorization of $K_n(A, q_1)$. Thus, tridiagonalizing $A$ with respect to a unit-norm $q_1$ determines $Q$. Indeed, the Implicit Q Theorem asserts that if an upper Hessenburg matrix $T \in \mathbb{R}^{n \times n}$ has only positive elements on its first subdiagonal and there exists an orthogonal matrix $Q$ such that $Q^T A Q = T$, then $Q$ and $T$ are *uniquely* determined by $(A, q_1)$.
 -->

[^1]: A variant of the Lanczos method is actually at the heart `scipy.sparse.linalg`'s default `eigsh` solver (which is a port of [ARPACK](https://en.wikipedia.org/wiki/ARPACK)). 
[^2]: tridiag ref 
[^3]: In fact, there's a strnger 
[^4]: testing
[^5]: Recall that if $S \subseteq \mathbb{R}^n$, then $S$ is called an _invariant subspace_ of $A$ or $A$-\emph{invariant} iff $x \in A \implies Ax \in S$ for all $x \in S$
[^6]: Up to a sign of the columns
