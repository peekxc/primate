{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"`primate` quickstart\"\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "`primate` is a package that contains a variety of algorithms for estimating quantities from matrix functions, with a focus \n",
        "on implicit matrix representations and support for common quantities of interest, such as the trace or the diagonal.\n",
        "\n",
        "Below is a quick introduction to: \n",
        "\n",
        "- Trace estimation\n",
        "- Diagonal estimation \n",
        "- Matrix function approximation\n",
        "- Configuring the output \n",
        "- Configuring the execution\n",
        "\n",
        "<br/>\n"
      ],
      "id": "eb8fe7a4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "from bokeh.plotting import figure, show\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "import numpy as np\n",
        "np.random.seed(1234)"
      ],
      "id": "8c240642",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trace estimation \n",
        "\n",
        "A variety of _trace_ estimators are available in the `primate.trace` module. These algorithms estimate the quantity: \n",
        "\n",
        "$$ \\mathrm{tr}(A) = \\sum\\limits_{i=1}^n A_{ii} = \\sum\\limits_{i=1}^n \\lambda_i $$\n",
        "\n",
        "The following example demonstrates a variety of trace estimators, including the Girard-Hutchinson estimator (`hutch`), the \n",
        "improved Hutch++ (`hutchpp`), and XTrace (`xtrace`):\n"
      ],
      "id": "55a2ef91"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from primate.trace import hutch, hutchpp, xtrace\n",
        "from primate.random import symmetric\n",
        "rng = np.random.default_rng(1234)      # for reproducibility \n",
        "A = symmetric(150, pd=True, seed=rng)  # random PD matrix \n",
        "\n",
        "print(f\"Trace   :  {A.trace():6f}\")   ## Actual trace\n",
        "print(f\"Hutch   :  {hutch(A):6f}\")    ## Crude Monte-Carlo estimator\n",
        "print(f\"Hutch++ :  {hutchpp(A):6f}\")  ## Monte-Carlo estimator w/ deflation \n",
        "print(f\"XTrace  :  {xtrace(A):6f}\")   ## Epperly's algorithm"
      ],
      "id": "0ab4e251",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, `A` can be a numpy matrix, a sparse matrix, or a `LinearOperator`. If the spectral sum of interest is not just the sum of the eigenvalues, but rather the sum under composition with some spectral function $f(\\lambda)$, i.e. the quantity of interest is of the form: \n",
        "\n",
        "$$ \\mathrm{tr}(f(A)) = \\sum\\limits_{i=1}^n f(A)_{ii} = \\sum\\limits_{i=1}^n f(\\lambda_i) $$\n",
        "\n",
        "Then you may alternatively use the `matrix_function` API to construct a `LinearOperator` by passing in the corresponding spectral function as a `Callable` or by passing a string representing the name of one of the following built-in spectral functions:\n",
        "\n",
        "| **name**       | **matrix function**       | **Applications**                          | **numpy call**                     |\n",
        "|----------------|---------------------------|-------------------------------------------|------------------------------------|\n",
        "| identity       | $A$                      | Basic matrix operations                   | `A`                                |\n",
        "| log            | $\\log(A)$                | Determinant, entropy-like measures        | `logm(A)`                         |\n",
        "| exp            | $e^A$                    | Dynamical systems, graph diffusion        | `expm(A)`                         |\n",
        "| inv            | $A^{-1}$                 | Stability analysis, linear systems        | `inv(A)`                          |\n",
        "| sign           | $\\text{sgn}_\\epsilon(A)$ | Rank approximation, low-rank modeling     | `U @ U.T`                         |\n",
        "| sqrt           | $A^{1/2}$                | Diffusion, kernel methods                 | `sqrtm(A)`                        |\n",
        "| square         | $A^2$                    | Energy measures, stability                | `A @ A`                           |\n",
        "| PCA            | $P_A$ (eigenspace)       | Dim. reduction, feature extraction        | Custom projection matrix          |\n",
        "| Tikhonov       | $(A + \\lambda I)^{-1}$   | Regularized inversion, stability          | `inv(A + lambda * eye(n))`        |\n",
        "| heat           | $e^{-tA}$                | Diffusion on graphs, spectral clustering  | `expm(-t * A)`                    |\n",
        "| pagerank       | $(I - \\alpha A)^{-1}v$   | Network centrality, web ranking           | Iterative solver                  |\n",
        "\n",
        "<br/>\n",
        "\n",
        "For example, one might compute the log-determinant of a positive definite matrix as follows:\n"
      ],
      "id": "51f6c504"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from primate.operators import matrix_function, IdentityOperator\n",
        "M = matrix_function(A, fun=\"log\") # or fun=np.log \n",
        "\n",
        "print(f\"logdet(A) :  {np.log(np.linalg.det(A)):6f}\")\n",
        "print(f\"tr(log(A)):  {np.sum(np.log(np.linalg.eigvalsh(A))):6f}\")\n",
        "print(f\"Hutch     :  {hutch(M):6f}\")\n",
        "print(f\"Hutch++   :  {hutchpp(M):6f}\")\n",
        "print(f\"XTrace    :  {xtrace(M):6f}\")"
      ],
      "id": "db334f74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagonal estimation \n",
        "\n",
        "The diagonals of matrices and matrix functions (implicitly or explicitly represented) can also be estimated via nearly identical API used for the trace.  \n"
      ],
      "id": "948d1f68"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from primate.estimators import arr_summary\n",
        "from primate.diagonal import diag, xdiag\n",
        "\n",
        "d1 = A.diagonal()\n",
        "d2 = diag(A, rtol=1e-4)\n",
        "d3 = xdiag(A)\n",
        "\n",
        "print(f\"Diagonal (true): {arr_summary(d1)}\")\n",
        "print(f\"Diagonal Hutch : {arr_summary(d2)}\")\n",
        "print(f\"Diagonal XDiag : {arr_summary(d3)}\")"
      ],
      "id": "ab942a19",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matrix function approximation \n",
        "\n",
        "In `primate`, the matrix function $f(A)$ is not constructed explicitly but instead the action $v \\mapsto f(A)v$ is approximated with a fixed-degree Krylov expansion. This can be useful when, for example, the matrix $A$ itself is so large that the corresponding (typically dense) matrix function $f(A) \\in \\mathbb{R}^{n \\times n}$ simply is too large to be explicitly represented. If you just want to approximate the action of a matrix function for a single vector $v \\in \\mathbb{R}^n$, simply supply the vector and the matrix alongside the `matrix_function` call: \n"
      ],
      "id": "1fcf40d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from primate.operators import matrix_function\n",
        "v = np.random.uniform(size=A.shape[0])\n",
        "y = matrix_function(A, fun=np.exp, v=v)\n",
        "print(f\"f(A)v = {arr_summary(y.ravel())}\")"
      ],
      "id": "36def99d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, if you prefer an object-oriented approach (or you plan on doing multiple matvecs), you can construct a `MatrixFunction` instance and use it like any other `LinearOperator`: \n"
      ],
      "id": "c55b9315"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from primate.operators import MatrixFunction\n",
        "ExpA = MatrixFunction(A, fun=np.exp)\n",
        "y = ExpA @ v\n",
        "print(f\"exp(A)v = {arr_summary(y)}\")"
      ],
      "id": "4b7e83e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you don't supply a vector `v` to the `matrix_function` call, a `MatrixFunction` instance is constructed using whatever additional arguments are passed in and returned. Note some function specializations are inherently more difficult to approximate and can depend on the smoothness of $f$ and the conditioning of the corresponding operator $f(A)$; in general, a `MatrixFunction` instance with degree $k$ approximates the action $v \\mapsto f(A)v$ about as well as the operator $p(A)$, where $p$ is a degree $2k-1$ polynomial interpolant of $f$. \n"
      ],
      "id": "c71d74d1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.linalg import expm\n",
        "ExpA = expm(A)\n",
        "ExpA0 = MatrixFunction(A, fun=np.exp, deg=5, orth=0)\n",
        "ExpA1 = MatrixFunction(A, fun=np.exp, deg=20, orth=0)\n",
        "ExpA2 = MatrixFunction(A, fun=np.exp, deg=50, orth=50)\n",
        "\n",
        "w = ExpA @ v\n",
        "x = ExpA0 @ v\n",
        "y = ExpA1 @ v \n",
        "z = ExpA2 @ v\n",
        "\n",
        "print(f\"Deg-5 approx error  (no reorth.)   : {np.linalg.norm(w - x)}\")\n",
        "print(f\"Deg-20 approx error (no reorth.)   : {np.linalg.norm(w - y)}\")\n",
        "print(f\"Deg-50 approx error (full reorth.) : {np.linalg.norm(w - z)}\")"
      ],
      "id": "d10ed5c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, for smoother matrix functions (like $\\mathrm{exp}(A)$), even a low degree Krylov expansion can be more than sufficient for many application purposes---all without any re-orthogonalization! See the matrix function guide for more background on this. \n",
        "\n",
        "## Configuring the output\n",
        "\n",
        "<!-- By default, the various estimators offered by `primate` simply return the estimated quantity under reasonable default parameter settings. However, in many applications one would like to have greater control over both the computation itself and the type of information collected during execution.  -->\n",
        "\n",
        "Passing `full=True` returns additional information about the computation in the form of `EstimatorResult` (along with the estimate itself), which contains information about execution itself, convergence information of the estimator, and other status messages. \n",
        "\n",
        "For example, with the default `converge=\"confidene\"` criterion, the margin of error of a default-constructed confidence interval is returned: \n"
      ],
      "id": "fa9ae07d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "est, info = hutch(A, converge=\"confidence\", full=True)\n",
        "print(info.message)"
      ],
      "id": "ffc402ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A more visual way of viewing the sample values and the corresponding estimate as a function of the sample size is to plot the sequence with \n",
        "the `figure_sequence` function (note this requires saving the samples with `record=True`):\n"
      ],
      "id": "a8dd9d44"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from primate.plotting import figure_sequence\n",
        "\n",
        "est, info = hutch(A, full=True, record=True)\n",
        "p = figure_sequence(info.estimator.values)\n",
        "show(p)"
      ],
      "id": "79235301",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also pass a callback function, which receives as its only argument an `EstimatorResult` instance. \n",
        "This can be useful for quickly monitoring convergence information. \n"
      ],
      "id": "78e594e5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def hutch_callback(result: EstimatorResult):\n",
        "\tif (result.nit % 150) == 0:\n",
        "\t\tprint(result.criterion.message(result.estimator))\n",
        "\n",
        "est, info = hutch(A, converge=\"confidence\", callback=hutch_callback)"
      ],
      "id": "10fbdb79",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- For example, consider applying the `sign` function at some tolerance level $\\epsilon$:\n",
        "\n",
        "$$ \\mathrm{sign}_\\epsilon(x) = \\begin{align} 1 & x \\geq \\epsilon \\\\ 0 & \\text{otherwise} \\end{align} $$\n",
        "\n",
        "The spectral sum defined by applying this function corresponds to the numerical rank of the operator, which is an interesting and sometimes useful quantity to estimate. However, the sign function itself is discontinuous (at $\\epsilon$) and otherwise not smooth; thus, it cannot be well approximated by any polynomial of finite degree. Non-analytic functions such as these can prove difficult to approximate, as shown below:\n",
        "\n",
        "\n",
        "ew = np.sort(np.random.uniform(size=150, low=0, high=1))\n",
        "ew[:30] = 0.0\n",
        "A = symmetric(150, ew = ew, pd = False)\n",
        "M = matrix_function(A, fun=\"numrank\")\n",
        "\n",
        "print(f\"numrank(A): {np.linalg.matrix_rank(A)}\")\n",
        "print(f\"GR approx:  {hutch(M)}\")\n",
        "print(f\"Hutch++ :   {hutchpp(M)}\")\n",
        "print(f\"XTrace:     {xtrace(M)}\")\n",
        "``` -->\n",
        "<!-- from timeit import timeit\n",
        "timeforlan = timeit(lambda: ExpA @ v, number=100)/100\n",
        "\n",
        "timeforexpmA = timeit(lambda: expm(A), number=100)/100\n",
        "B = expm(A)\n",
        "timeforexpmAv = timeit(lambda: B @ v, number=100)/100\n",
        "\n",
        "sample_index = 1 + np.arange(30)\n",
        "timeforsp = timeforexpmA + timeforexpmAv * sample_index\n",
        "timeformf = timeforlan * sample_index\n",
        "\n",
        "p = figure(width=350, height=200)\n",
        "\n",
        "p.line(sample_index, timeforsp, color='red')\n",
        "p.scatter(sample_index, timeforsp, color='red',  size=1)\n",
        "p.line(sample_index, timeformf, color='blue')\n",
        "p.scatter(sample_index, timeformf, color='blue', size=1)\n",
        "show(p) -->"
      ],
      "id": "b6891bba"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}