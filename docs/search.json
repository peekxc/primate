[
  {
    "objectID": "reference/get_include.html",
    "href": "reference/get_include.html",
    "title": "get_include",
    "section": "",
    "text": "get_include\nget_include()\nReturn the directory that contains the primate’s *.h header files.\nExtension modules that need to compile against primate should use this function to locate the appropriate include directory.\nNotes: When using distutils, for example in setup.py: python     import primate     ...     Extension('extension_name', ..., include_dirs=[primate.get_include()])     ... Or with meson-python, for example in meson.build: meson     ...     run_command(py, ['-c', 'import primate; print(primate.get_include())', check : true).stdout().strip()     ..."
  },
  {
    "objectID": "reference/trace.hutch.html",
    "href": "reference/trace.hutch.html",
    "title": "trace.hutch",
    "section": "",
    "text": "trace.hutch(A, fun=None, maxiter=200, deg=20, atol=None, rtol=None, stop=['confidence', 'change'], ncv=2, orth=0, quad='golub_welsch', confidence=0.95, pdf='rademacher', rng='pcg64', seed=-1, num_threads=0, verbose=False, info=False, plot=False, **kwargs)\nEstimates the trace of a symmetric A or matrix function f(A) via a Girard-Hutchinson estimator.\nThis function uses up to maxiter random isotropic vectors to estimate of the trace of f(A), where: \\mathrm{tr}(f(A)) = \\mathrm{tr}(U f(\\Lambda) U^T) = \\sum\\limits_{i=1}^n f(\\lambda_i)  The estimator is obtained by averaging quadratic forms v \\mapsto v^T f(A)v, rescaling as necessary. This estimator may be used to quickly approximate of a variety of quantities, such as the trace inverse, the log-determinant, the numerical rank, etc. See the online documentation for more details.\n\n\n\n\n\n\nNote\n\n\n\nConvergence behavior is controlled by the stop parameter: “confidence” uses the central limit theorem to generate confidence intervals on the fly, which may be used in conjunction with atol and rtol to upper-bound the error of the approximation. Alternatively, when stop = “change”, the estimator is considered converged when the error between the last two iterates is less than atol (or rtol, respectively), similar to the behavior of scipy.integrate.quadrature.\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nA\nndarray, sparray, or LinearOperator\nreal symmetric operator.\nrequired\n\n\nfun\nstr or typing.Callable\nreal-valued function defined on the spectrum of A.\n\"identity\"\n\n\nmaxiter\nint\nMaximum number of random vectors to sample for the trace estimate.\n10\n\n\ndeg\nint\nDegree of the quadrature approximation. Must be at least 1.\n20\n\n\natol\nfloat\nAbsolute tolerance to signal convergence for early-stopping. See notes.\nNone\n\n\nrtol\nfloat\nRelative tolerance to signal convergence for early-stopping. See notes.\n1e-2\n\n\nstop\nstr\nEarly-stopping criteria to test estimator convergence. See details.\n\"confidence\"\n\n\nncv\nint\nNumber of Lanczos vectors to allocate. Must be at least 2.\n2\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against. Must be less than ncv.\n0\n\n\nquad\nstr\nMethod used to obtain the weights of the Gaussian quadrature. See notes.\n'golub_welsch'\n\n\nconfidence\nfloat\nConfidence level to consider estimator as converged. Only used when stop = “confidence”.\n0.95\n\n\npdf\n‘rademacher’, ‘normal’\nChoice of zero-centered distribution to sample random vectors from.\n'rademacher'\n\n\nrng\n‘splitmix64’, ’xoshiro256**‘, ’pcg64’, ‘mt64’\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint\nSeed to initialize the rng entropy source. Set seed &gt; -1 for reproducibility.\n-1\n\n\nnum_threads\nint\nNumber of threads to use to parallelize the computation. Set to &lt;= 0 to let OpenMP decide.\n0\n\n\nplot\nbool\nIf true, plots the samples of the trace estimate along with their convergence characteristics.\nFalse\n\n\ninfo\nbool\nIf True, returns a dictionary containing all relevant information about the computation.\nFalse\n\n\nkwargs\ndict\nadditional key-values to parameterize the chosen function ‘fun’.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Union[float, tuple]\nEstimate the trace of f(A). If info = True, additional information about the computation is also returned.\n\n\n\n\n\n\nTo compute the weights of the quadrature, the GW computation uses implicit symmetric QR steps with Wilkinson shifts, while the FTTR algorithm uses the explicit expression for orthogonal polynomials. While both require O(\\mathrm{deg}^2) time to execute, the former requires O(\\mathrm{deg}^2) space but is highly accurate, while the latter uses only O(1) space at the cost of stability. If deg is large, fttr is preferred.\n\n\n\nlanczos : the lanczos tridiagonalization algorithm.\n\n\n\n\nUbaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099.\nHutchinson, Michael F. “A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines.” Communications in Statistics-Simulation and Computation 18.3 (1989): 1059-1076.",
    "crumbs": [
      "API Reference",
      "Hutch"
    ]
  },
  {
    "objectID": "reference/trace.hutch.html#parameters",
    "href": "reference/trace.hutch.html#parameters",
    "title": "trace.hutch",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nA\nndarray, sparray, or LinearOperator\nreal symmetric operator.\nrequired\n\n\nfun\nstr or typing.Callable\nreal-valued function defined on the spectrum of A.\n\"identity\"\n\n\nmaxiter\nint\nMaximum number of random vectors to sample for the trace estimate.\n10\n\n\ndeg\nint\nDegree of the quadrature approximation. Must be at least 1.\n20\n\n\natol\nfloat\nAbsolute tolerance to signal convergence for early-stopping. See notes.\nNone\n\n\nrtol\nfloat\nRelative tolerance to signal convergence for early-stopping. See notes.\n1e-2\n\n\nstop\nstr\nEarly-stopping criteria to test estimator convergence. See details.\n\"confidence\"\n\n\nncv\nint\nNumber of Lanczos vectors to allocate. Must be at least 2.\n2\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against. Must be less than ncv.\n0\n\n\nquad\nstr\nMethod used to obtain the weights of the Gaussian quadrature. See notes.\n'golub_welsch'\n\n\nconfidence\nfloat\nConfidence level to consider estimator as converged. Only used when stop = “confidence”.\n0.95\n\n\npdf\n‘rademacher’, ‘normal’\nChoice of zero-centered distribution to sample random vectors from.\n'rademacher'\n\n\nrng\n‘splitmix64’, ’xoshiro256**‘, ’pcg64’, ‘mt64’\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint\nSeed to initialize the rng entropy source. Set seed &gt; -1 for reproducibility.\n-1\n\n\nnum_threads\nint\nNumber of threads to use to parallelize the computation. Set to &lt;= 0 to let OpenMP decide.\n0\n\n\nplot\nbool\nIf true, plots the samples of the trace estimate along with their convergence characteristics.\nFalse\n\n\ninfo\nbool\nIf True, returns a dictionary containing all relevant information about the computation.\nFalse\n\n\nkwargs\ndict\nadditional key-values to parameterize the chosen function ‘fun’.\n{}",
    "crumbs": [
      "API Reference",
      "Hutch"
    ]
  },
  {
    "objectID": "reference/trace.hutch.html#returns",
    "href": "reference/trace.hutch.html#returns",
    "title": "trace.hutch",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\ntyping.Union[float, tuple]\nEstimate the trace of f(A). If info = True, additional information about the computation is also returned.",
    "crumbs": [
      "API Reference",
      "Hutch"
    ]
  },
  {
    "objectID": "reference/trace.hutch.html#notes",
    "href": "reference/trace.hutch.html#notes",
    "title": "trace.hutch",
    "section": "",
    "text": "To compute the weights of the quadrature, the GW computation uses implicit symmetric QR steps with Wilkinson shifts, while the FTTR algorithm uses the explicit expression for orthogonal polynomials. While both require O(\\mathrm{deg}^2) time to execute, the former requires O(\\mathrm{deg}^2) space but is highly accurate, while the latter uses only O(1) space at the cost of stability. If deg is large, fttr is preferred.",
    "crumbs": [
      "API Reference",
      "Hutch"
    ]
  },
  {
    "objectID": "reference/trace.hutch.html#see-also",
    "href": "reference/trace.hutch.html#see-also",
    "title": "trace.hutch",
    "section": "",
    "text": "lanczos : the lanczos tridiagonalization algorithm.",
    "crumbs": [
      "API Reference",
      "Hutch"
    ]
  },
  {
    "objectID": "reference/trace.hutch.html#reference",
    "href": "reference/trace.hutch.html#reference",
    "title": "trace.hutch",
    "section": "",
    "text": "Ubaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099.\nHutchinson, Michael F. “A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines.” Communications in Statistics-Simulation and Computation 18.3 (1989): 1059-1076.",
    "crumbs": [
      "API Reference",
      "Hutch"
    ]
  },
  {
    "objectID": "reference/trace.xtrace.html",
    "href": "reference/trace.xtrace.html",
    "title": "trace.xtrace",
    "section": "",
    "text": "trace.xtrace\ntrace.xtrace(A, nv='auto', pdf='sphere', atol=0.1, rtol=1e-06, cond_tol=100000000.0, verbose=0, info=False)",
    "crumbs": [
      "API Reference",
      "XTrace"
    ]
  },
  {
    "objectID": "reference/operator.matrix_function.html",
    "href": "reference/operator.matrix_function.html",
    "title": "operator.matrix_function",
    "section": "",
    "text": "operator.matrix_function(A, fun='identity', deg=20, rtol=None, orth=0, **kwargs)\nConstructs a LinearOperator approximating the action of the matrix function fun(A).\nThis function uses the Lanczos method to approximate the action of matrix function:  v \\mapsto f(A)v, \\quad f(A) = U f(\\lambda) U^T  The resulting operator may be used in conjunction with other estimation methods, such as hutch or xtrace.\nThe weights of the quadrature may be computed using either the Golub-Welsch (GW) or Forward Three Term Recurrence algorithms (FTTR) (see the quad parameter). For a description of the other parameters, see the Lanczos function.\n\n\n\n\n\n\nNote\n\n\n\nTo compute the weights of the quadrature, the GW computation uses implicit symmetric QR steps with Wilkinson shifts, while the FTTR algorithm uses the explicit expression for orthogonal polynomials. While both require O(\\mathrm{deg}^2) time to execute, the former requires O(\\mathrm{deg}^2) space but is highly accurate, while the latter uses only O(1) space at the cost of stability. If deg is large, fttr is preferred.\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nA\nscipy.sparse.linalg.LinearOperator or ndarray or sparray\nreal symmetric operator.\nrequired\n\n\nfun\nstr or typing.Callable\nreal-valued function defined on the spectrum of A.\n\"identity\"\n\n\ndeg\nint\nDegree of the Krylov expansion.\n20\n\n\nrtol\nfloat\nRelative tolerance to consider two Lanczos vectors are numerically orthogonal.\n1e-8\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against when building the Krylov basis.\n0\n\n\nkwargs\ndict\nadditional key-values to parameterize the chosen function ‘fun’.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nscipy.sparse.linalg.LinearOperator\na LinearOperator approximating the action of fun on the spectrum of A",
    "crumbs": [
      "API Reference",
      "Matrix Function"
    ]
  },
  {
    "objectID": "reference/operator.matrix_function.html#parameters",
    "href": "reference/operator.matrix_function.html#parameters",
    "title": "operator.matrix_function",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nA\nscipy.sparse.linalg.LinearOperator or ndarray or sparray\nreal symmetric operator.\nrequired\n\n\nfun\nstr or typing.Callable\nreal-valued function defined on the spectrum of A.\n\"identity\"\n\n\ndeg\nint\nDegree of the Krylov expansion.\n20\n\n\nrtol\nfloat\nRelative tolerance to consider two Lanczos vectors are numerically orthogonal.\n1e-8\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against when building the Krylov basis.\n0\n\n\nkwargs\ndict\nadditional key-values to parameterize the chosen function ‘fun’.\n{}",
    "crumbs": [
      "API Reference",
      "Matrix Function"
    ]
  },
  {
    "objectID": "reference/operator.matrix_function.html#returns",
    "href": "reference/operator.matrix_function.html#returns",
    "title": "operator.matrix_function",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nscipy.sparse.linalg.LinearOperator\na LinearOperator approximating the action of fun on the spectrum of A",
    "crumbs": [
      "API Reference",
      "Matrix Function"
    ]
  },
  {
    "objectID": "imate_compare.html",
    "href": "imate_compare.html",
    "title": "Comparison to imate",
    "section": "",
    "text": "primate’s namesake (and some of the original code1) was inspired from the (excellent) imate package, prompting questions about their differences. In general, primate was developed with slightly different goals in mind than imate, most of which have to do with things like integrability, extensibility, and choice of FFI / build system.\nNotable differences between the two packages include:\nOne motivation for developing primate was to modularize and streamline access to Lanczos-based methods, which is achieved through the use of things like function templates, type erasure, and header-only definitions. These modifications not only simplify access from user (i.e. dependent) packages, but they enable native support for arbitrary classes adhering to the LinearOperator concept. For more details on this, see the integration guides.",
    "crumbs": [
      "Basics",
      "Comparison to *imate*"
    ]
  },
  {
    "objectID": "imate_compare.html#footnotes",
    "href": "imate_compare.html#footnotes",
    "title": "Comparison to imate",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBefore v0.2, much of primate’s code was essentially ported and refactored from imate. The code for v0.2+ has been re-written using the Eigen template C++ library.↩︎\nprimate does not provide native GPU-implemented Linear operators. However, there is nothing preventing one from using e.g. CUDA- or ROCm-based GPU-based tensor libraries to accelerate matrix-vector products. Indeed, primate was designed to work with essentially any operator matching the interface.↩︎\nSee imates documentation for the list of supported functions.↩︎",
    "crumbs": [
      "Basics",
      "Comparison to *imate*"
    ]
  },
  {
    "objectID": "basic/usage.html",
    "href": "basic/usage.html",
    "title": "primate usage - quickstart",
    "section": "",
    "text": "Below is a quick introduction to primate. For more introductory material, theor\nTo do trace estimation, use functions in the trace module:\n\nimport primate.trace as TR\nfrom primate.random import symmetric\nA = symmetric(150)  ## random positive-definite matrix \n\nprint(f\"Actual trace: {A.trace():6f}\")        ## Actual trace\nprint(f\"Girard-Hutch: {TR.hutch(A):6f}\")      ## Monte-carlo tyoe estimator\nprint(f\"XTrace:       {TR.xtrace(A):6f}\")     ## Epperly's algorithm\n\nActual trace: 75.697397\nGirard-Hutch: 75.284468\nXTrace:       75.697398\n\n\nFor matrix functions, you can either construct a LinearOperator directly via the matrix_function API, or supply a string to the parameter fun describing the spectral function to apply. For example, one might compute the log-determinant as follows:\n\nfrom primate.operator import matrix_function\nM = matrix_function(A, fun=\"log\")\n\new = np.linalg.eigvalsh(A)\nprint(f\"logdet(A):  {np.sum(np.log(ew)):6f}\")\nprint(f\"GR approx:  {TR.hutch(M):6f}\")\nprint(f\"XTrace:     {TR.xtrace(M):6f}\")\n\n## Equivalently could've used: \n## M = matrix_function(A, fun=np.log)\n\nlogdet(A):  -148.321844\nGR approx:  -148.272202\nXTrace:     -148.315937\n\n\nNote in the above example you can supply to fun either string describing a built-in spectral function or an arbitrary Callable. The former is preferred when possible, as function evaluations will generally be faster and hutch can also be parallelized. Multi-threaded execution of e.g. hutch with arbitrary functions is not currently allowed due to the GIL, though there are options available, see the integration docs for more details.\nFor ‘plain’ operators, XTrace should recover the exact trace (up to roundoff error). For matrix functions f(A), there will be some inherent inaccuracy as the underlying matrix-vector multiplication is approximated with the Lanczos method.\nIn general, the amount of accuracy depends both on the Lanczos parameters and the type of matrix function. Spectral functions that are difficult or impossible to approximate via low-degree polynomials, for example, may suffer more from inaccuracy issues than otherwise. For example, consider the example below that computes that rank:\n\n## Make a rank-deficient operator\new = np.sort(ew)\new[:30] = 0.0\nA = symmetric(150, ew = ew, pd = False)\nM = matrix_function(A, fun=np.sign)\n\nprint(f\"Rank:       {np.linalg.matrix_rank(A)}\")\nprint(f\"GR approx:  {TR.hutch(M)}\")\nprint(f\"XTrace:     {TR.xtrace(M)}\")\n\nRank:       120\nGR approx:  145.36611938476562\nXTrace:     143.97018151807674\n\n\nThis is not so much a fault of hutch or xtrace as much as it is the choice of approximation and Lanczos parameters. The sign function has a discontinuity at 0, is not smooth, and is difficult to approximate with low-degree polynomials. One workaround to handle this issue is relax the sign function with a low-degree “soft-sign” function:  \\mathcal{S}_\\lambda(x) = \\sum\\limits_{i=0}^q \\left( x(1 - x^2)^i \\prod_{j=1}^i \\frac{2j - 1}{2j} \\right)\nVisually, the soft-sign function looks like this:\n\nfrom primate.special import soft_sign, figure_fun\nshow(figure_fun(\"smoothstep\"))\n\n\n  \n\n\n\n\n\nIt’s been shown that there is a low degree polynomial p^\\ast that uniformly approximates \\mathcal{S}_\\lambda up to a small error on the interval [-1,1]. Since matrix_function uses a low degree Krylov subspace to approximate the action v \\mapsto f(A)v, one way to improve the accuracy of rank estimation is to replace \\mathrm{sign} \\mapsto \\mathcal{S}_{\\lambda} for some choice of q \\in \\mathbb{Z}_+ (this function is available in primate under the name soft_sign):\n\nfrom primate.special import soft_sign\nfor q in range(0, 50, 5):\n  M = matrix_function(A, fun=soft_sign(q=q))\n  print(f\"XTrace S(A) for q={q}: {TR.xtrace(M):6f}\")\n\nXTrace S(A) for q=0: 72.776775\nXTrace S(A) for q=5: 109.325087\nXTrace S(A) for q=10: 114.843157\nXTrace S(A) for q=15: 116.952146\nXTrace S(A) for q=20: 118.027760\nXTrace S(A) for q=25: 118.657212\nXTrace S(A) for q=30: 119.055340\nXTrace S(A) for q=35: 119.319911\nXTrace S(A) for q=40: 119.501839\nXTrace S(A) for q=45: 119.630120\n\n\nIf the type of operator A is known to typically have a large spectral gap, another option is to compute the numerical rank by thresholding values above some fixed value \\lambda_{\\text{min}}. This is equivalent to applying the following spectral function:\n S_{\\lambda_{\\text{min}}}(x) =\n\\begin{cases}\n1 & \\text{ if } x \\geq \\lambda_{\\text{min}} \\\\\n0 & \\text{ otherwise }\n\\end{cases}\n\nIn the above example, the optimal cutoff \\lambda_{\\text{min}} is given by the smallest non-zero eigenvalue. Since the trace estimators all stochastic to some degree, we set the cutoff to slightly less than this value:\n\nlambda_min = min(ew[ew != 0.0])\nprint(f\"Smallest non-zero eigenvalue: {lambda_min:.6f}\")\n\nstep = lambda x: 1 if x &gt; (lambda_min * 0.90) else 0\nM = matrix_function(A, fun=step, deg=50)\nprint(f\"XTrace S_t(A) for t={lambda_min*0.90:.4f}: {TR.xtrace(M):6f}\")\n\nSmallest non-zero eigenvalue: 0.191916\nXTrace S_t(A) for t=0.1727: 120.000000\n\n\nIndeed, this works! Of course, here we’ve used the fact that we know the optimal cutoff value, but this can also be estimated with the lanczos method itself.\n\nfrom primate.diagonalize import lanczos\nfrom scipy.linalg import eigvalsh_tridiagonal\na,b = lanczos(A)\nrr = eigvalsh_tridiagonal(a,b) # Rayleigh-Ritz values\ntol = 10 * np.finfo(A.dtype).resolution\nprint(f\"Approx. cutoff: {np.min(rr[rr &gt; tol]):.6f}\")\n\nApprox. cutoff: 0.191916"
  },
  {
    "objectID": "basic/performance.html",
    "href": "basic/performance.html",
    "title": "Performance",
    "section": "",
    "text": "primate provides a variety of efficient algorithms for estimating quantities derived from matrix functions. These algorithms are largely implemented in C++ to minimize overhead, and for some computational problems primate can out-perform the standard algorithms for estimating spectral quantities by several orders of magnitude. Nonetheless, there are some performance-related caveats to be aware of.\n\n\n\nfrom scipy.linalg import toeplitz\nfrom primate.trace import hutch \n\nc = np.random.uniform(size=100, low=0, high=1)\nT = toeplitz(c)\n\n# np.sum(np.reciprocal(np.linalg.eigvalsh(T)))\nnp.sum(np.abs(np.linalg.eigvalsh(T)))\nest, info = hutch(T, fun=np.abs, maxiter=200, pdf='rademacher', rng=\"mt\", deg=20, seed=-1, plot=True)\n\nest, info = hutch(T, fun=np.abs, maxiter=800, pdf='rademacher', rng=\"mt\", quad=\"golub_welsch\", deg=200, seed=-1, plot=True)\nest, info = hutch(C, fun=np.abs, maxiter=800, pdf='rademacher', rng=\"mt\", quad=\"golub_welsch\", deg=200, seed=-1, plot=True)\n\n\n\nnp.sum(np.linalg.svd(T)[1])\n\nZ = np.zeros(T.shape)\nC = np.block([[Z, T], [T.T, Z]])\nhutch(C, fun=np.abs, maxiter=200, pdf='rademacher', rng=\"mt\", quad=\"golub_welsch\", deg=200, seed=-1, plot=False)\n\n\nhutch(T, fun=np.abs, maxiter=200, pdf='rademacher', deg=50, seed=-1, plot=False)\n\np = info['figure']\n\nfrom primate.trace import xtrace\nfrom primate.operator import matrix_function\nM = matrix_function(T, \"abs\")\nxtrace(M)\n\ns = info['samples']\nv = np.random.choice([-1, 1], size=T.shape[0])\nT @ v\n\ns[np.abs(s) &lt;= np.linalg.norm(T)].mean()\n\n\nfrom primate.diagonalize import lanczos\nfrom scipy.linalg import eigvalsh_tridiagonal\na, b = lanczos(T, deg=499, orth=150)\nnp.sum(np.abs(eigvalsh_tridiagonal(a,b)))\n\n\nimport timeit \ntimeit.timeit(lambda: hutch(A, maxiter=20, deg=5, fun=\"log\", quad=\"fttr\"), number = 1000)\ntimeit.timeit(lambda: np.sum(np.log(np.linalg.eigvalsh(A))), number = 1000)"
  },
  {
    "objectID": "basic/todo.html",
    "href": "basic/todo.html",
    "title": "primate",
    "section": "",
    "text": "# a, b = 0.8, 2\n# x = np.random.uniform(low=0, high=10, size=40)\n# eps = np.random.normal(loc=0, scale=1.0, size=40)\n# y = (a * x + b) + eps\n\n# p = figure(width=350, height=200)\n# p.scatter(x, y)\n# show(p)\n\n# X = np.c_[x,y]\n# from scipy.linalg import lstsq\n# from scipy.optimize import least_squares, leastsq, minimize, minimize_scalar\n# def L(beta: tuple):\n#   b, a = beta\n#   return np.linalg.norm(y - a * x + b)**2\n\n# def L(beta: tuple):\n#   b, a = beta\n#   return (y - (a * x + b))\n\n\n\n# res = minimize_scalar(L, x0=(0,1))\n# b_opt, a_opt = res.x\n# L([a_opt,b_opt])\n\n# res = least_squares(L, x0=(0,1))\n# b_opt, a_opt = res.x\n# f_opt = lambda x: x * a_opt + b_opt \n# p = figure(width=350, height=200)\n# p.scatter(x, y)\n# p.line(x=[0, 10], y=[b_opt, f_opt(10)])\n# show(p)\n\n# ## Normal equations...\n# XI = np.c_[np.ones(X.shape[0]), X]\n\n# c, b_opt, a_opt = (np.linalg.inv((XI.T @ XI)) @ XI.T) @ y\n# f_opt = lambda x: x * a_opt + b_opt \n# p = figure(width=350, height=200)\n# p.scatter(x, y)\n# p.line(x=[0, 10], y=[b_opt, f_opt(10)])\n# show(p)"
  },
  {
    "objectID": "theory/slq_pseudo.html",
    "href": "theory/slq_pseudo.html",
    "title": "SLQ Trace guide",
    "section": "",
    "text": "To clarify that that means, here’s an abstract presentation of the generic SLQ procedure:\n\n\n\\begin{algorithm} \\caption{Stochastic Lanczos Quadrature} \\begin{algorithmic} \\Input Symmetric operator ($A \\in \\mathbb{R}^{n \\times n}$) \\Require Number of queries ($n_v$), Degree of quadrature ($k$) \\Function{SLQ}{$A$, $n_v$, $k$} \\State $\\Gamma \\gets 0$ \\For{$j = 1, 2, \\dots, n_v$} \\State $v_i \\sim \\mathcal{D}$ where $\\mathcal{D}$ satisfies $\\mathbb{E}(v v^\\top) = I$ \\State $T^{(j)}(\\alpha, \\beta)$ $\\gets$ $\\mathrm{Lanczos}(A,v_j,k+1)$ \\State $[\\Theta, Y] \\gets \\mathrm{eigh\\_tridiag}(T^{(j)}(\\alpha, \\beta))$ \\State $\\tau_i \\gets \\langle e_1, y_i \\rangle$ \\State &lt; Do something with the node/weight pairs $(\\theta_i, \\tau_i^2)$ &gt; \\EndFor \\EndFunction \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "theory/intro.html",
    "href": "theory/intro.html",
    "title": "Introduction to trace estimation with primate",
    "section": "",
    "text": "primate contains a variety of methods for estimating quantities from matrix functions. One popular such quantity is the trace of f(A), for some generic f: [a,b] \\to \\mathbb{R} defined on the spectrum of A:  \\mathrm{tr}(f(A)) \\triangleq \\mathrm{tr}(U f(\\Lambda) U^{\\intercal}), \\quad \\quad f : [a,b] \\to \\mathbb{R}\nMany quantities of common interest can be expressed in as traces of matrix functions with the right choice of f. A few such function specializations include:\n\\begin{align*}\nf &= \\mathrm{id} \\quad &\\Longleftrightarrow& \\quad &\\mathrm{tr}(A) \\\\\nf &= f^{-1} \\quad &\\Longleftrightarrow& \\quad &\\mathrm{tr}(A^{-1}) \\\\\nf &= \\log \\quad &\\Longleftrightarrow& \\quad  &\\mathrm{logdet}(A) \\\\\nf &= \\mathrm{exp} \\quad &\\Longleftrightarrow& \\quad &\\mathrm{exp}(A) \\\\\nf &= \\mathrm{sign} \\quad &\\Longleftrightarrow& \\quad &\\mathrm{rank}(A)  \\\\\n&\\vdots \\quad &\\hphantom{\\Longleftrightarrow}& \\quad & \\vdots &\n\\end{align*}\nIn this introduction, the basics of randomized trace estimation are introduced, with a focus on how to use matrix-free algorithms to estimate traces of matrix functions.",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "theory/intro.html#basics-trace-computation",
    "href": "theory/intro.html#basics-trace-computation",
    "title": "Introduction to trace estimation with primate",
    "section": "Basics: Trace computation",
    "text": "Basics: Trace computation\n\nSuppose we wanted to compute the trace of some matrix A. By the very definition of the trace, we have a variety of means to do it: \\mathrm{tr}(A) \\triangleq \\sum\\limits_{i=1}^n A_{ii} = \\sum\\limits_{i=1}^n \\lambda_i = \\sum\\limits_{i=1}^n e_i^T A e_i \nwhere, in the right-most sum, we’re using e_i to represent the ith identity vector:  A_{ii} = e_i^T A e_i, \\quad e_i = [0, \\dots, 0, \\underbrace{1}_{i}, 0, \\dots, 0] \nLet’s see some code. First, we start with a simple (random) positive-definite matrix A \\in \\mathbb{R}^{n \\times n}.\n\nfrom primate.random import symmetric\nA = symmetric(150, pd = True)\n\nBy default, symmetric normalizes A such that the eigenvalues are uniformly distributed in the interval [0, 1]. For reference, the spectrum of A looks as follows:\n\n\n\n  \n\n\n\n\n\nNow, using numpy, we can verify all of these trace definitions are indeed equivalent:\n\n\nimport numpy as np\neye = lambda i: np.ravel(np.eye(1, 150, k=i))\nprint(f\"Direct: {np.sum(A.diagonal()):.8f}\")\nprint(f\"Eigen:  {np.sum(np.linalg.eigvalsh(A)):.8f}\")\nprint(f\"Matvec: {sum([eye(i) @ A @ eye(i) for i in range(150)]):.8f}\")\n\nDirect: 75.69739746\nEigen:  75.69739746\nMatvec: 75.69739746\n\n\nWhile (1) trivially yields \\mathrm{tr}(A) in (optimal) O(n) time, there exist situations where the diagonal entries of A are not available—particularly in the large-scale regime. In contrast, though both (2) and (3) are less efficient, they are nonetheless viable alternatives to obtain \\mathrm{tr}(A).",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "theory/intro.html#implicit-trace-estimation",
    "href": "theory/intro.html#implicit-trace-estimation",
    "title": "Introduction to trace estimation with primate",
    "section": "Implicit trace estimation",
    "text": "Implicit trace estimation\nThe implicit trace estimation problem can be stated as follows:\n\nGiven access to a square matrix A \\in \\mathbb{R}^{n \\times n} via its matrix-vector product operator x \\mapsto Ax, estimate its trace \\mathrm{tr}(A) = \\sum\\limits_{i=1}^n A_{ii}\n\nNote we no longer assume we have direct access to the diagonal here, leaving us with the Eigen or Matvec methods. The Eigen method is expensive, and though the Matvec method solves the problem exactly, its pretty inefficient from a computational standpoint—most of the entries of e_i are zero, thus most inner product computations do not contribute to the trace estimate at all.\nOne idea, attributed to A. Girard, is to replace e_i with random zero-mean vectors, e.g. random sign vectors v \\in \\{-1, +1\\}^{n} or random normal vectors v \\sim \\mathcal{N}(\\mu=0, \\sigma=1):\n\\mathtt{tr}(A) \\approx \\frac{1}{n_v}\\sum\\limits_{i=1}^{n_v} v_i^\\top A v_i \nIt was shown more formally later by M.F. Huchinson that if these random vectors v \\in \\mathbb{R}^n satisfy \\mathbb{E}[vv^T] = I then this approach indeed forms an unbiased estimator of \\mathrm{tr}(A). To see this, its sufficient to combine the linearity of expectation, the cyclic-property of the trace function, and the aforementioned isotropy conditions: \\mathtt{tr}(A) = \\mathtt{tr}(A I) = \\mathtt{tr}(A \\mathbb{E}[v v^T]) = \\mathbb{E}[\\mathtt{tr}(Avv^T)] = \\mathbb{E}[\\mathtt{tr}(v^T A v)] = \\mathbb{E}[v^T A v]\nNaturally we expect the approximation to gradually improve as more vectors are sampled, converging as n_v \\to \\infty. Let’s see if we can check this: we start by collecting a few sample quadratic forms\n\ndef isotropic(n):\n  yield from [np.random.choice([-1,+1], size=A.shape[1]) for i in range(n)]\nestimates = np.array([v @ A @ v for v in isotropic(500)])\n\nPlotting both the estimator formed by averaging the first k samples along with its absolute error, we get the following figures for k = 50 and k = 500, respectively:\n\n\n\n  \n\n\n\n\n\nSure enough, the estimator quickly gets within an error rate of about 1-5% away from the actual trace, getting generally closer as more samples are collected. On the other hand, improvement is not gaurenteed, and securing additional precision much less than 1% becomes increasingly difficult due to the randomness and variance of the sample estimates.",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "theory/intro.html#estimator-implementation",
    "href": "theory/intro.html#estimator-implementation",
    "title": "Introduction to trace estimation with primate",
    "section": "Estimator Implementation",
    "text": "Estimator Implementation\nRather than manually averaging samples, trace estimates can be obtained via the hutch function:\n\nfrom primate.trace import hutch\nprint(f\"Trace estimate: {hutch(A, maxiter=50)}\")\n\nTrace estimate: 75.34836195933401\n\n\nThough its estimation is identical to averaging quadratic forms v^T A v of isotropic vectors as above, hutch comes with a few extra features to simplify its usage. In particular, all v \\mapsto v^T A v evaluations are done in parallel, there are a variety of isotropic distributions and random number generators (RNGs) to choose from, and the estimator may be suppleid various convergence criteria to allow for early-stopping; see the hutch documentation page for more details.",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "theory/intro.html#extending-to-matrix-functions",
    "href": "theory/intro.html#extending-to-matrix-functions",
    "title": "Introduction to trace estimation with primate",
    "section": "Extending to matrix functions",
    "text": "Extending to matrix functions\nWhile there are some real-world applications for estimating \\mathrm{tr}(A), in many setting the trace of a given matrix is not a very informative quantity. On the other hand, as shown in the beginning, there are many situations where we might be interested in estimating the trace of a matrix function f(A). It is natural to consider extending the Hutchinson estimator above with something like:\n \\mathrm{tr}(f(A)) \\approx \\frac{n}{n_v} \\sum\\limits_{i=1}^{n_v} v_i^T f(A) v_i \nOf course, the remaining difficulty lies in computing quadratic forms v \\mapsto v^T f(A) v efficiently. The approach taken by Ubaru, Saad, and Seghouane (2017) is to notice that sums of these quantities can transformed into a Riemann-Stieltjes integral:\n v^T f(A) v = v^T U f(\\Lambda) U^T v = \\sum\\limits_{i}^n f(\\lambda_i) \\mu_i^2 = \\int\\limits_{a}^b f(t) \\mu(t)\nwhere scalars \\mu_i \\in \\mathbb{R} constitute a cumulative, piecewise constant function \\mu : \\mathbb{R} \\to \\mathbb{R}_+:\n\n\\mu_i = (U^T v)_i, \\quad \\mu(t) = \\begin{cases}\n0, & \\text{if } t &lt; a = \\lambda_1 \\\\\n\\sum_{j=1}^{i-1} \\mu_j^2, & \\text{if } \\lambda_{i-1} \\leq t &lt; \\lambda_i, i = 2, \\dots, n \\\\\n\\sum_{j=1}^n \\mu_j^2, & \\text{if } b = \\lambda_n \\leq t\n\\end{cases}\n\nAs with any definite integral, one would ideally like to approximate its value with a quadrature rule. An exemplary such approximation is the m-point Gaussian quadrature rule:\n \\int\\limits_{a}^b f(t) \\mu(t) \\approx \\sum\\limits_{k=1}^m \\omega_k f(\\eta_k) \nwith weights \\{\\omega_k\\} and nodes \\{ \\eta_k\\}. On one hand, Gaussian Quadrature (GQ) is just one of many quadrature rules that could be used to approximate v^T A v. On the other hand, GQ is often preferred over other rules due to its exactness on polynomials up to degree 2m - 1.\nOf course, both the weights and nodes of GQ rule are unknown for the integral above. This contrasts the typical quadrature setting, wherein \\omega is assumed uniform or is otherwise known ahead-of-time.\nA surprising and powerful fact is that both the weights \\{\\omega_k\\} and nodes \\{ \\eta_k\\} of the m-point GQ above are directly computable the degree m matrix T_m produced by the Lanczos method:\n Q_m^T A Q_m = T_m = Y \\Theta Y^T \\quad \\Leftrightarrow \\quad (\\eta_i, \\omega_i) = (\\theta_i, \\tau_i), \\; \\tau_i = (e_1^T y_i)^2 \nIn other words, the Ritz values \\{\\theta_i\\} of T_m are exactly the nodes of GQ quadrature rule applied to \\mu(t), while the first components of the Ritz vectors \\tau_i (squared) are exactly the weights. For more information about this non-trivial fact, see Golub and Meurant (2009).",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "theory/intro.html#stochastic-lanczos-quadrature",
    "href": "theory/intro.html#stochastic-lanczos-quadrature",
    "title": "Introduction to trace estimation with primate",
    "section": "Stochastic Lanczos Quadrature",
    "text": "Stochastic Lanczos Quadrature\nBy using Lanczos quadrature estimates, the theory above enables the Hutchinson estimator to extend to be used as an estimator for the trace of a matrix function:\n\\mathrm{tr}(f(A)) \\approx \\frac{n}{n_v} \\sum\\limits_{i=1}^{n_v} e_1^T f(T_m) e_1  = \\frac{n}{n_v} \\sum\\limits_{j=1}^{n_v} \\left ( \\sum\\limits_{i=1}^m \\tau_i^{(j)} f(\\theta_i)^{(j)} \\right )\nUnder relatively mild assumptions, if the function of interest f: [a,b] \\to \\mathbb{R} is analytic on [\\lambda_{\\text{min}}, \\lambda_{\\text{max}}], then for constants \\epsilon, \\eta \\in (0, 1) the output \\Gamma of the Hutchinson estimator satisfies:\n\n\\mathrm{Pr}\\Bigg[ \\lvert \\mathrm{tr}(f(A)) - \\Gamma \\rvert \\leq \\epsilon \\lvert \\mathrm{tr}(f(A)) \\rvert \\Bigg] \\geq 1 - \\eta\n\n…if the number of sample vectors n_v satisfies n_v \\geq (24/\\epsilon^2) \\log(2/\\eta) and the degree of the Lanczos expansion is sufficiently large. In other words, we can achieve an arbitrary (1 \\pm \\epsilon)-approximation of \\mathrm{tr}(f(A)) with success probability \\eta using on the order of \\sim O(\\epsilon^{-2} \\log(\\eta^{-1})) evaluations of e_1^T f(T_m) e_1. This probablistic guarantee is most useful when \\epsilon is not too small, i.e. only a relatively coarse approximation of \\mathrm{tr}(f(A)) is needed.",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "theory/intro.html#example-logdet-computation",
    "href": "theory/intro.html#example-logdet-computation",
    "title": "Introduction to trace estimation with primate",
    "section": "Example: Logdet computation",
    "text": "Example: Logdet computation\nBack to the computation, let’s see how the above theory translate to code with primate.\nTo parameterize a Girard-Hutchinson estimator for use with matrix-functions, it suffices to simply set the fun argument in hutch to either the name of a built-in function or an arbitrary Callable.\nFor example, to approximate the log-determinant of A:\n\n## Log-determinant\nlogdet_test = hutch(A, fun=\"log\", maxiter=25)\nlogdet_true = np.sum(np.log(np.linalg.eigvalsh(A)))\n\nprint(f\"Logdet (exact):  {logdet_true}\")\nprint(f\"Logdet (approx): {logdet_test}\")\n\nLogdet (exact):  -148.3218440234385\nLogdet (approx): -143.9468468239026\n\n\nHere we see that using only n / 6 evaluations of e_1^T f(T_m) e_1, we get a decent approximation of logdet(A). To get a better idea of the quality of the estimator, set verbose=True:\n\nest = hutch(A, fun=\"log\", maxiter=25, seed=5, verbose=True)\n\nGirard-Hutchinson estimator (fun=log, deg=20, quad=golub_welsch)\nEst: -142.843 +/- 6.06 (95% CI), CV: 2%, Evals: 25 [R] (seed: 5)\n\n\nThe first line of the statement contains fixed information about the estimator, including the type of estimator (Girard-Hutchinson), the function applied to the spectrum (log), the degree of the Lanczos quadrature approximation (20), and the quadrature method used (golub_welsch).\nThe second line prints the runtime information about the samples, such as the final trace estimate, its margin of error and coefficient of variation (a.k.a relative std. deviation), the number of samples vectors n_v, and the isotropic distribution of choice (‘R’ for rademacher).\nAnother way to get an idea of summarize the convergence behavior of the estimator is to pass the plot=True option. Here, we pass the same seed for reproducibility with the above call.\n\nest = hutch(A, fun=\"log\", maxiter=100, seed=5, plot=True)\n\n\n  \n\n\n\n\n\nSimilar information as reported by the verbose=True flag is shown cumulatively applied to every sample. The left plot shows the sample point estimates alongside yellow bands denoting the margin of error associated with the 95% confidence interval (CI) at every sample index. The right plot show convergence information, e.g. the running coefficient of variation is shown on the top-right and an upper bound on the absolute error is derived from the CI is shown on the bottom right.\nWhile the margin of error is typically a valid, conservative estimate of estimators error, in some situations these bounds may not be valid as they are based solely on the CLT implications for normally distributed random variables. In particular, there is an inherent amount of error associated with the Lanczos-based quadrature approximations that is not taken into account in the error bound, which may invalidate the corresponding CI’s.",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "advanced/cpp_integration.html",
    "href": "advanced/cpp_integration.html",
    "title": "C++ Integration",
    "section": "",
    "text": "primate’s generic API is enabled through function templates specialized using C++20 concepts. In other words, a function F requiring concept C will compile with any type T so long as T respects the constraints imposed by C. For example, generically, any type T respecting the LinearOperatorconcept shown below can be passed to the Lanczos method:\nIn english, an instance A of type T is said to support the LinearOperator concept if it has:\nshape() should return a pair (n,m) representing the sizes of the output and input vectors, respectively. Note in the matrix setting this corresponds to the number of rows and columns.",
    "crumbs": [
      "Advanced",
      "Usage from C++"
    ]
  },
  {
    "objectID": "advanced/cpp_integration.html#other-concepts",
    "href": "advanced/cpp_integration.html#other-concepts",
    "title": "C++ Integration",
    "section": "Other Concepts",
    "text": "Other Concepts\nDepending on the problem at hand, the supplied operator may need to meet other constraints. Here’s a short list additional operator concepts:\n\n\n\n\n\n\n\n\n\nConcept\nSupports\nSignature\nRequires\n\n\n\n\nLinearOperator\nA v \\mapsto o\nA.matvec(v, o)\nNA\n\n\nAdjointOperator\nA^T v \\mapsto o\nA.rmatvec(v, o)\nLinearOperator\n\n\nAdditiveOperator\no \\gets o + \\alpha Av\nA.matvec_add(v, alpha, o)\nLinearOperator\n\n\nAffineOperator\nSets t s.t. A + tB\nA.set_parameter(t)\nLinearOperator\n\n\nQuadOperator\nv^T A v\nA.quad(v)\nNA\n\n\n\nThe exported methods in primate only need the minimum constraints to be satisfied to compile: if you need access to the Lanczos method, then just supporting the LinearOperator concept is sufficient. On the other hand, adding support for other constraints can optimize the efficiency of certain methods; for example, the hutch method technically only requires a LinearOperator to do trace estimation (via matvec calls), but will also compile and prefer calling quad with a QuadOperator as input. In such a situaton, if your operator has an efficient quadratic form v \\mapsto v^T A v, then implementing quad may improve the performance of hutch.",
    "crumbs": [
      "Advanced",
      "Usage from C++"
    ]
  },
  {
    "objectID": "advanced/slq_param.html",
    "href": "advanced/slq_param.html",
    "title": "Parameterizing SLQ",
    "section": "",
    "text": "This guide walks through how to parameterize the SLQ method implemented in primate on the C++ side to approximate some spectral quantity of interest.",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#slq-as-a-function-template",
    "href": "advanced/slq_param.html#slq-as-a-function-template",
    "title": "Parameterizing SLQ",
    "section": "SLQ as a function template",
    "text": "SLQ as a function template\nBelow is the full signature of the SLQ function template:\n// Stochastic Lanczos quadrature method\ntemplate&lt; std::floating_point F, LinearOperator Matrix, ThreadSafeRBG RBG &gt;\nvoid slq (\n  const Matrix& A,                    // Any *LinearOperator*\n  const function&lt; F(int,F*,F*) &gt;& f,  // Generic function\n  const function&lt; bool(int) &gt;& stop,  // Early-stop function\n  const int nv,                       // Num. of sample vectors\n  const Distribution dist,            // Sample vector distribution\n  RBG& rng,                           // Random bit generator\n  const int lanczos_degree,           // Krylov subspace degree\n  const F lanczos_rtol,               // Lanczos residual tolerance\n  const int orth,                     // Add. vectors to orthogonalize\n  const int ncv,                      // Num. of Lanczos vectors\n  const int num_threads,              // # threads to allocate \n  const int seed                      // Seed for RNG \n)\nMany of the runtime arguments are documented in the lanczos or sl_trace docs; the compile-time (template) parameters are:\n\nThe floating point type (e.g. float, double, long double)\nThe operator type (e.g. Eigen::MatrixXf, torch::Tensor, LinOp)\nThe multi-threaded random number generator (e.g. ThreadedRNG64)\n\nNote any type combination satisfying these concepts (e.g. std::floating_point, LinearOperator) generates a function specialized of said types at compile-time—this is known as template instantiation.",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#generality-via-function-passing",
    "href": "advanced/slq_param.html#generality-via-function-passing",
    "title": "Parameterizing SLQ",
    "section": "Generality via function passing",
    "text": "Generality via function passing\nGiven a valid set of parameters, the main body of the SLQ looks something like this:\n  bool stop_flag = false;\n  #pragma omp parallel shared(stop_flag)\n  {\n    // &lt; allocations for Q, alpha, beta, etc. &gt; \n    int tid = omp_get_thread_num(); // thread-id \n    \n    #pragma omp for\n    for (i = 0; i &lt; nv; ++i){\n      if (stop_flag){ continue; }\n      generate_isotropic&lt; F &gt;(...); // populates q\n      lanczos_recurrence&lt; F &gt;(...); // populates alpha + beta\n      lanczos_quadrature&lt; F &gt;(...); // populates nodes + weights\n      f(i, q, Q, nodes, weights);   // Run user-supplied function \n      #pragma omp critical\n      {\n        stop_flag = stop(i);        // Checks for early-stopping\n      }\n    } // end for\n  } // end parallel \nThere are two functions that can be used for generalizing SLQ for different purposes.\nThe first generic function f can read, save, or modify the information available from the iteration index i, the isotropic vector q, the Lanczos vectors Q, and/or the quadrature information nodes, weights. Note this function is run in the parallel section.\nThe second is a boolean-valued function stop which can be used to stop the iteration early, for example if convergence has been achieved according to some rule. Since this is run in the critical section, it is called sequentially.",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#using-slq-to-estimate-mathrmtrfa",
    "href": "advanced/slq_param.html#using-slq-to-estimate-mathrmtrfa",
    "title": "Parameterizing SLQ",
    "section": "Using SLQ to estimate \\mathrm{tr}(f(A))",
    "text": "Using SLQ to estimate \\mathrm{tr}(f(A))\nThe SLQ method is often used to estimate the trace of an arbitrary matrix function:\n \\mathrm{tr}(f(A)), \\quad \\text{ where } f(A) = U f(\\Lambda) U^T \nIt’s has been shown1 that the information obtained by the Lanczos method is sufficient to obtained a Gaussian quadrature approximation of the empirical spectral measure of A. By sampling zero-mean vectors satisfying \\mathbb{E}[v v^T] = I, one can obtain estimates of the trace above: \\operatorname{tr}(f(A)) \\approx \\frac{n}{\\mathrm{n}_{\\mathrm{v}}} \\sum_{l=1}^{\\mathrm{n}_{\\mathrm{v}}}\\left(\\sum_{k=0}^m\\left(\\tau_k^{(l)}\\right)^2 f\\left(\\theta_k^{(l)}\\right)\\right)\nIt turns out averaging these trace estimates yields unbiased, Girard-Hutchinson estimator of the trace. To see why this estimator is unbiased, note that:  \\mathtt{tr}(A) = \\mathbb{E}[v^T A v] \\approx \\frac{1}{n_v}\\sum\\limits_{i=1}^{n_v} v_i^\\top A v_i \nThus, all we need to do to estimate the trace of a matrix function is multiply and sum the quadrature nodes and weights output by SLQ.",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#sl_trace-method",
    "href": "advanced/slq_param.html#sl_trace-method",
    "title": "Parameterizing SLQ",
    "section": "sl_trace method",
    "text": "sl_trace method\nTo see how these formulas are actually implemented with the generic SLQ implementation, here’s an abbreviated form of the sl_trace function implemented by primate:\ntemplate&lt; std::floating_point F, LinearOperator Matrix, ThreadSafeRBG RBG &gt;\nvoid sl_trace(\n  const Matrix& A, const std::function&lt; F(F) &gt; sf, RBG& rbg, \n  const int nv, const int dist, const int engine_id, const int seed,\n  const int deg, const float lanczos_rtol, const int orth, const int ncv,\n  const F atol, const F rtol\n  F* estimates\n){  \n  using VectorF = Eigen::Array&lt; F, Dynamic, 1&gt;;\n\n  // Parameterize the trace function (runs in parallel)\n  auto trace_f = [&](int i, F* q, F* Q, F* nodes, F* weights){\n    Map&lt; VectorF &gt; nodes_v(nodes, deg, 1);     // no-op\n    Map&lt; VectorF &gt; weights_v(weights, deg, 1); // no-op\n    nodes_v.unaryExpr(sf);\n    estimates[i] = (nodes_v * weights_v).sum();\n  };\n  \n  // Convergence checking like scipy.integrate.quadrature\n  int n = 0;\n  F mu_est = 0.0, mu_pre = 0.0;\n  const auto early_stop = [&](int i) -&gt; bool {\n    ++n; // Number of estimates\n    mu_est = (1.0 / F(n)) * (estimates[i] + (n - 1) * mu_pre); \n    bool atol_check = abs(mu_est - mu_pre) &lt;= atol;\n    bool rtol_check = abs(mu_est - mu_pre) / mu_est &lt;= rtol; \n    mu_pre = mu_est; \n    return atol_check || rtol_check;\n  };\n\n  // Execute the stochastic Lanczos quadrature with the trace function \n  slq&lt; float &gt;(A, trace_f, early_stop, ...);\n}\nAs before, two functions are used to parameterize the slq method.\nThe first (trace_f) applies an arbitrary spectral function sf to the Rayleigh-Ritz values obtained by the Lanczos tridiagonalization of A(or equivalently, the nodes of the Gaussian quadrature). These are the \\theta’s in the pseudocode above. When multiplied by the weights of the quadrature, the corresponding sum forms an estimate of the trace of the matrix function.\nThe second function early_stop is used to check for convergence of the estimator. First, it uses the trace estimate x_n to update the sample mean \\mu_n via the formula:\n \\mu_n = n^{-1} [x_n + (n - 1)\\mu_{n-1}] \nThen, much in the same way the quadrature function from scipy.integrate approximates a definite integral, it checks for convergence using the absolute and relative tolerances supplied by the user. Returning true signals convergence, stopping the iteration early.",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#references",
    "href": "advanced/slq_param.html#references",
    "title": "Parameterizing SLQ",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#footnotes",
    "href": "advanced/slq_param.html#footnotes",
    "title": "Parameterizing SLQ",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUbaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099.↩︎",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/pybind11_integration.html",
    "href": "advanced/pybind11_integration.html",
    "title": "pybind11 Integration",
    "section": "",
    "text": "If you’re using pybind11, you can easily incorporate your own custom linear operator / matrix function pair using primates binding headers.\nTODO",
    "crumbs": [
      "Advanced",
      "Integrating with pybind11"
    ]
  },
  {
    "objectID": "theory/matrix_functions.html",
    "href": "theory/matrix_functions.html",
    "title": "Matrix function estimation with primate",
    "section": "",
    "text": "In the introduction, the basics of implicit trace estimation were introduced, wherein it shown both in theory and with code using primate how to estimate the trace of matrix functions:\nf(A) = U f(\\Lambda) U^T, \\quad f: [a,b] \\to \\mathbb{R}\nIn particular, the introduction briefly covered how the Lanczos method is intimately connected to Gaussian Quadrature, and how this connection enables fast randomized trace estimation of f(A).\nIn this post, I’ll cover how to approximate the action v \\mapsto f(A)v for any function f with primate with its matrix_function API, and how to compose this functionality with other trace estimators.",
    "crumbs": [
      "Theory",
      "Matrix functions"
    ]
  },
  {
    "objectID": "theory/matrix_functions.html#matrix-function-approximation",
    "href": "theory/matrix_functions.html#matrix-function-approximation",
    "title": "Matrix function estimation with primate",
    "section": "Matrix function approximation",
    "text": "Matrix function approximation\nIf A \\in \\mathbb{R}^{n \\times n} and n is large, obtaining f(A) \\in \\mathbb{R}^{n \\times n} explicitly can be very expensive. One way to sidestep this difficulty is to approximate v \\mapsto f(A)v using the degree-k Lanczos expansion:\n Q^T A Q = T \\quad \\Leftrightarrow \\quad f(A)v \\approx \\lVert x \\rVert \\cdot Q f(T) e_1 \nIt’s been shown by (Musco, Musco, and Sidford 2018) that this approximation has the following guarantee:\n\\|f(\\mathbf{A}) \\mathbf{x}-\\mathbf{y}\\| \\leq 2\\|\\mathbf{x}\\| \\cdot \\min _{\\substack{\\text { polynomial } p \\\\ \\text { with degree }&lt;k}}\\left(\\max _{x \\in\\left[\\lambda_{\\min }(\\mathbf{A}), \\lambda_{\\max }(\\mathbf{A})\\right]}|f(x)-p(x)|\\right)\nIn other words, up to a factor of 2, the error \\|f(\\mathbf{A}) \\mathbf{x}-\\mathbf{y}\\| is bounded by the uniform error of the best polynomial approximation to f with degree &lt; k. For general matrix functions, this implies that finite-precision Lanczos essentially matches strongest known exact arithmetic bounds.\nThis suggests an idea: can we convert any matrix-like object into a LinearOperator that transparently converts matrix-vector products Av into products with matrix-functions f(A)v?",
    "crumbs": [
      "Theory",
      "Matrix functions"
    ]
  },
  {
    "objectID": "theory/matrix_functions.html#establishing-a-baseline",
    "href": "theory/matrix_functions.html#establishing-a-baseline",
    "title": "Matrix function estimation with primate",
    "section": "Establishing a baseline",
    "text": "Establishing a baseline\nprimate allows the user to construct a LinearOperator directly from an existing matrix-like object A and callable fun through its matrix_function API.\n\nAs a baseline example, consider the action that adds an \\epsilon amount of mass to the diagonal of A:\nv \\mapsto (A + \\epsilon I)v\nFor any fixed \\epsilon, imitating this matrix action can be done in three different ways:\n\nObtain u = Av and then add u \\gets u + \\epsilon \\cdot v\nForm (A + \\epsilon I) and explicitly carry out the multiplication\nMultiply v by f(A) induced by f(\\lambda) = \\lambda + \\epsilon\n\nLets see what the code to accomplish this using (3) looks like:\n\nfrom primate.random import symmetric\nfrom primate.operator import matrix_function\n\n## Random matrix + input vector\nA = symmetric(150, pd = True)\nv = np.random.uniform(size=A.shape[1])\n\n## Ground truth v |-&gt; f(A)v\nLambda, U = np.linalg.eigh(A)\nv_truth = (U @ np.diag(Lambda + 0.10) @ U.T) @ v\n\n## Lanczos approximation\nM = matrix_function(A, fun = lambda x: x + 0.10)\nv_test = np.ravel(M @ v)\n\nvn, vn_approx = np.linalg.norm(v_truth), np.linalg.norm(v_test)\nprint(f\"f(A)v norm:  {vn:.6f}\")\nprint(f\"Approx norm: {vn_approx:.6f}\")\nprint(f\"Max diff:    {np.max(np.abs(v_truth - np.ravel(v_test))):.6e}\")\nprint(f\"cosine sim:  {np.dot(v_test, v_truth) / (vn * vn_approx):6e}\")\n\nf(A)v norm:  4.392660\nApprox norm: 4.392660\nMax diff:    5.218048e-15\ncosine sim:  1.000000e+00\n\n\nObserve M matches the ground truth v \\mapsto (A + \\epsilon I)v. In this way, one benefit of using matrix_function is that it allows one to approximate f(A) by thinking only about what is happening at the spectral level (as opposed to the matrix level). Of course, this example is a bit non-convincing as there are simpler ways of achieving the same result as shown by (1) and (2), for example:\n\nnp.allclose(A @ v + 0.10 * v, v_truth)\n\nTrue\n\n\nBaseline established.",
    "crumbs": [
      "Theory",
      "Matrix functions"
    ]
  },
  {
    "objectID": "theory/matrix_functions.html#when-v-mapsto-fav-is-not-known",
    "href": "theory/matrix_functions.html#when-v-mapsto-fav-is-not-known",
    "title": "Matrix function estimation with primate",
    "section": "When v \\mapsto f(A)v is not known",
    "text": "When v \\mapsto f(A)v is not known\nOn the other hand, there are plenty of situations where one doesn’t have access to these simpler expressions. For example, consider the map:\nv \\mapsto (A + \\epsilon I)^{-1} v\nSuch expressions pop up in a variety of settings, such as in Tikhonov regularization, in Schatten-norm estimation Ubaru, Saad, and Seghouane (2017), in the Cholesky factorization of PSD matrices, and so on. Unlike the previous setting, we cannot readily access v \\mapsto f(A)v unless we explicitly compute the full spectral decomposition of A or the inverse of A, both of which are expensive to obtain.\n\n## Alternative: v_truth = np.linalg.inv((A + 0.10 * np.eye(A.shape[0]))) @ v\nv_truth = (U @ np.diag(np.reciprocal(Lambda + 0.10)) @ U.T) @ v\n\n## Lanczos approximation\nM = matrix_function(A, fun = lambda x: 1.0 / (x + 0.10))\nv_test = np.ravel(M @ v)\n\nvn, vn_approx = np.linalg.norm(v_truth), np.linalg.norm(v_test)\nprint(f\"f(A)v norm:  {vn:.6f}\")\nprint(f\"Approx norm: {vn_approx:.6f}\")\nprint(f\"Max diff:    {np.max(np.abs(v_truth - np.ravel(v_test))):.6e}\")\nprint(f\"cosine sim:  {np.dot(v_test, v_truth) / (vn * vn_approx):6e}\")\n\nf(A)v norm:  32.982583\nApprox norm: 32.982583\nMax diff:    1.626085e-05\ncosine sim:  1.000000e+00\n\n\nThere is a larger degree of error compared to the base as evidenced by the \\lVert \\cdot \\rVert_\\infty-normed difference between v_truth and v_test, however this is to be expected, as in general approximating the action v \\mapsto A^{-1} v will always be more difficult that v \\mapsto A v, even if A is well-conditioned.",
    "crumbs": [
      "Theory",
      "Matrix functions"
    ]
  },
  {
    "objectID": "theory/matrix_functions.html#back-to-trace-estimation",
    "href": "theory/matrix_functions.html#back-to-trace-estimation",
    "title": "Matrix function estimation with primate",
    "section": "Back to trace estimation",
    "text": "Back to trace estimation\nThere are several use-cases wherein one might be interested in the output f(A)v itself—see Musco, Musco, and Sidford (2018) and references within for some examples. One use-case is the extension of existing matvec-dependent implicit trace estimators to matrix-function trace estimators.\n\nfrom primate.trace import hutch, xtrace\nM = matrix_function(A, fun=\"log\")\nprint(f\"Logdet exact:  {np.sum(np.log(np.linalg.eigvalsh(A))):6e}\")\nprint(f\"Logdet Hutch:  {hutch(A, fun='log'):6e}\")\nprint(f\"Logdet XTrace: {xtrace(M):6e}\")\n\nLogdet exact:  -1.483218e+02\nLogdet Hutch:  -1.521959e+02\nLogdet XTrace: -1.483155e+02\n\n\nAs with the hutch estimators applied to matrix functions, note that the action v \\mapsto f(A)v is subject to the approximation errors studied by Musco, Musco, and Sidford (2018), making such extensions limited to functions that are well-approximated by the Lanczos method.",
    "crumbs": [
      "Theory",
      "Matrix functions"
    ]
  },
  {
    "objectID": "theory/lanczos.html",
    "href": "theory/lanczos.html",
    "title": "The Lanczos method",
    "section": "",
    "text": "Whether for simplifying the representation of complicated systems, characterizing the asymptotic behavior of differential equations, or even just fitting polynomials to data via least-squares, decomposing linear operators has had significant use in many areas of sciences and engineering.\nCentral to operator theory is the spectral theorem, which provides conditions under which a linear operator A : \\mathbb{R}^n \\to \\mathbb{R}^n can be diagonalized in terms of its eigenvalues and eigenvectors:  A = U \\Lambda U^{-1}\nIn the case where A is symmetric, the eigen-decomposition is not only guarenteed to exist, but its canonical form may be obtained via orthogonal diagonalization. Such matrices are among the most commonly encountered matrices in applications.\nIn 1950, Cornelius Lanczos studied an alternative means of decomposition via tridiagonalization:   AQ = Q T \\quad \\Leftrightarrow \\quad Q^T A Q = T  The algorithm by which one produces such a T is known as the Lanczos method. Despite its age, it remains the standard algorithm1 both for computing eigensets and solving linear systems in the large-scale regime. Having intrinsic connections to the conjugate gradient method, the theory of orthogonal polynomials, and Gaussian quadrature, it is one of the most important numerical methods of all time—indeed, an IEEE guest editorial places it among the top 10 most influential algorithms of the 20th century.\nAs the Lanczos method lies at the heart of primate’s design, this post introduces it, with a focus on its motivating principles and computational details. For its API usage, see the lanczos page.",
    "crumbs": [
      "Theory",
      "The Lanczos Method"
    ]
  },
  {
    "objectID": "theory/lanczos.html#lanczos-on-a-bumper-sticker",
    "href": "theory/lanczos.html#lanczos-on-a-bumper-sticker",
    "title": "The Lanczos method",
    "section": "Lanczos on a bumper sticker",
    "text": "Lanczos on a bumper sticker\nGiven any non-zero v \\in \\mathbb{R}^n, Lanczos generates a Krylov subspace via successive powers of A:\n\n\\mathcal{K}(A, v) \\triangleq \\{ \\, A^{0} v, A^{1}v, A^{2}v, \\dots, A^{n}v \\, \\}\n\nThese vectors are independent, so orthogonalizing them not only yields an orthonormal basis for \\mathcal{K}(A, v) but also a change-of-basis matrix Q, allowing A to be represented by a new matrix T:\n\n\\begin{align*}\nK &= [\\, v \\mid Av \\mid A^2 v \\mid \\dots \\mid A^{n-1}v \\,] && \\\\\nQ &= [\\, q_1, q_2, \\dots, q \\,] \\gets \\mathrm{qr}(K) &&  \\\\\nT &= Q^T A Q &&\n\\end{align*}\n\nIt turns out that since A is symmetric, T is guaranteed to have a symmetric tridiagonal structure:\n\nT = \\mathrm{tridiag}\\Bigg(\n\\begin{array}{ccccccccc}\n& \\beta_2 & & \\beta_3 & & \\cdot & & \\beta_n & \\\\\n\\alpha_1 & & \\alpha_2 & & \\cdot & & \\cdot & & \\alpha_n \\\\\n& \\beta_2 & & \\beta_3 & & \\cdot & & \\beta_n &\n\\end{array}\n\\Bigg)\n\n\n\nSince \\mathrm{range}(Q) = \\mathcal{K}(A, v), the change-of-basis A \\mapsto Q^{-1} A Q is in fact a similarity transform, which are known to be equivalence relations on \\mathcal{S}^n—thus we can obtain \\Lambda by diagonalizing T:\n T = Y \\Lambda Y^T  \nAs T is quite structured, it can be easily diagonalized, thus we have effectively solved the eigenvalue problem. To quote the Lanczos introduction from Parlett, could anything be more simple?",
    "crumbs": [
      "Theory",
      "The Lanczos Method"
    ]
  },
  {
    "objectID": "theory/lanczos.html#the-iteration-part",
    "href": "theory/lanczos.html#the-iteration-part",
    "title": "The Lanczos method",
    "section": "The “iteration” part",
    "text": "The “iteration” part\nLanczos originally referred to his algorithm as the method of minimized iterations, and indeed nowadays it is often called an iterative method. Where’s the iterative component?\nIf you squint hard enough, you can deduce that for every j \\in [1, n): A Q_j = Q_j T_j + \\beta_{j+1} q_{j+1} e_{j}^T Equating the j-th columns on each side of the equation and rearranging yields a three-term recurrence: \n\\begin{align*}\nA q_j &= \\beta_{j\\text{-}1} q_{j\\text{-}1} + \\alpha_j q_j + \\beta_j q_{j+1} \\\\\n\\Leftrightarrow \\beta_{j} \\, q_{j+1} &= A q_j - \\alpha_j \\, q_j - \\beta_{j\\text{-}1} \\, q_{j\\text{-}1}  \n\\end{align*}\n\nThe equation above is a variable-coefficient second-order linear difference equation, and it is known such equations have unique solutions; they are given below: \n\\alpha_j = q_j^T A q_j, \\;\\; \\beta_j = \\lVert r_j \\rVert_2, \\;\\; q_{j+1} = r_j / \\beta_j\n\n\n\\text{where  } r_j = (A - \\alpha_j I)q_j - \\beta_{j\\text{-}1} q_j\n\nIn other words, if (q_{j\\text{-}1}, \\beta_j, q_j) are known, then (\\alpha_j, \\beta_{j+1}, q_{j+1}) are completely determined. In theory, this means we can iteratively generate both Q and T using just a couple vectors at a time—no need to explicitly call to the QR algorithm as shown above. Pretty nifty, eh!",
    "crumbs": [
      "Theory",
      "The Lanczos Method"
    ]
  },
  {
    "objectID": "theory/lanczos.html#wait-isnt-t-arbitrary",
    "href": "theory/lanczos.html#wait-isnt-t-arbitrary",
    "title": "The Lanczos method",
    "section": "Wait, isn’t T arbitrary?",
    "text": "Wait, isn’t T arbitrary?\nUnfortunately—and unlike the spectral decomposition2—there is no canonical choice of T. Indeed, as T is a family with n - 1 degrees of freedom and v \\in \\mathbb{R}^n was chosen arbitrarily, there are infinitely many essentially distinct such decompositions.\nNot all hope is lost though, as it turns out that T is actually fully characterized by v. To see this, notice that since Q is an orthogonal matrix, we have:\n Q Q^T = I_n = [e_1, e_2, \\dots, e_n] \nBy extension, given an initial pair (A, q_1) satisfying \\lVert q_1 \\rVert = 1, the following holds:\n\nK_n(A, q_1) = Q Q^T K_n(A, q_1) = Q[ \\, e_1 \\mid T e_1 \\mid T^2 e_1 \\mid \\dots \\mid T^{n-1} e_1 \\, ]\n\n…this is actually a QR factorization, which is essentially unique! Indeed, the Implicit Q Theorem asserts that if an upper Hessenburg matrix T \\in \\mathbb{R}^{n \\times n} has only positive elements on its first subdiagonal and there exists an orthogonal matrix Q such that Q^T A Q = T, then Q and T are uniquely determined3 by (A, q_1).",
    "crumbs": [
      "Theory",
      "The Lanczos Method"
    ]
  },
  {
    "objectID": "theory/lanczos.html#beating-the-complexity-bounds",
    "href": "theory/lanczos.html#beating-the-complexity-bounds",
    "title": "The Lanczos method",
    "section": "Beating the complexity bounds",
    "text": "Beating the complexity bounds\nElegant and as theoretically founded as the Lanczos method may be, is it efficient in practice?\nLet’s start by establishing a baseline on its complexity:\n\nTheorem 1 (Parlett 1994) Given a symmetric rank-r matrix A \\in \\mathbb{R}^{n \\times n} whose operator x \\mapsto A x requires O(\\eta) time and O(\\nu) space, the Lanczos method computes \\Lambda(A) in O(\\max\\{\\eta, n\\}\\cdot r) time and O(\\max\\{\\nu, n\\}) space, when computation is done in exact arithmetic\n\nAs its clear from the theorem, if we specialize it such that r = n and \\eta = \\nu = n, then the Lanczos method requires just O(n^2) time and O(n) space to execute. In other words, the Lanczos method drops both the time and space complexity4 of obtaining spectral information by order of magnitude over similar eigen-algorithms that decompose A directly.\nTo see why this is true, note that a symmetric tridiagonal matrix is fully characterized by its diagonal and subdiagonal terms, which requires just O(n) space. If we assume that v \\mapsto Av \\sim O(n), then carrying out the recurrence clearly takes at most O(n^2) time, since there are most n such vectors \\{q_i\\}_{i=1}^n to generate!\nNow, if we need to store all of Y or Q explicitly, we clearly need O(n^2) space to do so. However, if we only need the eigen-values \\Lambda(A) (and not their eigen-vectors U), then we may execute the recurrence keeping at most three vectors \\{q_{j-1}, q_{j}, q_{j+1}\\} in memory at any given time. Since each of these is O(n) is size, the claim of O(n) space is justified!",
    "crumbs": [
      "Theory",
      "The Lanczos Method"
    ]
  },
  {
    "objectID": "theory/lanczos.html#footnotes",
    "href": "theory/lanczos.html#footnotes",
    "title": "The Lanczos method",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA variant of the Lanczos method is actually at the heart scipy.sparse.linalg’s default eigsh solver (which is a port of ARPACK).↩︎\nThe spectral decomposition A = U \\Lambda U^T identifies a diagonalizable A with its spectrum \\Lambda(A) up to a change of basis A \\mapsto M^{-1} A M↩︎\nThe spectral decomposition A = U \\Lambda U^T identifies a diagonalizable A with its spectrum \\Lambda(A) up to a change of basis A \\mapsto M^{-1} A M↩︎\nFor general A \\in \\mathbb{R}^{n \\times n}, computing the spectral-decomposition is essentially bounded by the matrix-multiplication time: \\Theta(n^\\omega) time and \\Theta(n^2) space, where \\omega \\approx 2.37\\dots is the matrix multiplication constant. If we exclude the Strassen model for computation, we get effectively a \\Omega(n^3) time and \\Omega(n^2) space bound.↩︎",
    "crumbs": [
      "Theory",
      "The Lanczos Method"
    ]
  },
  {
    "objectID": "basic/install.html",
    "href": "basic/install.html",
    "title": "Installation",
    "section": "",
    "text": "primate is a standard PEP-517 package, and thus can be installed via pip:\nCurrently the package must be built from source via cloning the repository. PYPI support is planned.",
    "crumbs": [
      "Basics",
      "Installation"
    ]
  },
  {
    "objectID": "basic/install.html#footnotes",
    "href": "basic/install.html#footnotes",
    "title": "Installation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSingle-thread execution only; ARM-based OSX runners compile with Apple’s clang, which doesn’t natively ship with libomp.dylib, though this may be fixable. Feel free to file an PR if you can get this working.↩︎\nSingle-thread execution only; primate depends on OpenMP 4.5+, which isn’t supported on any Windows compiler I’m aware of.↩︎",
    "crumbs": [
      "Basics",
      "Installation"
    ]
  },
  {
    "objectID": "basic/integration.html",
    "href": "basic/integration.html",
    "title": "Integration",
    "section": "",
    "text": "primate supports a variety of matrix-types of the box, including numpy ndarray’s, compressed sparse matrices (a lá SciPy), and LinearOperators—the latter enables the use of matrix free operators.\nOutside of the natively types above, the basic requirements for any operator A to be used with e.g. the Lanczos method in primate are:\nHere’s an example of a simple operator representing a Diagonal matrix, which inherits a .matvec() method by following the subclassing rules of SciPy’s LinearOperator:",
    "crumbs": [
      "Basics",
      "Integration"
    ]
  },
  {
    "objectID": "basic/integration.html#c-usage",
    "href": "basic/integration.html#c-usage",
    "title": "Integration",
    "section": "C++ usage",
    "text": "C++ usage\nSimilarly, to get started calling any matrix-free function provided by primate on the C++ side, such hutch or lanczos, simply pass any type with .shape() and .matvec() member functions:\nclass LinOp {\n  int nrow, ncol;\n  \n  LinOp(int nr, int nc) : nrow(nr), ncol(nc) {}\n  \n  void matvec(const float* input, float* output) const {\n    ... // implementation details \n  }\n\n  void shape() const { return std::make_pair(nrow, ncol); }\n}\nIt’s up to you to ensure shape() yields the correct size; primate will supply vectors to input of size .shape().second (number of columns) and guarantees the pointer to the output will be at least shape().first (number of rows), no more.\nTo read more about how semantics extend to the C++ side as well via C++20 concepts—see the C++ integration guide. If you’re using pybind11 and you want to extend primate’s Python API to work natively with linear operator implemented in C++, see the pybind11 integration guide.",
    "crumbs": [
      "Basics",
      "Integration"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Package overview",
    "section": "",
    "text": "primate, short for Probabalistic Implicit Matrix Trace Estimator, is a Python package that provides estimators of quantities derived from matrix functions; that is, matrices parameterized by functions:\nf(A) \\triangleq U f(\\Lambda) U^{\\intercal}, \\quad \\quad f : [a,b] \\to \\mathbb{R}\nEstimator approximations are obtained via the Lanczos1 and stochastic Lanczos quadrature2 methods, which are well-suited for sparse or structured operators supporting fast v \\mapsto Av actions.\nNotable features of primate include:\nprimate was partially inspired by the imate package—for a comparison of the two, see here.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Package overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMusco, Cameron, Christopher Musco, and Aaron Sidford. (2018) “Stability of the Lanczos method for matrix function approximation.”↩︎\nUbaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature.↩︎\nThis includes std::function’s, C-style function pointers, functors, and lambda expressions.↩︎",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "reference/random.rademacher.html",
    "href": "reference/random.rademacher.html",
    "title": "random.rademacher",
    "section": "",
    "text": "random.rademacher(size, rng='splitmix64', seed=-1, dtype=np.float32)\nGenerates random vectors from the rademacher distribution.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsize\nint or tuple\nOutput shape to generate.\nrequired\n\n\nrng\nstr = “splitmix64”\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint = -1\nSeed for the generator. Use -1 to for random (non-deterministic) behavior.\n-1\n\n\ndtype\ndtype = float32\nFloating point dtype for the output. Must be float32 or float64.\nnp.float32\n\n\n\n\n\n\nnp.narray Randomly generated matrix of shape size with entries in { -1, 1 }.",
    "crumbs": [
      "API Reference",
      "Random",
      "Rademacher"
    ]
  },
  {
    "objectID": "reference/random.rademacher.html#parameters",
    "href": "reference/random.rademacher.html#parameters",
    "title": "random.rademacher",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsize\nint or tuple\nOutput shape to generate.\nrequired\n\n\nrng\nstr = “splitmix64”\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint = -1\nSeed for the generator. Use -1 to for random (non-deterministic) behavior.\n-1\n\n\ndtype\ndtype = float32\nFloating point dtype for the output. Must be float32 or float64.\nnp.float32",
    "crumbs": [
      "API Reference",
      "Random",
      "Rademacher"
    ]
  },
  {
    "objectID": "reference/random.rademacher.html#returns",
    "href": "reference/random.rademacher.html#returns",
    "title": "random.rademacher",
    "section": "",
    "text": "np.narray Randomly generated matrix of shape size with entries in { -1, 1 }.",
    "crumbs": [
      "API Reference",
      "Random",
      "Rademacher"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html",
    "href": "reference/diagonalize.lanczos.html",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "diagonalize.lanczos(A, v0=None, deg=None, rtol=1e-08, orth=0, sparse_mat=False, return_basis=False, seed=None, dtype=None)\nLanczos method for matrix tridiagonalization.\nThis function implements Paiges A27 variant (1) of the Lanczos method for tridiagonalizing linear operators, with additional modifications to support varying degrees of re-orthogonalization. In particular, orth=0 corresponds to no re-orthogonalization, orth &lt; deg corresponds to partial re-orthogonalization, and orth &gt;= deg corresponds to full re-orthogonalization.\n\n\nThe Lanczos method builds a tridiagonal T from a symmetric A via an orthogonal change-of-basis Q:  Q^T A Q  = T  Unlike other Lanczos implementations (e.g. SciPy’s eigsh), which includes e.g. sophisticated restarting, deflation, and selective-reorthogonalization steps, this method simply executes deg steps of the Lanczos method with the supplied v0 and returns the resulting tridiagonal matrix T.\nRayleigh-Ritz approximations of the eigenvalues of A can be further obtained by diagonalizing T via any symmetric tridiagonal eigenvalue solver, scipy.linalg.eigh_tridiagonal though note unlike eigsh no checking is performed for ‘ghost’ or already converged eigenvalues. To increase the accuracy of these eigenvalue approximation, try increasing orth and deg. Supplying either negative values or values larger than deg for orth will result in full re-orthogonalization, though note the number of matvecs scales linearly with deg and the number of inner-products scales quadratically with orth.\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nA\nscipy.sparse.linalg.LinearOperator or ndarray or sparray\nSymmetric operator to tridiagonalize.\nrequired\n\n\nv0\nndarray\nInitial vector to orthogonalize against.\nNone\n\n\ndeg\nint\nSize of the Krylov subspace to expand.\nNone\n\n\nrtol\nfloat\nRelative tolerance to consider the invariant subspace as converged.\n1e-8\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against.\n0\n\n\nsparse_mat\nbool\nWhether to output the tridiagonal matrix as a sparse matrix.\nFalse\n\n\nreturn_basis\nbool\nIf True, returns the Krylov basis vectors Q.\nFalse\n\n\ndtype\ndtype\nThe precision dtype to specialize the computation.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntuple\nA tuple (a,b) parameterizing the diagonal and off-diagonal of the tridiagonal matrix. If return_basis=True, the tuple (a,b), Q is returned, where Q represents an orthogonal basis for the degree-deg Krylov subspace.\n\n\n\n\n\n\nscipy.linalg.eigh_tridiagonal : Eigenvalue solver for real symmetric tridiagonal matrices. operator.matrix_function : Approximates the action of a matrix function via the Lanczos method.\n\n\n\n\nPaige, Christopher C. “Computational variants of the Lanczos method for the eigenproblem.” IMA Journal of Applied Mathematics 10.3 (1972): 373-381.",
    "crumbs": [
      "API Reference",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#notes",
    "href": "reference/diagonalize.lanczos.html#notes",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "The Lanczos method builds a tridiagonal T from a symmetric A via an orthogonal change-of-basis Q:  Q^T A Q  = T  Unlike other Lanczos implementations (e.g. SciPy’s eigsh), which includes e.g. sophisticated restarting, deflation, and selective-reorthogonalization steps, this method simply executes deg steps of the Lanczos method with the supplied v0 and returns the resulting tridiagonal matrix T.\nRayleigh-Ritz approximations of the eigenvalues of A can be further obtained by diagonalizing T via any symmetric tridiagonal eigenvalue solver, scipy.linalg.eigh_tridiagonal though note unlike eigsh no checking is performed for ‘ghost’ or already converged eigenvalues. To increase the accuracy of these eigenvalue approximation, try increasing orth and deg. Supplying either negative values or values larger than deg for orth will result in full re-orthogonalization, though note the number of matvecs scales linearly with deg and the number of inner-products scales quadratically with orth.",
    "crumbs": [
      "API Reference",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#parameters",
    "href": "reference/diagonalize.lanczos.html#parameters",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nA\nscipy.sparse.linalg.LinearOperator or ndarray or sparray\nSymmetric operator to tridiagonalize.\nrequired\n\n\nv0\nndarray\nInitial vector to orthogonalize against.\nNone\n\n\ndeg\nint\nSize of the Krylov subspace to expand.\nNone\n\n\nrtol\nfloat\nRelative tolerance to consider the invariant subspace as converged.\n1e-8\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against.\n0\n\n\nsparse_mat\nbool\nWhether to output the tridiagonal matrix as a sparse matrix.\nFalse\n\n\nreturn_basis\nbool\nIf True, returns the Krylov basis vectors Q.\nFalse\n\n\ndtype\ndtype\nThe precision dtype to specialize the computation.\nNone",
    "crumbs": [
      "API Reference",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#returns",
    "href": "reference/diagonalize.lanczos.html#returns",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\ntuple\nA tuple (a,b) parameterizing the diagonal and off-diagonal of the tridiagonal matrix. If return_basis=True, the tuple (a,b), Q is returned, where Q represents an orthogonal basis for the degree-deg Krylov subspace.",
    "crumbs": [
      "API Reference",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#see-also",
    "href": "reference/diagonalize.lanczos.html#see-also",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "scipy.linalg.eigh_tridiagonal : Eigenvalue solver for real symmetric tridiagonal matrices. operator.matrix_function : Approximates the action of a matrix function via the Lanczos method.",
    "crumbs": [
      "API Reference",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#references",
    "href": "reference/diagonalize.lanczos.html#references",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "Paige, Christopher C. “Computational variants of the Lanczos method for the eigenproblem.” IMA Journal of Applied Mathematics 10.3 (1972): 373-381.",
    "crumbs": [
      "API Reference",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Functions for estimating the trace of matrices and matrix functions.\n\n\n\ntrace.hutch\nEstimates the trace of a symmetric A or matrix function f(A) via a Girard-Hutchinson estimator.\n\n\ntrace.xtrace\n\n\n\n\n\n\n\nMatrix operators\n\n\n\noperator.matrix_function\nConstructs a LinearOperator approximating the action of the matrix function fun(A).\n\n\n\n\n\n\nRandomized module\n\n\n\nrandom.rademacher\nGenerates random vectors from the rademacher distribution.\n\n\nrandom.normal\nGenerates random vectors from the standard normal distribution.\n\n\n\n\n\n\nDiagonalization methods\n\n\n\ndiagonalize.lanczos\nLanczos method for matrix tridiagonalization.\n\n\n\n\n\n\nMiscellenous functions\n\n\n\nget_include\nReturn the directory that contains the primate’s *.h header files.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#trace",
    "href": "reference/index.html#trace",
    "title": "Function reference",
    "section": "",
    "text": "Functions for estimating the trace of matrices and matrix functions.\n\n\n\ntrace.hutch\nEstimates the trace of a symmetric A or matrix function f(A) via a Girard-Hutchinson estimator.\n\n\ntrace.xtrace",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#operators",
    "href": "reference/index.html#operators",
    "title": "Function reference",
    "section": "",
    "text": "Matrix operators\n\n\n\noperator.matrix_function\nConstructs a LinearOperator approximating the action of the matrix function fun(A).",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#random",
    "href": "reference/index.html#random",
    "title": "Function reference",
    "section": "",
    "text": "Randomized module\n\n\n\nrandom.rademacher\nGenerates random vectors from the rademacher distribution.\n\n\nrandom.normal\nGenerates random vectors from the standard normal distribution.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#diagonalize",
    "href": "reference/index.html#diagonalize",
    "title": "Function reference",
    "section": "",
    "text": "Diagonalization methods\n\n\n\ndiagonalize.lanczos\nLanczos method for matrix tridiagonalization.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#misc",
    "href": "reference/index.html#misc",
    "title": "Function reference",
    "section": "",
    "text": "Miscellenous functions\n\n\n\nget_include\nReturn the directory that contains the primate’s *.h header files.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/random.normal.html",
    "href": "reference/random.normal.html",
    "title": "random.normal",
    "section": "",
    "text": "random.normal(size, rng='splitmix64', seed=-1, dtype=np.float32)\nGenerates random vectors from the standard normal distribution.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsize\nint or tuple\nOutput shape to generate.\nrequired\n\n\nrng\nstr = “splitmix64”\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint = -1\nSeed for the generator. Use -1 to for random (non-deterministic) behavior.\n-1\n\n\ndtype\ndtype = float32\nFloating point dtype for the output. Must be float32 or float64.\nnp.float32\n\n\n\n\n\n\n: Randomly generated matrix of shape size with entries in { -1, 1 }.",
    "crumbs": [
      "API Reference",
      "Random",
      "Normal"
    ]
  },
  {
    "objectID": "reference/random.normal.html#parameters",
    "href": "reference/random.normal.html#parameters",
    "title": "random.normal",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsize\nint or tuple\nOutput shape to generate.\nrequired\n\n\nrng\nstr = “splitmix64”\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint = -1\nSeed for the generator. Use -1 to for random (non-deterministic) behavior.\n-1\n\n\ndtype\ndtype = float32\nFloating point dtype for the output. Must be float32 or float64.\nnp.float32",
    "crumbs": [
      "API Reference",
      "Random",
      "Normal"
    ]
  },
  {
    "objectID": "reference/random.normal.html#returns",
    "href": "reference/random.normal.html#returns",
    "title": "random.normal",
    "section": "",
    "text": ": Randomly generated matrix of shape size with entries in { -1, 1 }.",
    "crumbs": [
      "API Reference",
      "Random",
      "Normal"
    ]
  }
]