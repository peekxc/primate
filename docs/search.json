[
  {
    "objectID": "reference/get_include.html",
    "href": "reference/get_include.html",
    "title": "get_include",
    "section": "",
    "text": "get_include\nget_include()\nReturn the directory that contains the primate’s *.h header files.\nExtension modules that need to compile against primate should use this function to locate the appropriate include directory.\nNotes: When using distutils, for example in setup.py: python     import primate     ...     Extension('extension_name', ..., include_dirs=[primate.get_include()])     ... Or with meson-python, for example in meson.build: meson     ...     run_command(py, ['-c', 'import primate; print(primate.get_include())', check : true).stdout().strip()     ..."
  },
  {
    "objectID": "reference/primate.trace.sl_trace.html",
    "href": "reference/primate.trace.sl_trace.html",
    "title": "sl_trace",
    "section": "",
    "text": "trace.sl_trace(A, fun='identity', maxiter=200, deg=20, atol=None, rtol=None, stop=['confidence', 'change'], orth=0, confidence=0.95, pdf='rademacher', rng='lcg', seed=-1, num_threads=0, verbose=False, info=False, plot=False, **kwargs)\nEstimates the trace of a matrix function f(A) using stochastic Lanczos quadrature (SLQ).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nA\nndarray, sparray, or LinearOperator\nreal symmetric operator.\nrequired\n\n\nfun\nstr or typing.Callable\nreal-valued function defined on the spectrum of A.\n= \"identity\"\n\n\nmaxiter\nint\nMaximum number of random vectors to sample for the trace estimate.\n= 10\n\n\ndeg\nint\nDegree of the quadrature approximation.\n20\n\n\natol\nfloat\nAbsolute tolerance to signal convergence for early-stopping. See details.\n= None\n\n\nrtol\nfloat\nRelative tolerance to signal convergence for early-stopping. See details.\n= 1e-2\n\n\nstop\nstr\nEarly-stopping criteria to test estimator convergence. See details.\n= \"confidence\"\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against when building the Krylov basis.\n0\n\n\nconfidence\nfloat\nConfidence level to Only used when stop = “confidence”.\n= 0.95\n\n\npdf\n‘rademacher’, ‘normal’\nChoice of zero-centered distribution to sample random vectors from.\n'rademacher'\n\n\nrng\nstr\nRandom number generator to use. Defaults to PCG64 generator.\n= \"pcg\"\n\n\nseed\nint\nSeed to initialize the entropy source. Use non-negative integers for reproducibility.\n= -1\n\n\nnum_threads\nint\nNumber of threads to use to parallelize the computation. Use values &lt;= 0 to maximize the number of threads.\n0\n\n\nplot\nbool\nIf true, plots the samples of the trace estimate along with their convergence characteristics.\n= False\n\n\ninfo\nbool\nIf True, returns a dictionary containing all relevant information about the computation.\nFalse\n\n\nkwargs\ndict\nadditional key-values to parameterize the chosen function ‘fun’.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nEstimate of the trace of the matrix function f(A).\n\n\n(dict, optional)\nIf ‘info = True’, additional information about the computation.\n\n\n\n\n\n\nlanczos : the lanczos algorithm.\n\n\n\n[1] Ubaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099.",
    "crumbs": [
      "API Reference",
      "Trace",
      "SL Trace"
    ]
  },
  {
    "objectID": "reference/primate.trace.sl_trace.html#parameters",
    "href": "reference/primate.trace.sl_trace.html#parameters",
    "title": "sl_trace",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nA\nndarray, sparray, or LinearOperator\nreal symmetric operator.\nrequired\n\n\nfun\nstr or typing.Callable\nreal-valued function defined on the spectrum of A.\n= \"identity\"\n\n\nmaxiter\nint\nMaximum number of random vectors to sample for the trace estimate.\n= 10\n\n\ndeg\nint\nDegree of the quadrature approximation.\n20\n\n\natol\nfloat\nAbsolute tolerance to signal convergence for early-stopping. See details.\n= None\n\n\nrtol\nfloat\nRelative tolerance to signal convergence for early-stopping. See details.\n= 1e-2\n\n\nstop\nstr\nEarly-stopping criteria to test estimator convergence. See details.\n= \"confidence\"\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against when building the Krylov basis.\n0\n\n\nconfidence\nfloat\nConfidence level to Only used when stop = “confidence”.\n= 0.95\n\n\npdf\n‘rademacher’, ‘normal’\nChoice of zero-centered distribution to sample random vectors from.\n'rademacher'\n\n\nrng\nstr\nRandom number generator to use. Defaults to PCG64 generator.\n= \"pcg\"\n\n\nseed\nint\nSeed to initialize the entropy source. Use non-negative integers for reproducibility.\n= -1\n\n\nnum_threads\nint\nNumber of threads to use to parallelize the computation. Use values &lt;= 0 to maximize the number of threads.\n0\n\n\nplot\nbool\nIf true, plots the samples of the trace estimate along with their convergence characteristics.\n= False\n\n\ninfo\nbool\nIf True, returns a dictionary containing all relevant information about the computation.\nFalse\n\n\nkwargs\ndict\nadditional key-values to parameterize the chosen function ‘fun’.\n{}",
    "crumbs": [
      "API Reference",
      "Trace",
      "SL Trace"
    ]
  },
  {
    "objectID": "reference/primate.trace.sl_trace.html#returns",
    "href": "reference/primate.trace.sl_trace.html#returns",
    "title": "sl_trace",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nfloat\nEstimate of the trace of the matrix function f(A).\n\n\n(dict, optional)\nIf ‘info = True’, additional information about the computation.",
    "crumbs": [
      "API Reference",
      "Trace",
      "SL Trace"
    ]
  },
  {
    "objectID": "reference/primate.trace.sl_trace.html#see-also",
    "href": "reference/primate.trace.sl_trace.html#see-also",
    "title": "sl_trace",
    "section": "",
    "text": "lanczos : the lanczos algorithm.",
    "crumbs": [
      "API Reference",
      "Trace",
      "SL Trace"
    ]
  },
  {
    "objectID": "reference/primate.trace.sl_trace.html#reference",
    "href": "reference/primate.trace.sl_trace.html#reference",
    "title": "sl_trace",
    "section": "",
    "text": "[1] Ubaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099.",
    "crumbs": [
      "API Reference",
      "Trace",
      "SL Trace"
    ]
  },
  {
    "objectID": "reference/random.normal.html",
    "href": "reference/random.normal.html",
    "title": "random.normal",
    "section": "",
    "text": "random.normal(size, rng='splitmix64', seed=-1, dtype=np.float32)\nGenerates random vectors from the rademacher distribution.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsize\nint or tuple\nOutput shape to generate.\nrequired\n\n\nrng\nstr = “splitmix64”\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint = -1\nSeed for the generator. Use -1 to for random (non-deterministic) behavior.\n-1\n\n\ndtype\ndtype = float32\nFloating point dtype for the output. Must be float32 or float64.\nnp.float32",
    "crumbs": [
      "API Reference",
      "Random",
      "Normal"
    ]
  },
  {
    "objectID": "reference/random.normal.html#parameters",
    "href": "reference/random.normal.html#parameters",
    "title": "random.normal",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsize\nint or tuple\nOutput shape to generate.\nrequired\n\n\nrng\nstr = “splitmix64”\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint = -1\nSeed for the generator. Use -1 to for random (non-deterministic) behavior.\n-1\n\n\ndtype\ndtype = float32\nFloating point dtype for the output. Must be float32 or float64.\nnp.float32",
    "crumbs": [
      "API Reference",
      "Random",
      "Normal"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html",
    "href": "reference/diagonalize.lanczos.html",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "diagonalize.lanczos(A, v0=None, deg=None, rtol=1e-08, orth=0, sparse_mat=False, return_basis=False, seed=None, dtype=None)\nLanczos method for matrix tridiagonalization.\nThis function implements Paiges A27 variant (1) of the Lanczos method for tridiagonalizing linear operators,\nwith additional modifications to support varying degrees of re-orthogonalization.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nA\nscipy.sparse.linalg.LinearOperator or ndarray or sparray\nSymmetric operator to tridiagonalize.\nrequired\n\n\nv0\nndarray\nInitial vector to orthogonalize against.\nNone\n\n\ndeg\nint\nSize of the Krylov subspace to expand.\nNone\n\n\nrtol\nfloat\nRelative tolerance to consider the invariant subspace as converged.\n1e-8\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against.\n0\n\n\nsparse_mat\nbool\nWhether to output the tridiagonal matrix as a sparse matrix.\nFalse\n\n\nreturn_basis\nbool\nWhether to return the orth + 2 Lanczos vectors.\nFalse\n\n\ndtype\ndtype\nThe precision dtype to specialize the computation.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nA tuple (a,b) parameterizing the diagonal and off-diagonal of the tridiagonal matrix. If return_basis is\n\n\n\nTrue, then the tuple (a,b), Q is returned, where Q represents orthogonal basis for the Krylov subspace.\n\n\n\n\n\n\n\nscipy.linalg.eigh_tridiagonal : Eigenvalue solver for real symmetric tridiagonal matrices. operator.matrix_function : Approximates the action of a matrix function via the Lanczos method.\n\n\n\nNo checking for performed for ghost, converged, or ‘locked’ eigenvalues. To increase the accuracy of the eigenvalue approximation, increase orth and deg. Note the number of matvecs with A scales linearly with deg and the number of inner-products scales quadratically with orth.\nSupplying either negative values or values larger than deg for orth will result in full re-orthogonalization.\n\n\n\n\nPaige, Christopher C. “Computational variants of the Lanczos method for the eigenproblem.” IMA Journal of Applied Mathematics 10.3 (1972): 373-381.",
    "crumbs": [
      "API Reference",
      "Diagonalize",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#parameters",
    "href": "reference/diagonalize.lanczos.html#parameters",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nA\nscipy.sparse.linalg.LinearOperator or ndarray or sparray\nSymmetric operator to tridiagonalize.\nrequired\n\n\nv0\nndarray\nInitial vector to orthogonalize against.\nNone\n\n\ndeg\nint\nSize of the Krylov subspace to expand.\nNone\n\n\nrtol\nfloat\nRelative tolerance to consider the invariant subspace as converged.\n1e-8\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against.\n0\n\n\nsparse_mat\nbool\nWhether to output the tridiagonal matrix as a sparse matrix.\nFalse\n\n\nreturn_basis\nbool\nWhether to return the orth + 2 Lanczos vectors.\nFalse\n\n\ndtype\ndtype\nThe precision dtype to specialize the computation.\nNone",
    "crumbs": [
      "API Reference",
      "Diagonalize",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#description",
    "href": "reference/diagonalize.lanczos.html#description",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "This function implements the Lanczos method, or as Lanczos called it, the method of minimized iterations.",
    "crumbs": [
      "API Reference",
      "Diagonalize",
      "Lanczos"
    ]
  },
  {
    "objectID": "imate_compare.html",
    "href": "imate_compare.html",
    "title": "Comparison to imate",
    "section": "",
    "text": "primate’s namesake (and some of the original code1) was inspired from the (excellent) imate package, prompting questions about their differences. In general, primate was developed with slightly different goals in mind than imate, most of which have to do with things like integrability, extensibility, and choice of FFI / build system.\nNotable differences between the two packages include:\nOne motivation for developing primate was to modularize and streamline access to Lanczos-based methods, which is achieved through the use of things like function templates, type erasure, and header-only definitions. These modifications not only simplify access from user (i.e. dependent) packages, but they enable native support for arbitrary classes adhering to the LinearOperator concept. For more details on this, see the integration guides.",
    "crumbs": [
      "Basics",
      "Comparison to *imate*"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Package overview",
    "section": "",
    "text": "primate, short for Probabalistic Implicit Matrix Trace Estimator, is a Python package that provides estimators of quantities derived from matrix functions; that is, matrices parameterized by functions:\nf(A) \\triangleq U f(\\Lambda) U^{\\intercal}, \\quad \\quad f : [a,b] \\to \\mathbb{R}\nEstimator approximations are obtained via the Lanczos1 and stochastic Lanczos quadrature2 methods, which are well-suited for sparse or structured operators supporting fast v \\mapsto Av actions.\nNotable features of primate include:\nprimate was partially inspired by the imate package—for a comparison of the two, see here.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#compilation-development",
    "href": "index.html#compilation-development",
    "title": "Package overview",
    "section": "",
    "text": "primate relies on BLAS libraries\n\npipx run cibuildwheel –platform linux",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "integration/cpp_integration.html",
    "href": "integration/cpp_integration.html",
    "title": "C++ Integration",
    "section": "",
    "text": "To get started calling any matrix-free function provided by primate, such sl_trace or lanczos, simply pass any type with a .shape() and .matvec() member functions defined like so:\nIt’s up to you to ensure shape() yields the correct size; primate will supply vectors to input of size .shape().second (number of columns) and guarantees the pointer to the output will be at least shape().first (number of rows), no more.",
    "crumbs": [
      "Integration Guide",
      "Usage from C++"
    ]
  },
  {
    "objectID": "integration/cpp_integration.html#the-linearoperator-concept",
    "href": "integration/cpp_integration.html#the-linearoperator-concept",
    "title": "C++ Integration",
    "section": "The LinearOperator concept",
    "text": "The LinearOperator concept\nprimate’s generic API is enabled through C++20 concepts. Thus, the more exact statement is that any type respecting the LinearOperator concept shown below can be passed:\nusing FP = std::floating_point; \ntemplate &lt; typename T, FP float_t = typename T::value_type &gt;\nconcept LinearOperator = requires(T A, const float_t* v, float_t* o) {\n  { A.matvec(v, o) }; // o = A v\n  { A.shape() } -&gt; std::convertible_to&lt; std::pair&lt; size_t, size_t &gt; &gt;;\n};\nAn instance A of type T is said to support the LinearOperator concept if it has:\n\nA method Av \\mapsto o, with signature A.matvec(const float_t* v, float_t* o)\nA method yielding (\\mathrm{card}(o), \\mathrm{card}(v)), with signatureA.shape() -&gt; pair&lt; ... &gt;\n\nshape() should yield a pair (n,m) representing the sizes of the output and input vectors, respectively. This corresponds to the number of rows and columns in the matrix setting.",
    "crumbs": [
      "Integration Guide",
      "Usage from C++"
    ]
  },
  {
    "objectID": "integration/cpp_integration.html#other-concepts",
    "href": "integration/cpp_integration.html#other-concepts",
    "title": "C++ Integration",
    "section": "Other Concepts",
    "text": "Other Concepts\nDepending on the problem at hand, the supplied operator may need to meet other constraints. Here’s a short list additional operator concepts:\n\n\n\n\n\n\n\n\n\nConcept\nSupports\nSignature\nRequires\n\n\n\n\nLinearOperator\nA v \\mapsto o\nA.matvec(v, o)\nNA\n\n\nAdjointOperator\nA^T v \\mapsto o\nA.rmatvec(v, o)\nLinearOperator\n\n\nAdditiveOperator\no \\gets o + \\alpha Av\nA.matvec_add(v, alpha, o)\nLinearOperator\n\n\nAffineOperator\nSets t s.t. A + tB\nA.set_parameter(t)\nLinearOperator\n\n\n\nRespecting these constraints is opt-in: if your operator is symmetric and you only need access to the Lanczos method, then any satisfying the LinearOperator concept is sufficient.",
    "crumbs": [
      "Integration Guide",
      "Usage from C++"
    ]
  },
  {
    "objectID": "guide/slq_guide.html",
    "href": "guide/slq_guide.html",
    "title": "SLQ Trace guide",
    "section": "",
    "text": "primate offers an extensible implementation of the stochastic Lanczos method (SLQ). There are many algorithms named the “stochastic Lanczos quadrature” in the literature; though each is typically related, they often have distinct goals. Pseudocode for a generic form of SLQ is given below:\nThis guide walks through the SLQ method implemented in primate, which can be specialized for different purposes.",
    "crumbs": [
      "User Guide",
      "The SLQ method"
    ]
  },
  {
    "objectID": "guide/slq_guide.html#slq-as-a-function-template",
    "href": "guide/slq_guide.html#slq-as-a-function-template",
    "title": "SLQ Trace guide",
    "section": "SLQ as a function template",
    "text": "SLQ as a function template\nBelow is the full signature of the SLQ function template:\n// Stochastic Lanczos quadrature method\ntemplate&lt; std::floating_point F, LinearOperator Matrix, ThreadSafeRBG RBG &gt;\nvoid slq (\n  const Matrix& A,                    // Any *LinearOperator*\n  const function&lt; F(int,F*,F*) &gt;& f,  // Generic function\n  const function&lt; bool(int) &gt;& stop,  // Early-stop function\n  const int nv,                       // Num. of sample vectors\n  const Distribution dist,            // Sample vector distribution\n  RBG& rng,                           // Random bit generator\n  const int lanczos_degree,           // Krylov subspace degree\n  const F lanczos_rtol,               // Lanczos residual tolerance\n  const int orth,                     // Add. vectors to orthogonalize\n  const int ncv,                      // Num. of Lanczos vectors\n  const int num_threads,              // # threads to allocate \n  const int seed                      // Seed for RNG \n)\nMany of the runtime arguments are documented in the lanczos or sl_trace docs; the compile-time (template) parameters are:\n\nThe floating point type (e.g. float, double, long double)\nThe operator type (e.g. Eigen::MatrixXf, torch::Tensor, LinOp)\nThe multi-threaded random number generator (e.g. ThreadedRNG64)\n\nNote any type combination satisfying these concepts (e.g. std::floating_point, LinearOperator) generates a function specialized of said types at compile-time—this is known as template instantiation.",
    "crumbs": [
      "User Guide",
      "The SLQ method"
    ]
  },
  {
    "objectID": "guide/slq_guide.html#generality-via-function-passing",
    "href": "guide/slq_guide.html#generality-via-function-passing",
    "title": "SLQ Trace guide",
    "section": "Generality via function passing",
    "text": "Generality via function passing\nGiven a valid set of parameters, the main body of the SLQ looks something like this:\n  bool stop_flag = false;\n  #pragma omp parallel shared(stop_flag)\n  {\n    // &lt; allocations for Q, alpha, beta, etc. &gt; \n    int tid = omp_get_thread_num(); // thread-id \n    \n    #pragma omp for\n    for (i = 0; i &lt; nv; ++i){\n      if (stop_flag){ continue; }\n      generate_isotropic&lt; F &gt;(...); // populates q\n      lanczos_recurrence&lt; F &gt;(...); // populates alpha + beta\n      lanczos_quadrature&lt; F &gt;(...); // populates nodes + weights\n      f(i, q, Q, nodes, weights);   // Run user-supplied function \n      #pragma omp critical\n      {\n        stop_flag = stop(i);        // Checks for early-stopping\n      }\n    } // end for\n  } // end parallel \nThere are two functions that can be used for generalizing SLQ for different purposes.\nThe first generic function f can read, save, or modify the information available from the iteration index i, the isotropic vector q, the Lanczos vectors Q, and/or the quadrature information nodes, weights. Note this function is run in the parallel section.\nThe second is a boolean-valued function stop which can be used to stop the iteration early, for example if convergence has been achieved according to some rule. Since this is run in the critical section, it is called sequentially.",
    "crumbs": [
      "User Guide",
      "The SLQ method"
    ]
  },
  {
    "objectID": "guide/slq_guide.html#using-slq-to-estimate-mathrmtrfa",
    "href": "guide/slq_guide.html#using-slq-to-estimate-mathrmtrfa",
    "title": "SLQ Trace guide",
    "section": "Using SLQ to estimate \\mathrm{tr}(f(A))",
    "text": "Using SLQ to estimate \\mathrm{tr}(f(A))\nThe SLQ method is often used to estimate the trace of an arbitrary matrix function:\n \\mathrm{tr}(f(A)), \\quad \\text{ where } f(A) = U f(\\Lambda) U^T \nIt’s has been shown1 that the information obtained by the Lanczos method is sufficient to obtained a Gaussian quadrature approximation of the empirical spectral measure of A. By sampling zero-mean vectors satisfying \\mathbb{E}[v v^T] = I, one can obtain estimates of the trace above: \\operatorname{tr}(f(A)) \\approx \\frac{n}{\\mathrm{n}_{\\mathrm{v}}} \\sum_{l=1}^{\\mathrm{n}_{\\mathrm{v}}}\\left(\\sum_{k=0}^m\\left(\\tau_k^{(l)}\\right)^2 f\\left(\\theta_k^{(l)}\\right)\\right)\nIt turns out averaging these trace estimates yields unbiased, Girard-Hutchinson estimator of the trace. To see why this estimator is unbiased, note that:  \\mathtt{tr}(A) = \\mathbb{E}[v^T A v] \\approx \\frac{1}{n_v}\\sum\\limits_{i=1}^{n_v} v_i^\\top A v_i \nThus, all we need to do to estimate the trace of a matrix function is multiply and sum the quadrature nodes and weights output by SLQ.",
    "crumbs": [
      "User Guide",
      "The SLQ method"
    ]
  },
  {
    "objectID": "guide/slq_guide.html#sl_trace-method",
    "href": "guide/slq_guide.html#sl_trace-method",
    "title": "SLQ Trace guide",
    "section": "sl_trace method",
    "text": "sl_trace method\nTo see how these formulas are actually implemented with the generic SLQ implementation, here’s an abbreviated form of the sl_trace function implemented by primate:\ntemplate&lt; std::floating_point F, LinearOperator Matrix, ThreadSafeRBG RBG &gt;\nvoid sl_trace(\n  const Matrix& A, const std::function&lt; F(F) &gt; sf, RBG& rbg, \n  const int nv, const int dist, const int engine_id, const int seed,\n  const int deg, const float lanczos_rtol, const int orth, const int ncv,\n  const F atol, const F rtol\n  F* estimates\n){  \n  using VectorF = Eigen::Array&lt; F, Dynamic, 1&gt;;\n\n  // Parameterize the trace function (runs in parallel)\n  auto trace_f = [&](int i, F* q, F* Q, F* nodes, F* weights){\n    Map&lt; VectorF &gt; nodes_v(nodes, deg, 1);     // no-op\n    Map&lt; VectorF &gt; weights_v(weights, deg, 1); // no-op\n    nodes_v.unaryExpr(sf);\n    estimates[i] = (nodes_v * weights_v).sum();\n  };\n  \n  // Convergence checking like scipy.integrate.quadrature\n  int n = 0;\n  F mu_est = 0.0, mu_pre = 0.0;\n  const auto early_stop = [&](int i) -&gt; bool {\n    ++n; // Number of estimates\n    mu_est = (1.0 / F(n)) * (estimates[i] + (n - 1) * mu_pre); \n    bool atol_check = abs(mu_est - mu_pre) &lt;= atol;\n    bool rtol_check = abs(mu_est - mu_pre) / mu_est &lt;= rtol; \n    mu_pre = mu_est; \n    return atol_check || rtol_check;\n  };\n\n  // Execute the stochastic Lanczos quadrature with the trace function \n  slq&lt; float &gt;(A, trace_f, early_stop, ...);\n}\nAs before, two functions are used to parameterize the slq method.\nThe first (trace_f) applies an arbitrary spectral function sf to the Rayleigh-Ritz values obtained by the Lanczos tridiagonalization of A(or equivalently, the nodes of the Gaussian quadrature). These are the \\theta’s in the pseudocode above. When multiplied by the weights of the quadrature, the corresponding sum forms an estimate of the trace of the matrix function.\nThe second function early_stop is used to check for convergence of the estimator. First, it uses the trace estimate x_n to update the sample mean \\mu_n via the formula:\n \\mu_n = n^{-1} [x_n + (n - 1)\\mu_{n-1}] \nThen, much in the same way the quadrature function from scipy.integrate approximates a definite integral, it checks for convergence using the absolute and relative tolerances supplied by the user. Returning true signals convergence, stopping the iteration early.",
    "crumbs": [
      "User Guide",
      "The SLQ method"
    ]
  },
  {
    "objectID": "guide/slq_guide.html#references",
    "href": "guide/slq_guide.html#references",
    "title": "SLQ Trace guide",
    "section": "References",
    "text": "References",
    "crumbs": [
      "User Guide",
      "The SLQ method"
    ]
  },
  {
    "objectID": "integration/python_integration.html",
    "href": "integration/python_integration.html",
    "title": "Python Integration",
    "section": "",
    "text": "To demonstrate the SLQ method in Python, we start with a simple symmetric matrix A \\in \\mathbb{R}^{n \\times n}.\n\nimport numpy as np\nfrom primate.random import symmetric\nA = symmetric(150, psd = True)\n\nThis generates a random positive semi-definite matrix with eigenvalues in the interval [0, 1].\n\nfrom primate.trace import sl_trace\ntrace_estimate = sl_trace(A)\nprint(A.trace()) \nprint(trace_estimate)\n\n78.57591588717017\n78.428444\n\n\n\n# tr_est = np.mean(estimates)\n# print(f\"Error: {abs(tr_est - A.trace()):.5}\")\n# print(f\"Samples std. deviation: {estimates.std(ddof=1)}\")\n# print(f\"Estimator standard error: {estimates.std(ddof=1)/np.sqrt(len(estimates))}\")",
    "crumbs": [
      "Integration Guide",
      "Usage from Python"
    ]
  },
  {
    "objectID": "integration/pybind11_integration.html",
    "href": "integration/pybind11_integration.html",
    "title": "pybind11 Integration",
    "section": "",
    "text": "If you’re using pybind11, you can easily incorporate your own custom linear operator / matrix function pair using primates binding headers.",
    "crumbs": [
      "Integration Guide",
      "Integrating with pybind11"
    ]
  },
  {
    "objectID": "integration/pybind11_integration.html#example-log-determinant",
    "href": "integration/pybind11_integration.html#example-log-determinant",
    "title": "pybind11 Integration",
    "section": "Example: Log determinant",
    "text": "Example: Log determinant\nFor explanatory purposes, the following code outline how to call the trace estimator to compute the log determinant using a custom user-implemented operator LinOp:\n#include &lt;cmath&gt;                              // std::log\n#include &lt;_linear_operator/linear_operator.h&gt; // LinearOperator\n#include &lt;_lanczos/lanczos.h&gt;                 // sl_trace\n#include \"LinOp.h\"                            // custom class\n\nvoid slq_log_det(LinOp A, ...){ \n  static_assert(LinearOperator&lt; LinOp &gt;);  // Constraint check\n  const auto matrix_func = std::log;       // any invocable\n  auto rbg = ThreadedRNG64();              // default RNG\n  auto estimates = vector&lt; float &gt;(n, 0);  // output estimates\n  sl_trace&lt; float &gt;(                       // specific precision\n    A, matrix_func, rbg,                   // main arguments\n    ...,                                   // other inputs \n    estimates.data()                       // output \n  );\n}",
    "crumbs": [
      "Integration Guide",
      "Integrating with pybind11"
    ]
  },
  {
    "objectID": "install.html",
    "href": "install.html",
    "title": "Installation",
    "section": "",
    "text": "primate is a standard PEP-517 package, and thus can be installed via pip:\npip install &lt; primate source directory &gt;\nCurrently the package must be built from source via cloning the repository. PYPI support is planned.",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "reference/random.rademacher.html",
    "href": "reference/random.rademacher.html",
    "title": "random.rademacher",
    "section": "",
    "text": "random.rademacher(size, rng='splitmix64', seed=-1, dtype=np.float32)\nGenerates random vectors from the rademacher distribution.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsize\nint or tuple\nOutput shape to generate.\nrequired\n\n\nrng\nstr = “splitmix64”\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint = -1\nSeed for the generator. Use -1 to for random (non-deterministic) behavior.\n-1\n\n\ndtype\ndtype = float32\nFloating point dtype for the output. Must be float32 or float64.\nnp.float32",
    "crumbs": [
      "API Reference",
      "Random",
      "Rademacher"
    ]
  },
  {
    "objectID": "reference/random.rademacher.html#parameters",
    "href": "reference/random.rademacher.html#parameters",
    "title": "random.rademacher",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsize\nint or tuple\nOutput shape to generate.\nrequired\n\n\nrng\nstr = “splitmix64”\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint = -1\nSeed for the generator. Use -1 to for random (non-deterministic) behavior.\n-1\n\n\ndtype\ndtype = float32\nFloating point dtype for the output. Must be float32 or float64.\nnp.float32",
    "crumbs": [
      "API Reference",
      "Random",
      "Rademacher"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Functions for estimating the trace of matrices and matrix functions.\n\n\n\ntrace\n\n\n\n\n\n\n\nRandomized module\n\n\n\nrandom.rademacher\nGenerates random vectors from the rademacher distribution.\n\n\nrandom.normal\nGenerates random vectors from the rademacher distribution.\n\n\n\n\n\n\nDiagonalization methods\n\n\n\ndiagonalize.lanczos\nLanczos method for matrix tridiagonalization.\n\n\n\n\n\n\nMiscellenous functions\n\n\n\nget_include\nReturn the directory that contains the primate’s *.h header files.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#trace",
    "href": "reference/index.html#trace",
    "title": "Function reference",
    "section": "",
    "text": "Functions for estimating the trace of matrices and matrix functions.\n\n\n\ntrace",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#random",
    "href": "reference/index.html#random",
    "title": "Function reference",
    "section": "",
    "text": "Randomized module\n\n\n\nrandom.rademacher\nGenerates random vectors from the rademacher distribution.\n\n\nrandom.normal\nGenerates random vectors from the rademacher distribution.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#diagonalize",
    "href": "reference/index.html#diagonalize",
    "title": "Function reference",
    "section": "",
    "text": "Diagonalization methods\n\n\n\ndiagonalize.lanczos\nLanczos method for matrix tridiagonalization.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#misc",
    "href": "reference/index.html#misc",
    "title": "Function reference",
    "section": "",
    "text": "Miscellenous functions\n\n\n\nget_include\nReturn the directory that contains the primate’s *.h header files.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/primate.trace.sl_gauss.html",
    "href": "reference/primate.trace.sl_gauss.html",
    "title": "sl_gauss",
    "section": "",
    "text": "trace.sl_gauss(A, n=150, deg=20, pdf='rademacher', rng='pcg', seed=-1, orth=0, num_threads=0)\nStochastic Gaussian quadrature approximation.\nComputes a set of sample nodes and weights for the degree-k orthogonal polynomial approximating the cumulative spectral measure of A. This function can be used to approximate the spectral density of A, or to approximate the spectral sum of any function applied to the spectrum of A.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nA\nndarray, sparray, or LinearOperator\nreal symmetric operator.\nrequired\n\n\nn\nint\nNumber of random vectors to sample for the quadrature estimate.\n150\n\n\ndeg\nint\nDegree of the quadrature approximation.\n20\n\n\nrng\n‘splitmix64’, ’xoshiro256**‘, ’pcg64’, ‘lcg64’, ‘mt64’\nRandom number generator to use (PCG64 by default).\n'splitmix64'\n\n\nseed\nint\nSeed to initialize the rng entropy source. Set seed &gt; -1 for reproducibility.\n-1\n\n\npdf\n‘rademacher’, ‘normal’\nChoice of zero-centered distribution to sample random vectors from.\n'rademacher'\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against when building the Krylov basis.\n0\n\n\nnum_threads\nint\nNumber of threads to use to parallelize the computation. Setting num_threads &lt; 1 to let OpenMP decide.\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nEstimate of the trace of the matrix function f(A).\n\n\n(dict, optional)\nIf ‘info = True’, additional information about the computation.",
    "crumbs": [
      "API Reference",
      "Trace",
      "SL Gauss"
    ]
  },
  {
    "objectID": "reference/trace.html",
    "href": "reference/trace.html",
    "title": "trace",
    "section": "",
    "text": "trace\n\n\n\n\n\nName\nDescription\n\n\n\n\nhutch\nEstimates the trace of a matrix A or matrix function f(A) via a Girard-Hutchinson estimator.\n\n\nsl_gauss\nStochastic Gaussian quadrature approximation."
  },
  {
    "objectID": "reference/trace.html#functions",
    "href": "reference/trace.html#functions",
    "title": "trace",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nhutch\nEstimates the trace of a matrix A or matrix function f(A) via a Girard-Hutchinson estimator.\n\n\nsl_gauss\nStochastic Gaussian quadrature approximation."
  },
  {
    "objectID": "basic/integration.html",
    "href": "basic/integration.html",
    "title": "Integration",
    "section": "",
    "text": "primate supports a variety of matrix-types of the box, including numpy ndarray’s, compressed sparse matrices (a lá SciPy), and LinearOperators—the latter enables the use of matrix free operators.\nOutside of the natively types above, the basic requirements for any operator A to be used with e.g. the Lanczos method in primate are:\n\nA method A.matvec(input: ndarray) -&gt; ndarray implementing v \\mapsto Av\nAn attribute A.shape -&gt; tuple[int, int] giving the output/input dimensions of A\n\n\n\nHere’s an example of a simple operator representing a Diagonal matrix, which inherits a .matvec() method by following the subclassing rules of SciPy’s LinearOperator:\nimport numpy as np \nfrom numpy.typing import ArrayLike\nfrom scipy.sparse.linalg import LinearOperator \n\nclass DiagonalOp(LinearOperator):\n  diag: np.ndarray = None\n  \n  def __init__(self, d: ArrayLike, dtype = None):\n    self.diag = np.array(d)\n    self.shape = (len(d), len(d))\n    self.dtype = np.dtype('float32') if dtype is None else dtype\n\n  def _matvec(self, x: ArrayLike) -&gt; np.ndarray:\n    out = self.diag * np.ravel(x)\n    return out.reshape(x.shape)",
    "crumbs": [
      "Basics",
      "Integration"
    ]
  },
  {
    "objectID": "theory/lanczos.html",
    "href": "theory/lanczos.html",
    "title": "The Lanczos method",
    "section": "",
    "text": "Whether for simplifying the representation of complicated systems, characterizing the asymptotic behavior of differential equations, or even just fitting polynomials to data via least-squares, decomposing linear operators has had significant use in many areas of sciences and engineering.\nCentral to operator theory is the spectral theorem, which provides conditions under which a linear operator A : \\mathbb{R}^n \\to \\mathbb{R}^n can be diagonalized in terms of its eigenvalues and eigenvectors:  A = U \\Lambda U^{-1}\nIn the case where A is symmetric, the eigen-decomposition is not only guarenteed to exist, but its canonical form may be obtained via orthogonal diagonalization. Such matrices are among the most commonly encountered matrices in applications.\nIn 1950, Cornelius Lanczos studied an alternative means of decomposition via tridiagonalization:   AQ = Q T \\quad \\Leftrightarrow \\quad Q^T A Q = T  The algorithm by which one produces such a T is now known as the Lanczos method. Despite its age, the Lanczos method remains the standard algorithm1 both for computing eigensets and solving linear systems in the large-scale regime. Having intrinsic connections to the conjugate gradient method, the theory of orthogonal polynomials, and Gaussian quadrature, it is one of the most important numerical methods of all time—indeed, an IEEE guest editorial places it among the top 10 most influential algorithms of the 20th century.\nAs the Lanczos method lies at the heart of primate’s design, this post introduces it, with a focus on its motivating principles and computational details. For its API usage, see the lanczos page.",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "theory/lanczos.html#lanczos-on-a-bumper-sticker",
    "href": "theory/lanczos.html#lanczos-on-a-bumper-sticker",
    "title": "The Lanczos method",
    "section": "Lanczos on a bumper sticker",
    "text": "Lanczos on a bumper sticker\nGiven any non-zero v \\in \\mathbb{R}^n, Lanczos generates a Krylov subspace via successive powers of A:\n\n\\mathcal{K}(A, v) \\triangleq \\{ \\, A^{0} v, A^{1}v, A^{2}v, \\dots, A^{n}v \\, \\}\n\nThese vectors are independent, so orthogonalizing them not only yields an orthonormal basis for \\mathcal{K}(A, v) but also a change-of-basis matrix Q, allowing A to be represented by a new matrix T:\n\n\\begin{align*}\nK &= [\\, v \\mid Av \\mid A^2 v \\mid \\dots \\mid A^{n-1}v \\,] && \\\\\nQ &= [\\, q_1, q_2, \\dots, q \\,] \\gets \\mathrm{qr}(K) &&  \\\\\nT &= Q^T A Q &&\n\\end{align*}\n\nIt turns out that since A is symmetric, T is guaranteed to have a symmetric tridiagonal structure:\n\nT = \\mathrm{tridiag}\\Bigg(\n\\begin{array}{ccccccccc}\n& \\beta_2 & & \\beta_3 & & \\cdot & & \\beta_n & \\\\\n\\alpha_1 & & \\alpha_2 & & \\cdot & & \\cdot & & \\alpha_n \\\\\n& \\beta_2 & & \\beta_3 & & \\cdot & & \\beta_n &\n\\end{array}\n\\Bigg)\n\n\n\nSince \\mathrm{range}(Q) = \\mathcal{K}(A, v), the change-of-basis A \\mapsto Q^{-1} A Q is in fact a similarity transform, which are known to be equivalence relations on \\mathcal{S}^n—thus we can obtain \\Lambda by diagonalizing T:\n T = Y \\Lambda Y^T  \nAs T is quite structured, it can be easily diagonalized, thus we have effectively solved the eigenvalue problem. To quote the Lanczos introduction from Parlett, could anything be more simple?",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "theory/lanczos.html#the-lanczos-iteration",
    "href": "theory/lanczos.html#the-lanczos-iteration",
    "title": "The Lanczos method",
    "section": "The Lanczos Iteration",
    "text": "The Lanczos Iteration\nThe Lanczos method exposes the spectrum of A by successively projecting onto Krylov subspaces. That is, given a symmetric A \\in \\mathbb{R}^{n \\times n} with eigenvalues \\lambda_1 \\geq \\lambda_2 &gt; \\dots \\geq \\lambda_r &gt; 0 and a vector v \\in \\mathbb{R} \\setminus \\{0\\}, the order-j Krylov subspaces / Krylov matrices of the pair (A, v) are given by:\n\n\\mathcal{K}_j(A, v) := \\mathrm{span}\\{ v, Av, A^2 v, \\dots, A^{j-1}v \\}, \\quad \\quad K_j(A, v) = [ v \\mid Av \\mid A^2 v \\mid \\dots \\mid A^{j-1}]\n\nKrylov subspaces arise naturally from using the minimal polynomial of A to express A^{-1} in terms of powers of A: if A is nonsingular and its minimal polynomial has degree m, then A^{-1}v \\in K_m(A, v) and K_m(A, v) is an invariant subspace.\nThe spectral theorem implies that since A is symmetric, it is orthogonally diagonalizable: thus, \\Lambda(A) may be obtained by generating an orthonormal basis for \\mathcal{K}_n(A, v). To do this, the Lanczos method constructs successive QR factorizations of K_j(A,v) = Q_j R_j for each j = 1, 2, \\dots, n. Due to A’s symmetry and the orthogonality of Q_j, we have q_k^T A q_l = q_l^T A^T q_k = 0 for k &gt; l + 1, implying T_j = Q_j^T A Q_j has a tridiagonal structure:\n\\begin{equation}\n    T_j = \\begin{bmatrix}\n    \\alpha_1 & \\beta_2 & & & \\\\\n    \\beta_2 & \\alpha_2 & \\beta_3 & & \\\\\n     & \\beta_3 & \\alpha_3 & \\ddots & \\\\\n    & & \\ddots & \\ddots & \\beta_{j} \\\\\n    & & & \\beta_{j} & \\alpha_{j}\n    \\end{bmatrix}, \\; \\beta_j &gt; 0, \\; j = 1, 2, \\dots, n\n\\end{equation}\n\nGiven an initial pair (A, q_1) satisfying \\lVert q_1 \\rVert = 1, one can restrict and project A to its j-th Krylov subspace T_j via: \n\\begin{equation}\n    A Q_j = Q_j T_j + \\beta_{j+1} q_{j+1} e_{j}^T \\quad\\quad (\\beta_j &gt; 0)\n\\end{equation}\n where Q_j = [\\, q_1 \\mid q_2 \\mid \\dots \\mid q_j \\,] is an orthonormal set of vectors mutually orthogonal to q_{j+1}. Equating the j-th columns on each side of the above and rearranging the terms yields the famed three-term recurrence: \\begin{equation}\n     \\beta_{j} \\, q_{j+1} = A q_j - \\alpha_j \\, q_j - \\beta_{j\\text{-}1} \\, q_{j\\text{-}1}  \n\\end{equation}\n where \\alpha_j = q_j^T A q_j, \\beta_j = \\lVert r_j \\rVert_2, r_j = (A - \\alpha_j I)q_j - \\beta_{j\\text{-}1} q_j, and q_{j+1} = r_j / \\beta_j. The equation above is a variable-coefficient second-order linear difference equation, and such equations have unique solutions: if (q_{j\\text{-}1}, \\beta_j, q_j) are known, then (\\alpha_j, \\beta_{j+1}, q_{j+1}) are completely determined. This sequential process which iteratively builds T_j via this three-term recurrence is what is known as the Lanczos iteration.",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "theory/lanczos.html#uniqueness-of-t",
    "href": "theory/lanczos.html#uniqueness-of-t",
    "title": "The Lanczos method",
    "section": "Uniqueness of T",
    "text": "Uniqueness of T\nUnfortunately, unlike the spectral decomposition A = V \\Lambda V^T—which identifies a diagonalizable A with its spectrum \\Lambda(A) up to a change of basis A \\mapsto M^{-1} A M—there is no canonical choice of T_j due to the arbitrary choice of v. However, there is a connection between the iterates K_j(A,v) and the full tridiagonalization of A: if Q^T A Q = T is tridiagonal and Q= [\\, q_1 \\mid q_2 \\mid \\dots \\mid q_n \\,] is an n \\times n orthogonal matrix Q Q^T = I_n = [e_1, e_2, \\dots, e_n], then we have: \n\\begin{equation}\n    K_n(A, q_1) = Q Q^T K_n(A, q_1) = Q[ \\, e_1 \\mid T e_1 \\mid T^2 e_1 \\mid \\dots \\mid T^{n-1} e_1 \\, ]\n\\end{equation}\n is the QR factorization of K_n(A, q_1). Thus, tridiagonalizing A with respect to a unit-norm q_1 determines Q. Indeed, the Implicit Q Theorem asserts that if an upper Hessenburg matrix T \\in \\mathbb{R}^{n \\times n} has only positive elements on its first subdiagonal and there exists an orthogonal matrix Q such that Q^T A Q = T, then Q and T are uniquely determined by (A, q_1).",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "integration/slq_guide.html",
    "href": "integration/slq_guide.html",
    "title": "SLQ Trace guide",
    "section": "",
    "text": "This guide walks through the SLQ method implemented in primate, which can be specialized for different purposes."
  },
  {
    "objectID": "integration/slq_guide.html#slq-as-a-function-template",
    "href": "integration/slq_guide.html#slq-as-a-function-template",
    "title": "SLQ Trace guide",
    "section": "SLQ as a function template",
    "text": "SLQ as a function template\nBelow is the full signature of the SLQ function template:\n// Stochastic Lanczos quadrature method\ntemplate&lt; std::floating_point F, LinearOperator Matrix, ThreadSafeRBG RBG &gt;\nvoid slq (\n  const Matrix& A,                    // Any *LinearOperator*\n  const function&lt; F(int,F*,F*) &gt;& f,  // Generic function\n  const function&lt; bool(int) &gt;& stop,  // Early-stop function\n  const int nv,                       // Num. of sample vectors\n  const Distribution dist,            // Sample vector distribution\n  RBG& rng,                           // Random bit generator\n  const int lanczos_degree,           // Krylov subspace degree\n  const F lanczos_rtol,               // Lanczos residual tolerance\n  const int orth,                     // Add. vectors to orthogonalize\n  const int ncv,                      // Num. of Lanczos vectors\n  const int num_threads,              // # threads to allocate \n  const int seed                      // Seed for RNG \n)\nMany of the runtime arguments are documented in the lanczos or sl_trace docs; the compile-time (template) parameters are:\n\nThe floating point type (e.g. float, double, long double)\nThe operator type (e.g. Eigen::MatrixXf, torch::Tensor, LinOp)\nThe multi-threaded random number generator (e.g. ThreadedRNG64)\n\nNote any type combination satisfying these concepts (e.g. std::floating_point, LinearOperator) generates a function specialized of said types at compile-time—this is known as template instantiation."
  },
  {
    "objectID": "integration/slq_guide.html#generality-via-function-passing",
    "href": "integration/slq_guide.html#generality-via-function-passing",
    "title": "SLQ Trace guide",
    "section": "Generality via function passing",
    "text": "Generality via function passing\nGiven a valid set of parameters, the main body of the SLQ looks something like this:\n  bool stop_flag = false;\n  #pragma omp parallel shared(stop_flag)\n  {\n    // &lt; allocations for Q, alpha, beta, etc. &gt; \n    int tid = omp_get_thread_num(); // thread-id \n    \n    #pragma omp for\n    for (i = 0; i &lt; nv; ++i){\n      if (stop_flag){ continue; }\n      generate_isotropic&lt; F &gt;(...); // populates q\n      lanczos_recurrence&lt; F &gt;(...); // populates alpha + beta\n      lanczos_quadrature&lt; F &gt;(...); // populates nodes + weights\n      f(i, q, Q, nodes, weights);   // Run user-supplied function \n      #pragma omp critical\n      {\n        stop_flag = stop(i);        // Checks for early-stopping\n      }\n    } // end for\n  } // end parallel \nThere are two functions that can be used for generalizing SLQ for different purposes.\nThe first generic function f can read, save, or modify the information available from the iteration index i, the isotropic vector q, the Lanczos vectors Q, and/or the quadrature information nodes, weights. Note this function is run in the parallel section.\nThe second is a boolean-valued function stop which can be used to stop the iteration early, for example if convergence has been achieved according to some rule. Since this is run in the critical section, it is called sequentially."
  },
  {
    "objectID": "integration/slq_guide.html#using-slq-to-estimate-mathrmtrfa",
    "href": "integration/slq_guide.html#using-slq-to-estimate-mathrmtrfa",
    "title": "SLQ Trace guide",
    "section": "Using SLQ to estimate \\mathrm{tr}(f(A))",
    "text": "Using SLQ to estimate \\mathrm{tr}(f(A))\nThe SLQ method is often used to estimate the trace of an arbitrary matrix function:\n \\mathrm{tr}(f(A)), \\quad \\text{ where } f(A) = U f(\\Lambda) U^T \nIt’s has been shown1 that the information obtained by the Lanczos method is sufficient to obtained a Gaussian quadrature approximation of the empirical spectral measure of A. By sampling zero-mean vectors satisfying \\mathbb{E}[v v^T] = I, one can obtain estimates of the trace above: \\operatorname{tr}(f(A)) \\approx \\frac{n}{\\mathrm{n}_{\\mathrm{v}}} \\sum_{l=1}^{\\mathrm{n}_{\\mathrm{v}}}\\left(\\sum_{k=0}^m\\left(\\tau_k^{(l)}\\right)^2 f\\left(\\theta_k^{(l)}\\right)\\right)\nIt turns out averaging these trace estimates yields unbiased, Girard-Hutchinson estimator of the trace. To see why this estimator is unbiased, note that:  \\mathtt{tr}(A) = \\mathbb{E}[v^T A v] \\approx \\frac{1}{n_v}\\sum\\limits_{i=1}^{n_v} v_i^\\top A v_i \nThus, all we need to do to estimate the trace of a matrix function is multiply and sum the quadrature nodes and weights output by SLQ."
  },
  {
    "objectID": "integration/slq_guide.html#sl_trace-method",
    "href": "integration/slq_guide.html#sl_trace-method",
    "title": "SLQ Trace guide",
    "section": "sl_trace method",
    "text": "sl_trace method\nTo see how these formulas are actually implemented with the generic SLQ implementation, here’s an abbreviated form of the sl_trace function implemented by primate:\ntemplate&lt; std::floating_point F, LinearOperator Matrix, ThreadSafeRBG RBG &gt;\nvoid sl_trace(\n  const Matrix& A, const std::function&lt; F(F) &gt; sf, RBG& rbg, \n  const int nv, const int dist, const int engine_id, const int seed,\n  const int deg, const float lanczos_rtol, const int orth, const int ncv,\n  const F atol, const F rtol\n  F* estimates\n){  \n  using VectorF = Eigen::Array&lt; F, Dynamic, 1&gt;;\n\n  // Parameterize the trace function (runs in parallel)\n  auto trace_f = [&](int i, F* q, F* Q, F* nodes, F* weights){\n    Map&lt; VectorF &gt; nodes_v(nodes, deg, 1);     // no-op\n    Map&lt; VectorF &gt; weights_v(weights, deg, 1); // no-op\n    nodes_v.unaryExpr(sf);\n    estimates[i] = (nodes_v * weights_v).sum();\n  };\n  \n  // Convergence checking like scipy.integrate.quadrature\n  int n = 0;\n  F mu_est = 0.0, mu_pre = 0.0;\n  const auto early_stop = [&](int i) -&gt; bool {\n    ++n; // Number of estimates\n    mu_est = (1.0 / F(n)) * (estimates[i] + (n - 1) * mu_pre); \n    bool atol_check = abs(mu_est - mu_pre) &lt;= atol;\n    bool rtol_check = abs(mu_est - mu_pre) / mu_est &lt;= rtol; \n    mu_pre = mu_est; \n    return atol_check || rtol_check;\n  };\n\n  // Execute the stochastic Lanczos quadrature with the trace function \n  slq&lt; float &gt;(A, trace_f, early_stop, ...);\n}\nAs before, two functions are used to parameterize the slq method.\nThe first (trace_f) applies an arbitrary spectral function sf to the Rayleigh-Ritz values obtained by the Lanczos tridiagonalization of A(or equivalently, the nodes of the Gaussian quadrature). These are the \\theta’s in the pseudocode above. When multiplied by the weights of the quadrature, the corresponding sum forms an estimate of the trace of the matrix function.\nThe second function early_stop is used to check for convergence of the estimator. First, it uses the trace estimate x_n to update the sample mean \\mu_n via the formula:\n \\mu_n = n^{-1} [x_n + (n - 1)\\mu_{n-1}] \nThen, much in the same way the quadrature function from scipy.integrate approximates a definite integral, it checks for convergence using the absolute and relative tolerances supplied by the user. Returning true signals convergence, stopping the iteration early."
  },
  {
    "objectID": "integration/slq_guide.html#references",
    "href": "integration/slq_guide.html#references",
    "title": "SLQ Trace guide",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "theory/slq.html",
    "href": "theory/slq.html",
    "title": "SLQ Trace guide",
    "section": "",
    "text": "To clarify that that means, here’s an abstract presentation of the generic SLQ procedure:\n\n\n\\begin{algorithm} \\caption{Stochastic Lanczos Quadrature} \\begin{algorithmic} \\Input Symmetric operator ($A \\in \\mathbb{R}^{n \\times n}$) \\Require Number of queries ($n_v$), Degree of quadrature ($k$) \\Function{SLQ}{$A$, $n_v$, $k$} \\State $\\Gamma \\gets 0$ \\For{$j = 1, 2, \\dots, n_v$} \\State $v_i \\sim \\mathcal{D}$ where $\\mathcal{D}$ satisfies $\\mathbb{E}(v v^\\top) = I$ \\State $T^{(j)}(\\alpha, \\beta)$ $\\gets$ $\\mathrm{Lanczos}(A,v_j,k+1)$ \\State $[\\Theta, Y] \\gets \\mathrm{eigh\\_tridiag}(T^{(j)}(\\alpha, \\beta))$ \\State $\\tau_i \\gets \\langle e_1, y_i \\rangle$ \\State &lt; Do something with the node/weight pairs $(\\theta_i, \\tau_i^2)$ &gt; \\EndFor \\EndFunction \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Theory",
      "SLQ"
    ]
  },
  {
    "objectID": "theory/lanczos.html#why-care",
    "href": "theory/lanczos.html#why-care",
    "title": "The Lanczos method",
    "section": "Why care?",
    "text": "Why care?\nComputing the eigen-decomposition A = U \\Lambda U^T for general symmetric A \\in \\mathbb{R}^{n \\times n} is essentially bounded above by \\Theta(n^\\omega) time and \\Theta(n^2) space, where \\omega \\approx is the matrix-multiplication constant. This translates to an effective \\Omega(n^3) time bound if we exclude the Strassen-model for matrix multiplication (since it is not practical anyways). However, if one can show that v \\mapsto Av \\approx O(n), then there is a simple way of obtaining \\Lambda(A) in O(n^2) time and O(n) space!",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "theory/lanczos.html#quadratic-time-and-linear-space-how",
    "href": "theory/lanczos.html#quadratic-time-and-linear-space-how",
    "title": "The Lanczos method",
    "section": "Quadratic time and linear space? How?",
    "text": "Quadratic time and linear space? How?\nUnless you know the tricks, its not obvious at all decompositions above takes just O(n^2) time and O(n) space to obtain. So… how does the complexity argument play out?\n\nThe cubic bound\nFirst, its important to establish that computing the eigen-decomposition A = U \\Lambda U^T for general symmetric A \\in \\mathbb{R}^{n \\times n} is essentially bounded by \\Theta(n^\\omega) time and \\Theta(n^2) space, where \\omega \\approx 2.37\\dots is the matrix-multiplication constant. Conceptually, if we exclude the Strassen-model for matrix multiplication (since it is not practical anyways), this translates to an effective \\Omega(n^3) time bound.\n\n\nSolving the right problem\nIn some applications, the eigenvectors are not needed all at once (or at all, even). One of the main draws to the Lanczos method is its efficiency: if one can perform v \\mapsto Av quickly—say, in \\approx O(n) time—then the Lanczos method can construct \\Lambda(A) in just O(n^2) time and O(n) space! Moreover, entire method is matrix free as the only input to the algorithm is a (fast) matrix-vector product v \\mapsto Av: one need not store A explicitly to do this for many special types of linear operators.\nIf you squint hard enough, you can deduce that since A Q_j = Q_j T_j + \\beta_{j+1} q_{j+1} e_{j}^T, every symmetric A \\in \\mathbb{R}^{n \\times n} expanded this way admits a three-term recurrence: \n\\begin{align*}\nA q_j &= \\beta_{j\\text{-}1} q_{j\\text{-}1} + \\alpha_j q_j + \\beta_j q_{j+1} \\\\\n\\Leftrightarrow \\beta_{j} \\, q_{j+1} &= A q_j - \\alpha_j \\, q_j - \\beta_{j\\text{-}1} \\, q_{j\\text{-}1}  \n\\end{align*}\n\nThe equation above is a variable-coefficient second-order linear difference equation, and it is known such equations have unique solutions: \n\\alpha_j = q_j^T A q_j, \\;\\; \\beta_j = \\lVert r_j \\rVert_2, \\;\\; q_{j+1} = r_j / \\beta_j\n\n\n\\text{where  } r_j = (A - \\alpha_j I)q_j - \\beta_{j\\text{-}1} q_j\n\nIn other words, if (q_{j\\text{-}1}, \\beta_j, q_j) are known, then (\\alpha_j, \\beta_{j+1}, q_{j+1}) are completely determined. This fact is fantastic from a computational point of view: no explicit call to the QR algorithm necessary2!\nNote that a symmetric tridiagonal matrix is fully characterized by its diagonal and subdiagonal terms, which requires just O(n) space. If we assume that v \\mapsto Av \\sim O(n), then the above procedure clearly takes at most O(n^2) time, since there are most n such vectors \\{q_i\\}_{i=1}^n to generate!\nMoreover, if we only need the eigen-values \\Lambda(A) ( and not their vectors U), then we may execute the recurrence keeping at most three vectors \\{q_{j-1}, q_{j}, q_{j+1}\\} in memory at any given time. Since each of these is O(n) is size, the claim of O(n) space is justified!\nThe description above is essentially the proof of the following Theorem:\n\nTheorem 1 (Parlett 1994, Simon 1984) Given a symmetric rank-r matrix A \\in \\mathbb{R}^{n \\times n} whose operator x \\mapsto A x requires O(\\eta) time and O(\\nu) space, the Lanczos iteration computes \\Lambda(A) = \\{ \\lambda_1, \\lambda_2, \\dots, \\lambda_r \\} in O(\\max\\{\\eta, n\\}\\cdot r) time and O(\\max\\{\\nu, n\\}) space, when computation is done in exact arithmetic",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "basic/install.html",
    "href": "basic/install.html",
    "title": "Installation",
    "section": "",
    "text": "primate is a standard PEP-517 package, and thus can be installed via pip:\npip install &lt; primate source directory &gt;\nCurrently the package must be built from source via cloning the repository. PYPI support is planned.\n\n\nC++ Installation\nprimate’s C++ interface is header-only, making it easy to compile your own extension modules. The simplest way to link these headers is to add primate as a dependency to your package and use the get_include() function to find the appropriate directory.\n\nsetuptoolsmeson-pythongit submodule\n\n\n# setup.py\nimport primate as pm\n...\nExtension('extension_name', ..., include_dirs=[pm.get_include()])\n...\n\n\n# meson.build\n...\nprimate_include_dirs = run_command(py, \n  ['-c', 'import primate as pm; print(pm.get_include())']\n).stdout().strip()\n...\n\n\nAssuming your headers are located in extern, from your git repository, you can use:\ngit submodule add https://github.com/peekxc/primate extern/primate\ngit submodule update --init\nFrom here, you can now include extern/primate/include into your C++ source files, or you can add this directory to the search path used other various build tools, such as CMake or Meson.",
    "crumbs": [
      "Basics",
      "Installation"
    ]
  },
  {
    "objectID": "theory/lanczos.html#surpassing-the-cubic-bound",
    "href": "theory/lanczos.html#surpassing-the-cubic-bound",
    "title": "The Lanczos method",
    "section": "Surpassing the cubic bound",
    "text": "Surpassing the cubic bound\nComputing the eigen-decomposition A = U \\Lambda U^T for general symmetric A \\in \\mathbb{R}^{n \\times n} is essentially bounded by \\Theta(n^\\omega) time and \\Theta(n^2) space, where \\omega \\approx 2.37\\dots is the matrix-multiplication constant. Conceptually, if we exclude the Strassen-model for matrix multiplication (since it is not practical anyways), this translates to an effective \\Omega(n^3) time bound. Not great!\nOne of the main draws to the Lanczos method is its efficiency: if one can perform v \\mapsto Av quickly—say, in \\approx O(n) time—then the Lanczos method can construct \\Lambda(A) in just O(n^2) time and O(n) space! Moreover, entire method is matrix free as the only input to the algorithm is a (fast) matrix-vector product v \\mapsto Av: one need not store A explicitly to do this for many special types of linear operators.",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Package overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMusco, Cameron, Christopher Musco, and Aaron Sidford. (2018) “Stability of the Lanczos method for matrix function approximation.”↩︎\nUbaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature.↩︎\nThis includes std::function’s, C-style function pointers, functors, and lambda expressions.↩︎",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "theory/lanczos.html#footnotes",
    "href": "theory/lanczos.html#footnotes",
    "title": "The Lanczos method",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA variant of the Lanczos method is actually at the heart scipy.sparse.linalg’s default eigsh solver (which is a port of ARPACK).↩︎\nThe spectral decomposition A = U \\Lambda U^T identifies a diagonalizable A with its spectrum \\Lambda(A) up to a change of basis A \\mapsto M^{-1} A M↩︎\nThe spectral decomposition A = U \\Lambda U^T identifies a diagonalizable A with its spectrum \\Lambda(A) up to a change of basis A \\mapsto M^{-1} A M↩︎\nFor general A \\in \\mathbb{R}^{n \\times n}, computing the spectral-decomposition is essentially bounded by the matrix-multiplication time: \\Theta(n^\\omega) time and \\Theta(n^2) space, where \\omega \\approx 2.37\\dots is the matrix multiplication constant. If we exclude the Strassen model for computation, we get effectively a \\Omega(n^3) time and \\Omega(n^2) space bound.↩︎",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "imate_compare.html#footnotes",
    "href": "imate_compare.html#footnotes",
    "title": "Comparison to imate",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBefore v0.2, much of primate’s code was essentially ported and refactored from imate. The code for v0.2+ has been re-written using the Eigen template C++ library.↩︎\nprimate does not provide native GPU-implemented Linear operators. However, there is nothing preventing one from using e.g. CUDA- or ROCm-based GPU-based tensor libraries to accelerate matrix-vector products. Indeed, primate was designed to work with essentially any operator matching the interface.↩︎\nSee imates documentation for the list of supported functions.↩︎",
    "crumbs": [
      "Basics",
      "Comparison to *imate*"
    ]
  },
  {
    "objectID": "theory/lanczos.html#pseudocode",
    "href": "theory/lanczos.html#pseudocode",
    "title": "The Lanczos method",
    "section": "Pseudocode",
    "text": "Pseudocode\nThere are several ways to implement the Lanczos method, some of which are “better” than others. Below is pseudocode equivalent to Paige’s A27 variant, which has been shown to have a variety of attractive properties.\nThere are many extensions that modify the Lanczos method to make it more robust, more computationally efficient, etc., though many of these have non-trivial implications on the space and time complexities.",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "advanced/slq_param.html",
    "href": "advanced/slq_param.html",
    "title": "Parameterizing SLQ",
    "section": "",
    "text": "This guide walks through how to parameterize the SLQ method implemented in primate on the C++ side to approximate some spectral quantity of interest.",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#slq-as-a-function-template",
    "href": "advanced/slq_param.html#slq-as-a-function-template",
    "title": "Parameterizing SLQ",
    "section": "SLQ as a function template",
    "text": "SLQ as a function template\nBelow is the full signature of the SLQ function template:\n// Stochastic Lanczos quadrature method\ntemplate&lt; std::floating_point F, LinearOperator Matrix, ThreadSafeRBG RBG &gt;\nvoid slq (\n  const Matrix& A,                    // Any *LinearOperator*\n  const function&lt; F(int,F*,F*) &gt;& f,  // Generic function\n  const function&lt; bool(int) &gt;& stop,  // Early-stop function\n  const int nv,                       // Num. of sample vectors\n  const Distribution dist,            // Sample vector distribution\n  RBG& rng,                           // Random bit generator\n  const int lanczos_degree,           // Krylov subspace degree\n  const F lanczos_rtol,               // Lanczos residual tolerance\n  const int orth,                     // Add. vectors to orthogonalize\n  const int ncv,                      // Num. of Lanczos vectors\n  const int num_threads,              // # threads to allocate \n  const int seed                      // Seed for RNG \n)\nMany of the runtime arguments are documented in the lanczos or sl_trace docs; the compile-time (template) parameters are:\n\nThe floating point type (e.g. float, double, long double)\nThe operator type (e.g. Eigen::MatrixXf, torch::Tensor, LinOp)\nThe multi-threaded random number generator (e.g. ThreadedRNG64)\n\nNote any type combination satisfying these concepts (e.g. std::floating_point, LinearOperator) generates a function specialized of said types at compile-time—this is known as template instantiation.",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#generality-via-function-passing",
    "href": "advanced/slq_param.html#generality-via-function-passing",
    "title": "Parameterizing SLQ",
    "section": "Generality via function passing",
    "text": "Generality via function passing\nGiven a valid set of parameters, the main body of the SLQ looks something like this:\n  bool stop_flag = false;\n  #pragma omp parallel shared(stop_flag)\n  {\n    // &lt; allocations for Q, alpha, beta, etc. &gt; \n    int tid = omp_get_thread_num(); // thread-id \n    \n    #pragma omp for\n    for (i = 0; i &lt; nv; ++i){\n      if (stop_flag){ continue; }\n      generate_isotropic&lt; F &gt;(...); // populates q\n      lanczos_recurrence&lt; F &gt;(...); // populates alpha + beta\n      lanczos_quadrature&lt; F &gt;(...); // populates nodes + weights\n      f(i, q, Q, nodes, weights);   // Run user-supplied function \n      #pragma omp critical\n      {\n        stop_flag = stop(i);        // Checks for early-stopping\n      }\n    } // end for\n  } // end parallel \nThere are two functions that can be used for generalizing SLQ for different purposes.\nThe first generic function f can read, save, or modify the information available from the iteration index i, the isotropic vector q, the Lanczos vectors Q, and/or the quadrature information nodes, weights. Note this function is run in the parallel section.\nThe second is a boolean-valued function stop which can be used to stop the iteration early, for example if convergence has been achieved according to some rule. Since this is run in the critical section, it is called sequentially.",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#using-slq-to-estimate-mathrmtrfa",
    "href": "advanced/slq_param.html#using-slq-to-estimate-mathrmtrfa",
    "title": "Parameterizing SLQ",
    "section": "Using SLQ to estimate \\mathrm{tr}(f(A))",
    "text": "Using SLQ to estimate \\mathrm{tr}(f(A))\nThe SLQ method is often used to estimate the trace of an arbitrary matrix function:\n \\mathrm{tr}(f(A)), \\quad \\text{ where } f(A) = U f(\\Lambda) U^T \nIt’s has been shown1 that the information obtained by the Lanczos method is sufficient to obtained a Gaussian quadrature approximation of the empirical spectral measure of A. By sampling zero-mean vectors satisfying \\mathbb{E}[v v^T] = I, one can obtain estimates of the trace above: \\operatorname{tr}(f(A)) \\approx \\frac{n}{\\mathrm{n}_{\\mathrm{v}}} \\sum_{l=1}^{\\mathrm{n}_{\\mathrm{v}}}\\left(\\sum_{k=0}^m\\left(\\tau_k^{(l)}\\right)^2 f\\left(\\theta_k^{(l)}\\right)\\right)\nIt turns out averaging these trace estimates yields unbiased, Girard-Hutchinson estimator of the trace. To see why this estimator is unbiased, note that:  \\mathtt{tr}(A) = \\mathbb{E}[v^T A v] \\approx \\frac{1}{n_v}\\sum\\limits_{i=1}^{n_v} v_i^\\top A v_i \nThus, all we need to do to estimate the trace of a matrix function is multiply and sum the quadrature nodes and weights output by SLQ.",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#sl_trace-method",
    "href": "advanced/slq_param.html#sl_trace-method",
    "title": "Parameterizing SLQ",
    "section": "sl_trace method",
    "text": "sl_trace method\nTo see how these formulas are actually implemented with the generic SLQ implementation, here’s an abbreviated form of the sl_trace function implemented by primate:\ntemplate&lt; std::floating_point F, LinearOperator Matrix, ThreadSafeRBG RBG &gt;\nvoid sl_trace(\n  const Matrix& A, const std::function&lt; F(F) &gt; sf, RBG& rbg, \n  const int nv, const int dist, const int engine_id, const int seed,\n  const int deg, const float lanczos_rtol, const int orth, const int ncv,\n  const F atol, const F rtol\n  F* estimates\n){  \n  using VectorF = Eigen::Array&lt; F, Dynamic, 1&gt;;\n\n  // Parameterize the trace function (runs in parallel)\n  auto trace_f = [&](int i, F* q, F* Q, F* nodes, F* weights){\n    Map&lt; VectorF &gt; nodes_v(nodes, deg, 1);     // no-op\n    Map&lt; VectorF &gt; weights_v(weights, deg, 1); // no-op\n    nodes_v.unaryExpr(sf);\n    estimates[i] = (nodes_v * weights_v).sum();\n  };\n  \n  // Convergence checking like scipy.integrate.quadrature\n  int n = 0;\n  F mu_est = 0.0, mu_pre = 0.0;\n  const auto early_stop = [&](int i) -&gt; bool {\n    ++n; // Number of estimates\n    mu_est = (1.0 / F(n)) * (estimates[i] + (n - 1) * mu_pre); \n    bool atol_check = abs(mu_est - mu_pre) &lt;= atol;\n    bool rtol_check = abs(mu_est - mu_pre) / mu_est &lt;= rtol; \n    mu_pre = mu_est; \n    return atol_check || rtol_check;\n  };\n\n  // Execute the stochastic Lanczos quadrature with the trace function \n  slq&lt; float &gt;(A, trace_f, early_stop, ...);\n}\nAs before, two functions are used to parameterize the slq method.\nThe first (trace_f) applies an arbitrary spectral function sf to the Rayleigh-Ritz values obtained by the Lanczos tridiagonalization of A(or equivalently, the nodes of the Gaussian quadrature). These are the \\theta’s in the pseudocode above. When multiplied by the weights of the quadrature, the corresponding sum forms an estimate of the trace of the matrix function.\nThe second function early_stop is used to check for convergence of the estimator. First, it uses the trace estimate x_n to update the sample mean \\mu_n via the formula:\n \\mu_n = n^{-1} [x_n + (n - 1)\\mu_{n-1}] \nThen, much in the same way the quadrature function from scipy.integrate approximates a definite integral, it checks for convergence using the absolute and relative tolerances supplied by the user. Returning true signals convergence, stopping the iteration early.",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#references",
    "href": "advanced/slq_param.html#references",
    "title": "Parameterizing SLQ",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#footnotes",
    "href": "advanced/slq_param.html#footnotes",
    "title": "Parameterizing SLQ",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUbaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099.↩︎",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "theory/lanczos.html#but-wait-isnt-t-arbitrary",
    "href": "theory/lanczos.html#but-wait-isnt-t-arbitrary",
    "title": "The Lanczos method",
    "section": "But wait, isn’t T arbitrary?",
    "text": "But wait, isn’t T arbitrary?\nUnfortunately, there is no canonical choice of T_j. Indeed, as T_n is a family with n - 1 degrees of freedom and v \\in \\mathbb{R}^n was chosen arbitrarily, there are infinitely many essentially distinct such decompositions. In contrast, the spectral decomposition A = U \\Lambda U^T identifies a diagonalizable A with its spectrum \\Lambda(A) up to a change of basis A \\mapsto M^{-1} A M.\nNot all hope is lost though. Notice that since Q is an orthogonal matrix, thus we have:\n Q Q^T = I_n = [e_1, e_2, \\dots, e_n] \nBy extension, given an initial pair (A, q_1) satisfying \\lVert q_1 \\rVert = 1, we have:\n\nK_n(A, q_1) = Q Q^T K_n(A, q_1) = Q[ \\, e_1 \\mid T e_1 \\mid T^2 e_1 \\mid \\dots \\mid T^{n-1} e_1 \\, ]\n\nThis is actually QR factorization! Indeed, the Implicit Q Theorem asserts that if an upper Hessenburg matrix T \\in \\mathbb{R}^{n \\times n} has only positive elements on its first subdiagonal and there exists an orthogonal matrix Q such that Q^T A Q = T, then Q and T are uniquely determined3 by (A, q_1).\nThus, tridiagonalizing A with respect to an arbitrary q_1 \\in \\mathbb{R}^n satisfying \\lVert q_1\\rVert = 1 determines Q.",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "advanced/cpp_integration.html",
    "href": "advanced/cpp_integration.html",
    "title": "C++ Integration",
    "section": "",
    "text": "primate’s generic API is enabled through function templates specialized using C++20 concepts. In other words, a function F requiring concept C will compile with any type T so long as T respects the constraints imposed by C. For example, generically, any type T respecting the LinearOperatorconcept shown below can be passed to the Lanczos method:\nIn english, an instance A of type T is said to support the LinearOperator concept if it has:\nshape() should return a pair (n,m) representing the sizes of the output and input vectors, respectively. Note in the matrix setting this corresponds to the number of rows and columns.",
    "crumbs": [
      "Advanced",
      "Usage from C++"
    ]
  },
  {
    "objectID": "advanced/cpp_integration.html#the-linearoperator-concept",
    "href": "advanced/cpp_integration.html#the-linearoperator-concept",
    "title": "C++ Integration",
    "section": "",
    "text": "primate’s generic API is enabled through C++20 concepts. Thus, the more exact statement is that any type respecting the LinearOperator concept shown below can be passed:\nusing FP = std::floating_point; \ntemplate &lt; typename T, FP float_t = typename T::value_type &gt;\nconcept LinearOperator = requires(T A, const float_t* v, float_t* o) {\n  { A.matvec(v, o) }; // o = A v\n  { A.shape() } -&gt; std::convertible_to&lt; std::pair&lt; size_t, size_t &gt; &gt;;\n};\nAn instance A of type T is said to support the LinearOperator concept if it has:\n\nA method Av \\mapsto o, with signature A.matvec(const float_t* v, float_t* o)\nA method yielding (\\mathrm{card}(o), \\mathrm{card}(v)), with signatureA.shape() -&gt; pair&lt; ... &gt;\n\nshape() should yield a pair (n,m) representing the sizes of the output and input vectors, respectively. This corresponds to the number of rows and columns in the matrix setting.",
    "crumbs": [
      "Advanced",
      "Usage from C++"
    ]
  },
  {
    "objectID": "advanced/cpp_integration.html#other-concepts",
    "href": "advanced/cpp_integration.html#other-concepts",
    "title": "C++ Integration",
    "section": "Other Concepts",
    "text": "Other Concepts\nDepending on the problem at hand, the supplied operator may need to meet other constraints. Here’s a short list additional operator concepts:\n\n\n\n\n\n\n\n\n\nConcept\nSupports\nSignature\nRequires\n\n\n\n\nLinearOperator\nA v \\mapsto o\nA.matvec(v, o)\nNA\n\n\nAdjointOperator\nA^T v \\mapsto o\nA.rmatvec(v, o)\nLinearOperator\n\n\nAdditiveOperator\no \\gets o + \\alpha Av\nA.matvec_add(v, alpha, o)\nLinearOperator\n\n\nAffineOperator\nSets t s.t. A + tB\nA.set_parameter(t)\nLinearOperator\n\n\nQuadOperator\nv^T A v\nA.quad(v)\nNA\n\n\n\nThe exported methods in primate only need the minimum constraints to be satisfied to compile: if you need access to the Lanczos method, then just supporting the LinearOperator concept is sufficient. On the other hand, adding support for other constraints can optimize the efficiency of certain methods; for example, the hutch method technically only requires a LinearOperator to do trace estimation (via matvec calls), but will also compile and prefer calling quad with a QuadOperator as input. In such a situaton, if your operator has an efficient quadratic form v \\mapsto v^T A v, then implementing quad may improve the performance of hutch.",
    "crumbs": [
      "Advanced",
      "Usage from C++"
    ]
  },
  {
    "objectID": "advanced/pybind11_integration.html",
    "href": "advanced/pybind11_integration.html",
    "title": "pybind11 Integration",
    "section": "",
    "text": "If you’re using pybind11, you can easily incorporate your own custom linear operator / matrix function pair using primates binding headers.\nTODO",
    "crumbs": [
      "Advanced",
      "Integrating with pybind11"
    ]
  },
  {
    "objectID": "theory/lanczos.html#complexities",
    "href": "theory/lanczos.html#complexities",
    "title": "The Lanczos method",
    "section": "Complexities",
    "text": "Complexities\nElegant as the Lanczos method may be, what does it net us? Well, for starters, the Lanczos method can drop the complexity of obtaining the spectral decomposition by order of magnitude.\n\nTheorem 1 (Parlett 1994, Simon 1984) Given a symmetric rank-r matrix A \\in \\mathbb{R}^{n \\times n} whose operator x \\mapsto A x requires O(\\eta) time and O(\\nu) space, the Lanczos method computes \\Lambda(A) in O(\\max\\{\\eta, n\\}\\cdot r) time and O(\\max\\{\\nu, n\\}) space, when computation is done in exact arithmetic\n\nFor general A \\in \\mathbb{R}^{n \\times n}, computing the spectral-decomposition is essentially bounded by the matrix-multiplication time: \\Theta(n^\\omega) time and \\Theta(n^2) space, where \\omega \\approx 2.37\\dots is the matrix multiplication constant. If we exclude the Strassen model for computation, we get effectively a \\Omega(n^3) time and \\Omega(n^2) space bound. In contrast, when both r = n and \\eta = \\nu = n, the Theorem above implies the spectral decomposition takes just O(n^2) time and O(n) space to obtain. Unless you know the tricks, this feels like a contradiction. So… how does the complexity argument play out?\n\nSolving the right problem\n\n\nIf you squint hard enough, you can deduce that since A Q_j = Q_j T_j + \\beta_{j+1} q_{j+1} e_{j}^T, every symmetric A \\in \\mathbb{R}^{n \\times n} expanded this way admits a three-term recurrence: \n\\begin{align*}\nA q_j &= \\beta_{j\\text{-}1} q_{j\\text{-}1} + \\alpha_j q_j + \\beta_j q_{j+1} \\\\\n\\Leftrightarrow \\beta_{j} \\, q_{j+1} &= A q_j - \\alpha_j \\, q_j - \\beta_{j\\text{-}1} \\, q_{j\\text{-}1}  \n\\end{align*}\n\nThe equation above is a variable-coefficient second-order linear difference equation, and it is known such equations have unique solutions, which are given by: \n\\alpha_j = q_j^T A q_j, \\;\\; \\beta_j = \\lVert r_j \\rVert_2, \\;\\; q_{j+1} = r_j / \\beta_j\n\n\n\\text{where  } r_j = (A - \\alpha_j I)q_j - \\beta_{j\\text{-}1} q_j\n\nIn other words, if (q_{j\\text{-}1}, \\beta_j, q_j) are known, then (\\alpha_j, \\beta_{j+1}, q_{j+1}) are completely determined. This fact is fantastic from a computational point of view: no explicit call to the QR algorithm necessary2!\nNote that a symmetric tridiagonal matrix is fully characterized by its diagonal and subdiagonal terms, which requires just O(n) space. If we assume that v \\mapsto Av \\sim O(n), then the above procedure clearly takes at most O(n^2) time, since there are most n such vectors \\{q_i\\}_{i=1}^n to generate!\nMoreover, if we only need the eigen-values \\Lambda(A) ( and not their vectors U), then we may execute the recurrence keeping at most three vectors \\{q_{j-1}, q_{j}, q_{j+1}\\} in memory at any given time. Since each of these is O(n) is size, the claim of O(n) space is justified!\n\n\nBut wait, isn’t T arbitrary?\nUnfortunately, there is no canonical choice of T_j. Indeed, as T_n is a family with n - 1 degrees of freedom and v \\in \\mathbb{R}^n was chosen arbitrarily, there are infinitely many essentially distinct such decompositions. In contrast, the spectral decomposition A = U \\Lambda U^T identifies a diagonalizable A with its spectrum \\Lambda(A) up to a change of basis A \\mapsto M^{-1} A M.\nNot all hope is lost though. Notice that since Q is an orthogonal matrix, thus we have:\n Q Q^T = I_n = [e_1, e_2, \\dots, e_n] \nBy extension, given an initial pair (A, q_1) satisfying \\lVert q_1 \\rVert = 1, we have:\n\nK_n(A, q_1) = Q Q^T K_n(A, q_1) = Q[ \\, e_1 \\mid T e_1 \\mid T^2 e_1 \\mid \\dots \\mid T^{n-1} e_1 \\, ]\n\nThis is actually QR factorization! Indeed, the Implicit Q Theorem asserts that if an upper Hessenburg matrix T \\in \\mathbb{R}^{n \\times n} has only positive elements on its first subdiagonal and there exists an orthogonal matrix Q such that Q^T A Q = T, then Q and T are uniquely determined3 by (A, q_1).",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "theory/lanczos.html#approximation",
    "href": "theory/lanczos.html#approximation",
    "title": "The Lanczos method",
    "section": "Approximation",
    "text": "Approximation\nTODO",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#examples",
    "href": "reference/diagonalize.lanczos.html#examples",
    "title": "diagonalize.lanczos",
    "section": "Examples",
    "text": "Examples\n  ::: {#af54fa09 .cell execution_count=1}\n  ``` {.python .cell-code}\n          print(\"hello\")\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  hello\n  ```\n  :::\n  :::",
    "crumbs": [
      "API Reference",
      "Diagonalize",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#returns",
    "href": "reference/diagonalize.lanczos.html#returns",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nA tuple (a,b) parameterizing the diagonal and off-diagonal of the tridiagonal matrix. If return_basis is\n\n\n\nTrue, then the tuple (a,b), Q is returned, where Q represents orthogonal basis for the Krylov subspace.",
    "crumbs": [
      "API Reference",
      "Diagonalize",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#see-also",
    "href": "reference/diagonalize.lanczos.html#see-also",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "scipy.linalg.eigh_tridiagonal : Eigenvalue solver for real symmetric tridiagonal matrices. operator.matrix_function : Approximates the action of a matrix function via the Lanczos method.",
    "crumbs": [
      "API Reference",
      "Diagonalize",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#notes",
    "href": "reference/diagonalize.lanczos.html#notes",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "No checking for performed for ghost, converged, or ‘locked’ eigenvalues. To increase the accuracy of the eigenvalue approximation, increase orth and deg. Note the number of matvecs with A scales linearly with deg and the number of inner-products scales quadratically with orth.\nSupplying either negative values or values larger than deg for orth will result in full re-orthogonalization.",
    "crumbs": [
      "API Reference",
      "Diagonalize",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#references",
    "href": "reference/diagonalize.lanczos.html#references",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "Paige, Christopher C. “Computational variants of the Lanczos method for the eigenproblem.” IMA Journal of Applied Mathematics 10.3 (1972): 373-381.",
    "crumbs": [
      "API Reference",
      "Diagonalize",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/primate.trace.hutch.html",
    "href": "reference/primate.trace.hutch.html",
    "title": "hutch",
    "section": "",
    "text": "trace.hutch(A, fun=None, maxiter=200, deg=20, atol=None, rtol=None, stop=['confidence', 'change'], ncv=2, orth=0, quad='fttr', confidence=0.95, pdf='rademacher', rng='pcg64', seed=-1, num_threads=0, verbose=False, info=False, plot=False, **kwargs)\nEstimates the trace of a matrix A or matrix function f(A) via a Girard-Hutchinson estimator.\nThis function uses up to maxiter random isotropic vectors to form an unbiased estimator of the trace of A. The estimator is obtained by averaging quadratic forms of A (or f(A)), rescaling as necessary.\n\n\nFor matrix functions, the Lanczos method up to degree deg is used to approximate the action of f(A). By default,\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nA\nndarray, sparray, or LinearOperator\nreal symmetric operator.\nrequired\n\n\nfun\nstr or typing.Callable\nreal-valued function defined on the spectrum of A.\n\"identity\"\n\n\nmaxiter\nint\nMaximum number of random vectors to sample for the trace estimate.\n10\n\n\ndeg\nint\nDegree of the quadrature approximation. Must be at least 1.\n20\n\n\natol\nfloat\nAbsolute tolerance to signal convergence for early-stopping. See notes.\nNone\n\n\nrtol\nfloat\nRelative tolerance to signal convergence for early-stopping. See notes.\n1e-2\n\n\nstop\nstr\nEarly-stopping criteria to test estimator convergence. See details.\n\"confidence\"\n\n\nncv\nint\nNumber of Lanczos vectors to allocate. Must be at least 2.\n2\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against. Must be less than ncv.\n0\n\n\nquad\nstr\nMethod used to obtain the weights of the Gaussian quadrature. See notes.\n'fttr'\n\n\nconfidence\nfloat\nConfidence level to consider estimator as converged. Only used when stop = “confidence”.\n0.95\n\n\npdf\n‘rademacher’, ‘normal’\nChoice of zero-centered distribution to sample random vectors from.\n'rademacher'\n\n\nrng\n‘splitmix64’, ’xoshiro256**‘, ’pcg64’, ‘lcg64’, ‘mt64’\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint\nSeed to initialize the rng entropy source. Set seed &gt; -1 for reproducibility.\n-1\n\n\nnum_threads\nint\nNumber of threads to use to parallelize the computation. Set to &lt;= 0 to let OpenMP decide.\n0\n\n\nplot\nbool\nIf true, plots the samples of the trace estimate along with their convergence characteristics.\nFalse\n\n\ninfo\nbool\nIf True, returns a dictionary containing all relevant information about the computation.\nFalse\n\n\nkwargs\ndict\nadditional key-values to parameterize the chosen function ‘fun’.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nEstimate of the trace of A, if fun = \"identity\", otherwise estimates the trace of f(A).\n\n\n(dict, optional)\nIf ‘info = True’, additional information about the computation.\n\n\n\n\n\n\nlanczos : the lanczos algorithm.\n\n\n\n[1] Ubaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099."
  },
  {
    "objectID": "reference/primate.trace.hutch.html#notes",
    "href": "reference/primate.trace.hutch.html#notes",
    "title": "hutch",
    "section": "",
    "text": "For matrix functions, the Lanczos method up to degree deg is used to approximate the action of f(A). By default,"
  },
  {
    "objectID": "reference/primate.trace.hutch.html#parameters",
    "href": "reference/primate.trace.hutch.html#parameters",
    "title": "hutch",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nA\nndarray, sparray, or LinearOperator\nreal symmetric operator.\nrequired\n\n\nfun\nstr or typing.Callable\nreal-valued function defined on the spectrum of A.\n\"identity\"\n\n\nmaxiter\nint\nMaximum number of random vectors to sample for the trace estimate.\n10\n\n\ndeg\nint\nDegree of the quadrature approximation. Must be at least 1.\n20\n\n\natol\nfloat\nAbsolute tolerance to signal convergence for early-stopping. See notes.\nNone\n\n\nrtol\nfloat\nRelative tolerance to signal convergence for early-stopping. See notes.\n1e-2\n\n\nstop\nstr\nEarly-stopping criteria to test estimator convergence. See details.\n\"confidence\"\n\n\nncv\nint\nNumber of Lanczos vectors to allocate. Must be at least 2.\n2\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against. Must be less than ncv.\n0\n\n\nquad\nstr\nMethod used to obtain the weights of the Gaussian quadrature. See notes.\n'fttr'\n\n\nconfidence\nfloat\nConfidence level to consider estimator as converged. Only used when stop = “confidence”.\n0.95\n\n\npdf\n‘rademacher’, ‘normal’\nChoice of zero-centered distribution to sample random vectors from.\n'rademacher'\n\n\nrng\n‘splitmix64’, ’xoshiro256**‘, ’pcg64’, ‘lcg64’, ‘mt64’\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint\nSeed to initialize the rng entropy source. Set seed &gt; -1 for reproducibility.\n-1\n\n\nnum_threads\nint\nNumber of threads to use to parallelize the computation. Set to &lt;= 0 to let OpenMP decide.\n0\n\n\nplot\nbool\nIf true, plots the samples of the trace estimate along with their convergence characteristics.\nFalse\n\n\ninfo\nbool\nIf True, returns a dictionary containing all relevant information about the computation.\nFalse\n\n\nkwargs\ndict\nadditional key-values to parameterize the chosen function ‘fun’.\n{}"
  },
  {
    "objectID": "reference/primate.trace.hutch.html#returns",
    "href": "reference/primate.trace.hutch.html#returns",
    "title": "hutch",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nfloat\nEstimate of the trace of A, if fun = \"identity\", otherwise estimates the trace of f(A).\n\n\n(dict, optional)\nIf ‘info = True’, additional information about the computation."
  },
  {
    "objectID": "reference/primate.trace.hutch.html#see-also",
    "href": "reference/primate.trace.hutch.html#see-also",
    "title": "hutch",
    "section": "",
    "text": "lanczos : the lanczos algorithm."
  },
  {
    "objectID": "reference/primate.trace.hutch.html#reference",
    "href": "reference/primate.trace.hutch.html#reference",
    "title": "hutch",
    "section": "",
    "text": "[1] Ubaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099."
  },
  {
    "objectID": "reference/primate.trace.sl_gauss.html#parameters",
    "href": "reference/primate.trace.sl_gauss.html#parameters",
    "title": "sl_gauss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nA\nndarray, sparray, or LinearOperator\nreal symmetric operator.\nrequired\n\n\nn\nint\nNumber of random vectors to sample for the quadrature estimate.\n150\n\n\ndeg\nint\nDegree of the quadrature approximation.\n20\n\n\nrng\n‘splitmix64’, ’xoshiro256**‘, ’pcg64’, ‘lcg64’, ‘mt64’\nRandom number generator to use (PCG64 by default).\n'splitmix64'\n\n\nseed\nint\nSeed to initialize the rng entropy source. Set seed &gt; -1 for reproducibility.\n-1\n\n\npdf\n‘rademacher’, ‘normal’\nChoice of zero-centered distribution to sample random vectors from.\n'rademacher'\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against when building the Krylov basis.\n0\n\n\nnum_threads\nint\nNumber of threads to use to parallelize the computation. Setting num_threads &lt; 1 to let OpenMP decide.\n0",
    "crumbs": [
      "API Reference",
      "Trace",
      "SL Gauss"
    ]
  },
  {
    "objectID": "reference/primate.trace.sl_gauss.html#returns",
    "href": "reference/primate.trace.sl_gauss.html#returns",
    "title": "sl_gauss",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nfloat\nEstimate of the trace of the matrix function f(A).\n\n\n(dict, optional)\nIf ‘info = True’, additional information about the computation.",
    "crumbs": [
      "API Reference",
      "Trace",
      "SL Gauss"
    ]
  },
  {
    "objectID": "theory/lanczos.html#the-iteration-part",
    "href": "theory/lanczos.html#the-iteration-part",
    "title": "The Lanczos method",
    "section": "The “iteration” part",
    "text": "The “iteration” part\nLanczos originally referred to his algorithm as the method of minimized iterations, and indeed nowadays it is often called an iterative method. Where’s the iterative component?\nIf you squint hard enough, you can deduce that for every j \\in [1, n): A Q_j = Q_j T_j + \\beta_{j+1} q_{j+1} e_{j}^T Equating the j-th columns on each side of the equation and rearranging yields a three-term recurrence: \n\\begin{align*}\nA q_j &= \\beta_{j\\text{-}1} q_{j\\text{-}1} + \\alpha_j q_j + \\beta_j q_{j+1} \\\\\n\\Leftrightarrow \\beta_{j} \\, q_{j+1} &= A q_j - \\alpha_j \\, q_j - \\beta_{j\\text{-}1} \\, q_{j\\text{-}1}  \n\\end{align*}\n\nThe equation above is a variable-coefficient second-order linear difference equation, and it is known such equations have unique solutions; they are given below: \n\\alpha_j = q_j^T A q_j, \\;\\; \\beta_j = \\lVert r_j \\rVert_2, \\;\\; q_{j+1} = r_j / \\beta_j\n\n\n\\text{where  } r_j = (A - \\alpha_j I)q_j - \\beta_{j\\text{-}1} q_j\n\nIn other words, if (q_{j\\text{-}1}, \\beta_j, q_j) are known, then (\\alpha_j, \\beta_{j+1}, q_{j+1}) are completely determined. In theory, this means we can iteratively generate both Q and T using just a couple vectors at a time—no need to explicitly call to the QR algorithm as shown above. Pretty nifty, eh!",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "theory/lanczos.html#complexity-analysis",
    "href": "theory/lanczos.html#complexity-analysis",
    "title": "The Lanczos method",
    "section": "Complexity analysis",
    "text": "Complexity analysis\nElegant and as theoretically founded as the Lanczos method may be, is it efficient in practice?\nLet’s start by establishing a baseline on its complexity:\n\nTheorem 1 (Parlett 1994) Given a symmetric rank-r matrix A \\in \\mathbb{R}^{n \\times n} whose operator x \\mapsto A x requires O(\\eta) time and O(\\nu) space, the Lanczos method computes \\Lambda(A) in O(\\max\\{\\eta, n\\}\\cdot r) time and O(\\max\\{\\nu, n\\}) space, when computation is done in exact arithmetic\n\nAs its clear from the theorem, if we specialize it such that r = n and \\eta = \\nu = n, then the Lanczos method requires just O(n^2) time and O(n) space to execute. In other words, the Lanczos method drops both the time and space complexity4 of obtaining spectral information by order of magnitude over similar eigen-algorithms that decompose A directly.\nTo see why this is true, note that a symmetric tridiagonal matrix is fully characterized by its diagonal and subdiagonal terms, which requires just O(n) space. If we assume that v \\mapsto Av \\sim O(n), then carrying out the recurrence clearly takes at most O(n^2) time, since there are most n such vectors \\{q_i\\}_{i=1}^n to generate!\nNow, if we need to store all of Y or Q explicitly, we clearly need O(n^2) space to do so. However, if we only need the eigen-values \\Lambda(A) (and not their eigen-vectors U), then we may execute the recurrence keeping at most three vectors \\{q_{j-1}, q_{j}, q_{j+1}\\} in memory at any given time. Since each of these is O(n) is size, the claim of O(n) space is justified!",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "theory/lanczos.html#rayleigh-ritz-approximations",
    "href": "theory/lanczos.html#rayleigh-ritz-approximations",
    "title": "The Lanczos method",
    "section": "Rayleigh-Ritz approximations",
    "text": "Rayleigh-Ritz approximations\nSuppose instead of constructing the full T \\in \\mathbb{R}^{n \\times n}, we stop at the j^{\\text{th}} iteration, where 1 \\leq j &lt; n.\n\nT_j = \\mathrm{tridiag}\\Bigg(\n\\begin{array}{ccccccccc}\n& \\beta_2 & & \\beta_3 & & \\cdot & & \\beta_j & \\\\\n\\alpha_1 & & \\alpha_2 & & \\cdot & & \\cdot & & \\alpha_j \\\\\n& \\beta_2 & & \\beta_3 & & \\cdot & & \\beta_j &\n\\end{array}\n\\Bigg)\n\nIt is natural to assume that the eigenvalues of T_j approximate some of eigenvalues of \\Lambda(A). This intuition not only turns out to be true, but in fact the eigenvalues of T_j are known to be optimal approximations of \\Lambda(A) under many appealing notions of optimality. Thus, if we need only to approximate the spectrum of A, we may potentially do so using just j &lt;&lt; n iterations; indeed, this is the hallmark of the iterative approach to obtaining eigenvalues:",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "theory/lanczos.html#wait-isnt-t-arbitrary",
    "href": "theory/lanczos.html#wait-isnt-t-arbitrary",
    "title": "The Lanczos method",
    "section": "Wait, isn’t T arbitrary?",
    "text": "Wait, isn’t T arbitrary?\nUnfortunately—and unlike the spectral decomposition2—there is no canonical choice of T. Indeed, as T is a family with n - 1 degrees of freedom and v \\in \\mathbb{R}^n was chosen arbitrarily, there are infinitely many essentially distinct such decompositions.\nNot all hope is lost though, as it turns out that T is actually fully characterized by v. To see this, notice that since Q is an orthogonal matrix, we have:\n Q Q^T = I_n = [e_1, e_2, \\dots, e_n] \nBy extension, given an initial pair (A, q_1) satisfying \\lVert q_1 \\rVert = 1, the following holds:\n\nK_n(A, q_1) = Q Q^T K_n(A, q_1) = Q[ \\, e_1 \\mid T e_1 \\mid T^2 e_1 \\mid \\dots \\mid T^{n-1} e_1 \\, ]\n\n…this is actually a QR factorization, which is essentially unique! Indeed, the Implicit Q Theorem asserts that if an upper Hessenburg matrix T \\in \\mathbb{R}^{n \\times n} has only positive elements on its first subdiagonal and there exists an orthogonal matrix Q such that Q^T A Q = T, then Q and T are uniquely determined3 by (A, q_1).",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "theory/lanczos.html#finite-precision",
    "href": "theory/lanczos.html#finite-precision",
    "title": "The Lanczos method",
    "section": "Finite-precision",
    "text": "Finite-precision\nThough elegant as the Lanczos method is, the complexity statements and much of the theory holds only in exact arith",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "basic/integration.html#python-usage",
    "href": "basic/integration.html#python-usage",
    "title": "Integration",
    "section": "",
    "text": "primate supports a variety of matrix-types of the box, including numpy ndarray’s, compressed sparse matrices (a lá SciPy), and LinearOperators—the latter enables the use of matrix free operators.\nOutside of the natively types above, the basic requirements for any operator A to be used with e.g. the Lanczos method in primate are:\n\nA method A.matvec(input: ndarray) -&gt; ndarray implementing v \\mapsto Av\nAn attribute A.shape -&gt; tuple[int, int] giving the output/input dimensions of A\n\n\n\nHere’s an example of a simple operator representing a Diagonal matrix, which inherits a .matvec() method by following the subclassing rules of SciPy’s LinearOperator:\nimport numpy as np \nfrom numpy.typing import ArrayLike\nfrom scipy.sparse.linalg import LinearOperator \n\nclass DiagonalOp(LinearOperator):\n  diag: np.ndarray = None\n  \n  def __init__(self, d: ArrayLike, dtype = None):\n    self.diag = np.array(d)\n    self.shape = (len(d), len(d))\n    self.dtype = np.dtype('float32') if dtype is None else dtype\n\n  def _matvec(self, x: ArrayLike) -&gt; np.ndarray:\n    out = self.diag * np.ravel(x)\n    return out.reshape(x.shape)",
    "crumbs": [
      "Basics",
      "Integration"
    ]
  },
  {
    "objectID": "basic/integration.html#c-usage",
    "href": "basic/integration.html#c-usage",
    "title": "Integration",
    "section": "C++ usage",
    "text": "C++ usage\nSimilarly, to get started calling any matrix-free function provided by primate on the C++ side, such hutch or lanczos, simply pass any type with .shape() and .matvec() member functions:\nclass LinOp {\n  int nrow, ncol;\n  \n  LinOp(int nr, int nc) : nrow(nr), ncol(nc) {}\n  \n  void matvec(const float* input, float* output) const {\n    ... // implementation details \n  }\n\n  void shape() const { return std::make_pair(nrow, ncol); }\n}\nIt’s up to you to ensure shape() yields the correct size; primate will supply vectors to input of size .shape().second (number of columns) and guarantees the pointer to the output will be at least shape().first (number of rows), no more.\nTo read more about how semantics extend to the C++ side as well via C++20 concepts—see the C++ integration guide. If you’re using pybind11 and you want to extend primate’s Python API to work natively with linear operator implemented in C++, see the pybind11 integration guide.",
    "crumbs": [
      "Basics",
      "Integration"
    ]
  },
  {
    "objectID": "basic/usage.html",
    "href": "basic/usage.html",
    "title": "primate",
    "section": "",
    "text": "This page gives a walk-through of how to use primate to do implicit trace estimation."
  },
  {
    "objectID": "theory/intro.html",
    "href": "theory/intro.html",
    "title": "Introduction to trace estimation with primate",
    "section": "",
    "text": "primate contains an extensible implementations of a variety of methods for estimating quantities from matrix functions. One popular application of such estimators is the approximation of certain trace quantities.\nIn the examples below, I’ll use a simple (random) positive-definite matrix A \\in \\mathbb{R}^{n \\times n}.\nfrom primate.random import symmetric\nA = symmetric(150, pd = True)\nBy default, symmetric normalizes A such that the eigenvalues are uniformly distributed in the interval [0, 1]. For reference, the spectrum of A looks as follows:",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "theory/intro.html#basics-trace-estimation",
    "href": "theory/intro.html#basics-trace-estimation",
    "title": "Introduction to trace estimation with primate",
    "section": "Basics: Trace estimation",
    "text": "Basics: Trace estimation\n\nSuppose we wanted to compute the trace of A. From its definition, we have a variety of means to do it: \\mathrm{tr}(A) \\triangleq \\sum\\limits_{i=1}^n A_{ii} = \\sum\\limits_{i=1}^n \\lambda_i = \\sum\\limits_{i=1}^n e_i^T A e_i \nwhere, in the right-most sum, we’re using e_i to represent the ith identity vector:  A_{ii} = e_i^T A e_i, \\quad e_i = [0, \\dots, 0, \\underbrace{1}_{i}, 0, \\dots, 0] \nUsing numpy, we can verify all of these give the exact trace of A:\n\n\nimport numpy as np\neye = lambda i: np.ravel(np.eye(1, 150, k=i))\nprint(f\"Direct: {np.sum(A.diagonal()):.8f}\")\nprint(f\"Eigen:  {np.sum(np.linalg.eigvalsh(A)):.8f}\")\nprint(f\"Matvec: {sum([eye(i) @ A @ eye(i) for i in range(150)]):.8f}\")\n\nDirect: 75.69739746\nEigen:  75.69739746\nMatvec: 75.69739746\n\n\nWhile (1) trivially yields \\mathrm{tr}(A) in (optimal) O(n) time, there exist situations where the diagonal entries of A are not available—particularly in the large-scale regime. In contrast, though both (2) and (3) are less efficient, they yield alternative ways nonetheless of obtaining \\mathrm{tr}(A).",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "theory/intro.html#the-implicit-trace-estimation-problem",
    "href": "theory/intro.html#the-implicit-trace-estimation-problem",
    "title": "Introduction to trace estimation with primate",
    "section": "The implicit trace estimation problem",
    "text": "The implicit trace estimation problem\nThe implicit trace estimation problem can be stated as follows:\n\nGiven access to a square matrix A \\in \\mathbb{R}^{n \\times n} via its matrix-vector product operator x \\mapsto Ax, estimate its trace \\mathrm{tr}(A) = \\sum\\limits_{i=1}^n A_{ii}\n\nPutting aside the fact that each v \\mapsto Av takes O(n^2) here, observe this last approach is pretty inefficient in general as most of the entries of v are zero. The zero components in v still yield inner product computations, despite not contributing to the trace estimate at all. One idea, accredited first to A. Girard and then studied more in-depth by M.F. Huchinson, is to use random sign vector v \\in \\{-1, +1\\}^{n}.\n\\mathtt{tr}(A) = \\mathbb{E}[v^T A v] \\approx \\frac{1}{n_v}\\sum\\limits_{i=1}^{n_v} v_i^\\top A v_i = n_v^{-1} \\cdot V^\\top A V\nLet’s see how this fares using, let’s say, \\frac{1}{4} the number of matvecs:\n\nn: int = A.shape[0]\ntrace_estimate = 0.0\nfor j in range(n // 4):\n  v = np.random.choice([-1, +1], size=n)\n  trace_estimate += v @ A @ v\nprint(trace_estimate / (n // 4))\n\n75.21454596302897\n\n\nNot bad!",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "theory/intro.html#the-statistical-side",
    "href": "theory/intro.html#the-statistical-side",
    "title": "Introduction to trace estimation with primate",
    "section": "The statistical side",
    "text": "The statistical side\nThis forms an unbiased estimator of \\mathrm{tr}(A); to see this, its sufficient to combine the linearity of expectation, the cyclic-property of the trace function, and the aforementioned isotropy conditions \\mathtt{tr}(A) = \\mathtt{tr}(A I) = \\mathtt{tr}(A \\mathbb{E}[v v^T]) = \\mathbb{E}[\\mathtt{tr}(Avv^T)] = \\mathbb{E}[\\mathtt{tr}(v^T A v)] = \\mathbb{E}[v^T A v]",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "theory/intro.html#ex-logdet-computation",
    "href": "theory/intro.html#ex-logdet-computation",
    "title": "Introduction to trace estimation with primate",
    "section": "Ex: Logdet computation",
    "text": "Ex: Logdet computation\nThe basic way to approximate the trace of a matrix function is to simply set fun to the name the spectral function. For example, to approximate the log-determinant:\n\nfrom primate.trace import hutch\n\n## Log-determinant\nlogdet_test = hutch(A, fun=\"log\", maxiter=25)\nlogdet_true = np.sum(np.log(np.linalg.eigvalsh(A)))\n\nprint(f\"Logdet (exact):  {logdet_true}\")\nprint(f\"Logdet (approx): {logdet_test}\")\n\nLogdet (exact):  -148.3218440234385\nLogdet (approx): -148.12677777759933\n\n\nEven using n / 6 matvecs, we get a decent approximation. But how good is this estimate?\nTo get a slightly better idea, you can set verbose=True:\n\nest = hutch(A, fun=\"log\", maxiter=25, verbose=True)\n\nGirard-Hutchinson estimator (fun=log, deg=20, quad=fttr)\nEst: -146.538 +/- 5.51 (95% CI), CV: 2%, Evals: 25 [R]\n\n\nThe first line of the statement contains fixed about the estimator:\n\nThe type of estimator (Girard-Hutchinson)\nThe spectral function being approximated (log)\nThe degree of the polynomial approximation (20)\nThe quadrature method used (‘fttr’)\n\nThe second line prints the runtime information about the samples:\n\nThe final trace estimate\nIts margin of error\nThe coefficient of variation (aka the relative std. deviation)\nThe number of samples used + their distribution prefix (‘R’ for rademacher)\n\n\nest = hutch(A, fun=\"log\", maxiter=100, plot=True)",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  }
]