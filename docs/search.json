[
  {
    "objectID": "reference/get_include.html",
    "href": "reference/get_include.html",
    "title": "get_include",
    "section": "",
    "text": "get_include\nget_include()\nReturn the directory that contains the primate’s *.h header files.\nExtension modules that need to compile against primate should use this function to locate the appropriate include directory.\nNotes: When using distutils, for example in setup.py: python     import primate     ...     Extension('extension_name', ..., include_dirs=[primate.get_include()])     ... Or with meson-python, for example in meson.build: meson     ...     run_command(py, ['-c', 'import primate; print(primate.get_include())', check : true).stdout().strip()     ..."
  },
  {
    "objectID": "reference/primate.trace.sl_trace.html",
    "href": "reference/primate.trace.sl_trace.html",
    "title": "sl_trace",
    "section": "",
    "text": "trace.sl_trace(A, fun='identity', maxiter=200, deg=20, atol=None, rtol=None, stop=['confidence', 'change'], orth=0, confidence=0.95, pdf='rademacher', rng='lcg', seed=-1, num_threads=0, verbose=False, info=False, plot=False, **kwargs)\nEstimates the trace of a matrix function f(A) using stochastic Lanczos quadrature (SLQ).\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nA\nndarray, sparray, or LinearOperator\nreal symmetric operator.\nrequired\n\n\nfun\nstr or typing.Callable\nreal-valued function defined on the spectrum of A.\n= \"identity\"\n\n\nmaxiter\nint\nMaximum number of random vectors to sample for the trace estimate.\n= 10\n\n\ndeg\nint\nDegree of the quadrature approximation.\n20\n\n\natol\nfloat\nAbsolute tolerance to signal convergence for early-stopping. See details.\n= None\n\n\nrtol\nfloat\nRelative tolerance to signal convergence for early-stopping. See details.\n= 1e-2\n\n\nstop\nstr\nEarly-stopping criteria to test estimator convergence. See details.\n= \"confidence\"\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against when building the Krylov basis.\n0\n\n\nconfidence\nfloat\nConfidence level to Only used when stop = “confidence”.\n= 0.95\n\n\npdf\n‘rademacher’, ‘normal’\nChoice of zero-centered distribution to sample random vectors from.\n'rademacher'\n\n\nrng\nstr\nRandom number generator to use. Defaults to PCG64 generator.\n= \"pcg\"\n\n\nseed\nint\nSeed to initialize the entropy source. Use non-negative integers for reproducibility.\n= -1\n\n\nnum_threads\nint\nNumber of threads to use to parallelize the computation. Use values &lt;= 0 to maximize the number of threads.\n0\n\n\nplot\nbool\nIf true, plots the samples of the trace estimate along with their convergence characteristics.\n= False\n\n\ninfo\nbool\nIf True, returns a dictionary containing all relevant information about the computation.\nFalse\n\n\nkwargs\ndict\nadditional key-values to parameterize the chosen function ‘fun’.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nEstimate of the trace of the matrix function f(A).\n\n\n(dict, optional)\nIf ‘info = True’, additional information about the computation.\n\n\n\n\n\n\nlanczos : the lanczos algorithm.\n\n\n\n[1] Ubaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099.",
    "crumbs": [
      "API Reference",
      "Trace",
      "SL Trace"
    ]
  },
  {
    "objectID": "reference/primate.trace.sl_trace.html#parameters",
    "href": "reference/primate.trace.sl_trace.html#parameters",
    "title": "sl_trace",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nA\nndarray, sparray, or LinearOperator\nreal symmetric operator.\nrequired\n\n\nfun\nstr or typing.Callable\nreal-valued function defined on the spectrum of A.\n= \"identity\"\n\n\nmaxiter\nint\nMaximum number of random vectors to sample for the trace estimate.\n= 10\n\n\ndeg\nint\nDegree of the quadrature approximation.\n20\n\n\natol\nfloat\nAbsolute tolerance to signal convergence for early-stopping. See details.\n= None\n\n\nrtol\nfloat\nRelative tolerance to signal convergence for early-stopping. See details.\n= 1e-2\n\n\nstop\nstr\nEarly-stopping criteria to test estimator convergence. See details.\n= \"confidence\"\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against when building the Krylov basis.\n0\n\n\nconfidence\nfloat\nConfidence level to Only used when stop = “confidence”.\n= 0.95\n\n\npdf\n‘rademacher’, ‘normal’\nChoice of zero-centered distribution to sample random vectors from.\n'rademacher'\n\n\nrng\nstr\nRandom number generator to use. Defaults to PCG64 generator.\n= \"pcg\"\n\n\nseed\nint\nSeed to initialize the entropy source. Use non-negative integers for reproducibility.\n= -1\n\n\nnum_threads\nint\nNumber of threads to use to parallelize the computation. Use values &lt;= 0 to maximize the number of threads.\n0\n\n\nplot\nbool\nIf true, plots the samples of the trace estimate along with their convergence characteristics.\n= False\n\n\ninfo\nbool\nIf True, returns a dictionary containing all relevant information about the computation.\nFalse\n\n\nkwargs\ndict\nadditional key-values to parameterize the chosen function ‘fun’.\n{}",
    "crumbs": [
      "API Reference",
      "Trace",
      "SL Trace"
    ]
  },
  {
    "objectID": "reference/primate.trace.sl_trace.html#returns",
    "href": "reference/primate.trace.sl_trace.html#returns",
    "title": "sl_trace",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nfloat\nEstimate of the trace of the matrix function f(A).\n\n\n(dict, optional)\nIf ‘info = True’, additional information about the computation.",
    "crumbs": [
      "API Reference",
      "Trace",
      "SL Trace"
    ]
  },
  {
    "objectID": "reference/primate.trace.sl_trace.html#see-also",
    "href": "reference/primate.trace.sl_trace.html#see-also",
    "title": "sl_trace",
    "section": "",
    "text": "lanczos : the lanczos algorithm.",
    "crumbs": [
      "API Reference",
      "Trace",
      "SL Trace"
    ]
  },
  {
    "objectID": "reference/primate.trace.sl_trace.html#reference",
    "href": "reference/primate.trace.sl_trace.html#reference",
    "title": "sl_trace",
    "section": "",
    "text": "[1] Ubaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099.",
    "crumbs": [
      "API Reference",
      "Trace",
      "SL Trace"
    ]
  },
  {
    "objectID": "reference/random.normal.html",
    "href": "reference/random.normal.html",
    "title": "random.normal",
    "section": "",
    "text": "random.normal(size, rng='splitmix64', seed=-1, dtype=np.float32)\nGenerates random vectors from the standard normal distribution.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsize\nint or tuple\nOutput shape to generate.\nrequired\n\n\nrng\nstr = “splitmix64”\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint = -1\nSeed for the generator. Use -1 to for random (non-deterministic) behavior.\n-1\n\n\ndtype\ndtype = float32\nFloating point dtype for the output. Must be float32 or float64.\nnp.float32\n\n\n\n\n\n\nnp.narray Randomly generated matrix of shape size with entries in { -1, 1 }.",
    "crumbs": [
      "API Reference",
      "Random",
      "Normal"
    ]
  },
  {
    "objectID": "reference/random.normal.html#parameters",
    "href": "reference/random.normal.html#parameters",
    "title": "random.normal",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsize\nint or tuple\nOutput shape to generate.\nrequired\n\n\nrng\nstr = “splitmix64”\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint = -1\nSeed for the generator. Use -1 to for random (non-deterministic) behavior.\n-1\n\n\ndtype\ndtype = float32\nFloating point dtype for the output. Must be float32 or float64.\nnp.float32",
    "crumbs": [
      "API Reference",
      "Random",
      "Normal"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html",
    "href": "reference/diagonalize.lanczos.html",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "diagonalize.lanczos(A, v0=None, deg=None, rtol=1e-08, orth=0, sparse_mat=False, return_basis=False, seed=None, dtype=None)\nLanczos method for matrix tridiagonalization.\nThis function implements Paiges A27 variant (1) of the Lanczos method for tridiagonalizing linear operators, with additional modifications to support varying degrees of re-orthogonalization. In particular, orth=0 corresponds to no re-orthogonalization, orth &lt; deg corresponds to partial re-orthogonalization, and orth &gt;= deg corresponds to full re-orthogonalization.\n\n\nThe Lanczos method iteratively builds a tridiagonal matrix T of a symmetric A via an orthogonal change-of-basis Q:  Q^T A Q  = T  Unlike other Lanczos implementations (e.g. SciPy’s eigsh), which includes e.g. sophisticated restarting, deflation, and selective-reorthogonalization steps, this method simply executes deg steps of the Lanczos method with the supplied v0 and returns the resulting tridiagonal matrix.\nDiagonalizing T via e.g. scipy.linalg.eigh_tridiagonal yields Rayleigh-Ritz approximations of the eigenvalues of A, though note no checking is performed for ‘ghost’ or already converged eigenvalues. To increase the accuracy of these eigenvalue approximation, try increasing orth and deg. Supplying either negative values or values larger than deg for orth will result in full re-orthogonalization, though note the number of matvecs scales linearly with deg and the number of inner-products scales quadratically with orth.\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nA\nscipy.sparse.linalg.LinearOperator or ndarray or sparray\nSymmetric operator to tridiagonalize.\nrequired\n\n\nv0\nndarray\nInitial vector to orthogonalize against.\nNone\n\n\ndeg\nint\nSize of the Krylov subspace to expand.\nNone\n\n\nrtol\nfloat\nRelative tolerance to consider the invariant subspace as converged.\n1e-8\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against.\n0\n\n\nsparse_mat\nbool\nWhether to output the tridiagonal matrix as a sparse matrix.\nFalse\n\n\nreturn_basis\nbool\nIf True, returns the Krylov basis vectors Q.\nFalse\n\n\ndtype\ndtype\nThe precision dtype to specialize the computation.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntuple\nA tuple (a,b) parameterizing the diagonal and off-diagonal of the tridiagonal matrix. If return_basis=True, the tuple (a,b), Q is returned, where Q represents an orthogonal basis for the degree-deg Krylov subspace.\n\n\n\n\n\n\nscipy.linalg.eigh_tridiagonal : Eigenvalue solver for real symmetric tridiagonal matrices. operator.matrix_function : Approximates the action of a matrix function via the Lanczos method.\n\n\n\n\nPaige, Christopher C. “Computational variants of the Lanczos method for the eigenproblem.” IMA Journal of Applied Mathematics 10.3 (1972): 373-381.",
    "crumbs": [
      "API Reference",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#parameters",
    "href": "reference/diagonalize.lanczos.html#parameters",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nA\nscipy.sparse.linalg.LinearOperator or ndarray or sparray\nSymmetric operator to tridiagonalize.\nrequired\n\n\nv0\nndarray\nInitial vector to orthogonalize against.\nNone\n\n\ndeg\nint\nSize of the Krylov subspace to expand.\nNone\n\n\nrtol\nfloat\nRelative tolerance to consider the invariant subspace as converged.\n1e-8\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against.\n0\n\n\nsparse_mat\nbool\nWhether to output the tridiagonal matrix as a sparse matrix.\nFalse\n\n\nreturn_basis\nbool\nIf True, returns the Krylov basis vectors Q.\nFalse\n\n\ndtype\ndtype\nThe precision dtype to specialize the computation.\nNone",
    "crumbs": [
      "API Reference",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#description",
    "href": "reference/diagonalize.lanczos.html#description",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "This function implements the Lanczos method, or as Lanczos called it, the method of minimized iterations.",
    "crumbs": [
      "API Reference",
      "Diagonalize",
      "Lanczos"
    ]
  },
  {
    "objectID": "imate_compare.html",
    "href": "imate_compare.html",
    "title": "Comparison to imate",
    "section": "",
    "text": "primate’s namesake (and some of the original code1) was inspired from the (excellent) imate package, prompting questions about their differences. In general, primate was developed with slightly different goals in mind than imate, most of which have to do with things like integrability, extensibility, and choice of FFI / build system.\nNotable differences between the two packages include:\nOne motivation for developing primate was to modularize and streamline access to Lanczos-based methods, which is achieved through the use of things like function templates, type erasure, and header-only definitions. These modifications not only simplify access from user (i.e. dependent) packages, but they enable native support for arbitrary classes adhering to the LinearOperator concept. For more details on this, see the integration guides.",
    "crumbs": [
      "Basics",
      "Comparison to *imate*"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Package overview",
    "section": "",
    "text": "primate, short for Probabalistic Implicit Matrix Trace Estimator, is a Python package that provides estimators of quantities derived from matrix functions; that is, matrices parameterized by functions:\nf(A) \\triangleq U f(\\Lambda) U^{\\intercal}, \\quad \\quad f : [a,b] \\to \\mathbb{R}\nEstimator approximations are obtained via the Lanczos1 and stochastic Lanczos quadrature2 methods, which are well-suited for sparse or structured operators supporting fast v \\mapsto Av actions.\nNotable features of primate include:\nprimate was partially inspired by the imate package—for a comparison of the two, see here.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#compilation-development",
    "href": "index.html#compilation-development",
    "title": "Package overview",
    "section": "",
    "text": "primate relies on BLAS libraries\n\npipx run cibuildwheel –platform linux",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "integration/cpp_integration.html",
    "href": "integration/cpp_integration.html",
    "title": "C++ Integration",
    "section": "",
    "text": "To get started calling any matrix-free function provided by primate, such sl_trace or lanczos, simply pass any type with a .shape() and .matvec() member functions defined like so:\nIt’s up to you to ensure shape() yields the correct size; primate will supply vectors to input of size .shape().second (number of columns) and guarantees the pointer to the output will be at least shape().first (number of rows), no more.",
    "crumbs": [
      "Integration Guide",
      "Usage from C++"
    ]
  },
  {
    "objectID": "integration/cpp_integration.html#the-linearoperator-concept",
    "href": "integration/cpp_integration.html#the-linearoperator-concept",
    "title": "C++ Integration",
    "section": "The LinearOperator concept",
    "text": "The LinearOperator concept\nprimate’s generic API is enabled through C++20 concepts. Thus, the more exact statement is that any type respecting the LinearOperator concept shown below can be passed:\nusing FP = std::floating_point; \ntemplate &lt; typename T, FP float_t = typename T::value_type &gt;\nconcept LinearOperator = requires(T A, const float_t* v, float_t* o) {\n  { A.matvec(v, o) }; // o = A v\n  { A.shape() } -&gt; std::convertible_to&lt; std::pair&lt; size_t, size_t &gt; &gt;;\n};\nAn instance A of type T is said to support the LinearOperator concept if it has:\n\nA method Av \\mapsto o, with signature A.matvec(const float_t* v, float_t* o)\nA method yielding (\\mathrm{card}(o), \\mathrm{card}(v)), with signatureA.shape() -&gt; pair&lt; ... &gt;\n\nshape() should yield a pair (n,m) representing the sizes of the output and input vectors, respectively. This corresponds to the number of rows and columns in the matrix setting.",
    "crumbs": [
      "Integration Guide",
      "Usage from C++"
    ]
  },
  {
    "objectID": "integration/cpp_integration.html#other-concepts",
    "href": "integration/cpp_integration.html#other-concepts",
    "title": "C++ Integration",
    "section": "Other Concepts",
    "text": "Other Concepts\nDepending on the problem at hand, the supplied operator may need to meet other constraints. Here’s a short list additional operator concepts:\n\n\n\n\n\n\n\n\n\nConcept\nSupports\nSignature\nRequires\n\n\n\n\nLinearOperator\nA v \\mapsto o\nA.matvec(v, o)\nNA\n\n\nAdjointOperator\nA^T v \\mapsto o\nA.rmatvec(v, o)\nLinearOperator\n\n\nAdditiveOperator\no \\gets o + \\alpha Av\nA.matvec_add(v, alpha, o)\nLinearOperator\n\n\nAffineOperator\nSets t s.t. A + tB\nA.set_parameter(t)\nLinearOperator\n\n\n\nRespecting these constraints is opt-in: if your operator is symmetric and you only need access to the Lanczos method, then any satisfying the LinearOperator concept is sufficient.",
    "crumbs": [
      "Integration Guide",
      "Usage from C++"
    ]
  },
  {
    "objectID": "guide/slq_guide.html",
    "href": "guide/slq_guide.html",
    "title": "SLQ Trace guide",
    "section": "",
    "text": "primate offers an extensible implementation of the stochastic Lanczos method (SLQ). There are many algorithms named the “stochastic Lanczos quadrature” in the literature; though each is typically related, they often have distinct goals. Pseudocode for a generic form of SLQ is given below:\nThis guide walks through the SLQ method implemented in primate, which can be specialized for different purposes.",
    "crumbs": [
      "User Guide",
      "The SLQ method"
    ]
  },
  {
    "objectID": "guide/slq_guide.html#slq-as-a-function-template",
    "href": "guide/slq_guide.html#slq-as-a-function-template",
    "title": "SLQ Trace guide",
    "section": "SLQ as a function template",
    "text": "SLQ as a function template\nBelow is the full signature of the SLQ function template:\n// Stochastic Lanczos quadrature method\ntemplate&lt; std::floating_point F, LinearOperator Matrix, ThreadSafeRBG RBG &gt;\nvoid slq (\n  const Matrix& A,                    // Any *LinearOperator*\n  const function&lt; F(int,F*,F*) &gt;& f,  // Generic function\n  const function&lt; bool(int) &gt;& stop,  // Early-stop function\n  const int nv,                       // Num. of sample vectors\n  const Distribution dist,            // Sample vector distribution\n  RBG& rng,                           // Random bit generator\n  const int lanczos_degree,           // Krylov subspace degree\n  const F lanczos_rtol,               // Lanczos residual tolerance\n  const int orth,                     // Add. vectors to orthogonalize\n  const int ncv,                      // Num. of Lanczos vectors\n  const int num_threads,              // # threads to allocate \n  const int seed                      // Seed for RNG \n)\nMany of the runtime arguments are documented in the lanczos or sl_trace docs; the compile-time (template) parameters are:\n\nThe floating point type (e.g. float, double, long double)\nThe operator type (e.g. Eigen::MatrixXf, torch::Tensor, LinOp)\nThe multi-threaded random number generator (e.g. ThreadedRNG64)\n\nNote any type combination satisfying these concepts (e.g. std::floating_point, LinearOperator) generates a function specialized of said types at compile-time—this is known as template instantiation.",
    "crumbs": [
      "User Guide",
      "The SLQ method"
    ]
  },
  {
    "objectID": "guide/slq_guide.html#generality-via-function-passing",
    "href": "guide/slq_guide.html#generality-via-function-passing",
    "title": "SLQ Trace guide",
    "section": "Generality via function passing",
    "text": "Generality via function passing\nGiven a valid set of parameters, the main body of the SLQ looks something like this:\n  bool stop_flag = false;\n  #pragma omp parallel shared(stop_flag)\n  {\n    // &lt; allocations for Q, alpha, beta, etc. &gt; \n    int tid = omp_get_thread_num(); // thread-id \n    \n    #pragma omp for\n    for (i = 0; i &lt; nv; ++i){\n      if (stop_flag){ continue; }\n      generate_isotropic&lt; F &gt;(...); // populates q\n      lanczos_recurrence&lt; F &gt;(...); // populates alpha + beta\n      lanczos_quadrature&lt; F &gt;(...); // populates nodes + weights\n      f(i, q, Q, nodes, weights);   // Run user-supplied function \n      #pragma omp critical\n      {\n        stop_flag = stop(i);        // Checks for early-stopping\n      }\n    } // end for\n  } // end parallel \nThere are two functions that can be used for generalizing SLQ for different purposes.\nThe first generic function f can read, save, or modify the information available from the iteration index i, the isotropic vector q, the Lanczos vectors Q, and/or the quadrature information nodes, weights. Note this function is run in the parallel section.\nThe second is a boolean-valued function stop which can be used to stop the iteration early, for example if convergence has been achieved according to some rule. Since this is run in the critical section, it is called sequentially.",
    "crumbs": [
      "User Guide",
      "The SLQ method"
    ]
  },
  {
    "objectID": "guide/slq_guide.html#using-slq-to-estimate-mathrmtrfa",
    "href": "guide/slq_guide.html#using-slq-to-estimate-mathrmtrfa",
    "title": "SLQ Trace guide",
    "section": "Using SLQ to estimate \\mathrm{tr}(f(A))",
    "text": "Using SLQ to estimate \\mathrm{tr}(f(A))\nThe SLQ method is often used to estimate the trace of an arbitrary matrix function:\n \\mathrm{tr}(f(A)), \\quad \\text{ where } f(A) = U f(\\Lambda) U^T \nIt’s has been shown1 that the information obtained by the Lanczos method is sufficient to obtained a Gaussian quadrature approximation of the empirical spectral measure of A. By sampling zero-mean vectors satisfying \\mathbb{E}[v v^T] = I, one can obtain estimates of the trace above: \\operatorname{tr}(f(A)) \\approx \\frac{n}{\\mathrm{n}_{\\mathrm{v}}} \\sum_{l=1}^{\\mathrm{n}_{\\mathrm{v}}}\\left(\\sum_{k=0}^m\\left(\\tau_k^{(l)}\\right)^2 f\\left(\\theta_k^{(l)}\\right)\\right)\nIt turns out averaging these trace estimates yields unbiased, Girard-Hutchinson estimator of the trace. To see why this estimator is unbiased, note that:  \\mathtt{tr}(A) = \\mathbb{E}[v^T A v] \\approx \\frac{1}{n_v}\\sum\\limits_{i=1}^{n_v} v_i^\\top A v_i \nThus, all we need to do to estimate the trace of a matrix function is multiply and sum the quadrature nodes and weights output by SLQ.",
    "crumbs": [
      "User Guide",
      "The SLQ method"
    ]
  },
  {
    "objectID": "guide/slq_guide.html#sl_trace-method",
    "href": "guide/slq_guide.html#sl_trace-method",
    "title": "SLQ Trace guide",
    "section": "sl_trace method",
    "text": "sl_trace method\nTo see how these formulas are actually implemented with the generic SLQ implementation, here’s an abbreviated form of the sl_trace function implemented by primate:\ntemplate&lt; std::floating_point F, LinearOperator Matrix, ThreadSafeRBG RBG &gt;\nvoid sl_trace(\n  const Matrix& A, const std::function&lt; F(F) &gt; sf, RBG& rbg, \n  const int nv, const int dist, const int engine_id, const int seed,\n  const int deg, const float lanczos_rtol, const int orth, const int ncv,\n  const F atol, const F rtol\n  F* estimates\n){  \n  using VectorF = Eigen::Array&lt; F, Dynamic, 1&gt;;\n\n  // Parameterize the trace function (runs in parallel)\n  auto trace_f = [&](int i, F* q, F* Q, F* nodes, F* weights){\n    Map&lt; VectorF &gt; nodes_v(nodes, deg, 1);     // no-op\n    Map&lt; VectorF &gt; weights_v(weights, deg, 1); // no-op\n    nodes_v.unaryExpr(sf);\n    estimates[i] = (nodes_v * weights_v).sum();\n  };\n  \n  // Convergence checking like scipy.integrate.quadrature\n  int n = 0;\n  F mu_est = 0.0, mu_pre = 0.0;\n  const auto early_stop = [&](int i) -&gt; bool {\n    ++n; // Number of estimates\n    mu_est = (1.0 / F(n)) * (estimates[i] + (n - 1) * mu_pre); \n    bool atol_check = abs(mu_est - mu_pre) &lt;= atol;\n    bool rtol_check = abs(mu_est - mu_pre) / mu_est &lt;= rtol; \n    mu_pre = mu_est; \n    return atol_check || rtol_check;\n  };\n\n  // Execute the stochastic Lanczos quadrature with the trace function \n  slq&lt; float &gt;(A, trace_f, early_stop, ...);\n}\nAs before, two functions are used to parameterize the slq method.\nThe first (trace_f) applies an arbitrary spectral function sf to the Rayleigh-Ritz values obtained by the Lanczos tridiagonalization of A(or equivalently, the nodes of the Gaussian quadrature). These are the \\theta’s in the pseudocode above. When multiplied by the weights of the quadrature, the corresponding sum forms an estimate of the trace of the matrix function.\nThe second function early_stop is used to check for convergence of the estimator. First, it uses the trace estimate x_n to update the sample mean \\mu_n via the formula:\n \\mu_n = n^{-1} [x_n + (n - 1)\\mu_{n-1}] \nThen, much in the same way the quadrature function from scipy.integrate approximates a definite integral, it checks for convergence using the absolute and relative tolerances supplied by the user. Returning true signals convergence, stopping the iteration early.",
    "crumbs": [
      "User Guide",
      "The SLQ method"
    ]
  },
  {
    "objectID": "guide/slq_guide.html#references",
    "href": "guide/slq_guide.html#references",
    "title": "SLQ Trace guide",
    "section": "References",
    "text": "References",
    "crumbs": [
      "User Guide",
      "The SLQ method"
    ]
  },
  {
    "objectID": "integration/python_integration.html",
    "href": "integration/python_integration.html",
    "title": "Python Integration",
    "section": "",
    "text": "To demonstrate the SLQ method in Python, we start with a simple symmetric matrix A \\in \\mathbb{R}^{n \\times n}.\n\nimport numpy as np\nfrom primate.random import symmetric\nA = symmetric(150, psd = True)\n\nThis generates a random positive semi-definite matrix with eigenvalues in the interval [0, 1].\n\nfrom primate.trace import sl_trace\ntrace_estimate = sl_trace(A)\nprint(A.trace()) \nprint(trace_estimate)\n\n78.57591588717017\n78.428444\n\n\n\n# tr_est = np.mean(estimates)\n# print(f\"Error: {abs(tr_est - A.trace()):.5}\")\n# print(f\"Samples std. deviation: {estimates.std(ddof=1)}\")\n# print(f\"Estimator standard error: {estimates.std(ddof=1)/np.sqrt(len(estimates))}\")",
    "crumbs": [
      "Integration Guide",
      "Usage from Python"
    ]
  },
  {
    "objectID": "integration/pybind11_integration.html",
    "href": "integration/pybind11_integration.html",
    "title": "pybind11 Integration",
    "section": "",
    "text": "If you’re using pybind11, you can easily incorporate your own custom linear operator / matrix function pair using primates binding headers.",
    "crumbs": [
      "Integration Guide",
      "Integrating with pybind11"
    ]
  },
  {
    "objectID": "integration/pybind11_integration.html#example-log-determinant",
    "href": "integration/pybind11_integration.html#example-log-determinant",
    "title": "pybind11 Integration",
    "section": "Example: Log determinant",
    "text": "Example: Log determinant\nFor explanatory purposes, the following code outline how to call the trace estimator to compute the log determinant using a custom user-implemented operator LinOp:\n#include &lt;cmath&gt;                              // std::log\n#include &lt;_linear_operator/linear_operator.h&gt; // LinearOperator\n#include &lt;_lanczos/lanczos.h&gt;                 // sl_trace\n#include \"LinOp.h\"                            // custom class\n\nvoid slq_log_det(LinOp A, ...){ \n  static_assert(LinearOperator&lt; LinOp &gt;);  // Constraint check\n  const auto matrix_func = std::log;       // any invocable\n  auto rbg = ThreadedRNG64();              // default RNG\n  auto estimates = vector&lt; float &gt;(n, 0);  // output estimates\n  sl_trace&lt; float &gt;(                       // specific precision\n    A, matrix_func, rbg,                   // main arguments\n    ...,                                   // other inputs \n    estimates.data()                       // output \n  );\n}",
    "crumbs": [
      "Integration Guide",
      "Integrating with pybind11"
    ]
  },
  {
    "objectID": "install.html",
    "href": "install.html",
    "title": "Installation",
    "section": "",
    "text": "primate is a standard PEP-517 package, and thus can be installed via pip:\npip install &lt; primate source directory &gt;\nCurrently the package must be built from source via cloning the repository. PYPI support is planned.",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "reference/random.rademacher.html",
    "href": "reference/random.rademacher.html",
    "title": "random.rademacher",
    "section": "",
    "text": "random.rademacher(size, rng='splitmix64', seed=-1, dtype=np.float32)\nGenerates random vectors from the rademacher distribution.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsize\nint or tuple\nOutput shape to generate.\nrequired\n\n\nrng\nstr = “splitmix64”\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint = -1\nSeed for the generator. Use -1 to for random (non-deterministic) behavior.\n-1\n\n\ndtype\ndtype = float32\nFloating point dtype for the output. Must be float32 or float64.\nnp.float32\n\n\n\n\n\n\nnp.narray Randomly generated matrix of shape size with entries in { -1, 1 }.",
    "crumbs": [
      "API Reference",
      "Random",
      "Rademacher"
    ]
  },
  {
    "objectID": "reference/random.rademacher.html#parameters",
    "href": "reference/random.rademacher.html#parameters",
    "title": "random.rademacher",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsize\nint or tuple\nOutput shape to generate.\nrequired\n\n\nrng\nstr = “splitmix64”\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint = -1\nSeed for the generator. Use -1 to for random (non-deterministic) behavior.\n-1\n\n\ndtype\ndtype = float32\nFloating point dtype for the output. Must be float32 or float64.\nnp.float32",
    "crumbs": [
      "API Reference",
      "Random",
      "Rademacher"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Functions for estimating the trace of matrices and matrix functions.\n\n\n\ntrace.hutch\nEstimates the trace of a symmetric A or matrix function f(A) via a Girard-Hutchinson estimator.\n\n\ntrace.xtrace\n\n\n\n\n\n\n\nMatrix operators\n\n\n\noperator.matrix_function\nConstructs a LinearOperator approximating the action of the matrix function fun(A).\n\n\n\n\n\n\nRandomized module\n\n\n\nrandom.rademacher\nGenerates random vectors from the rademacher distribution.\n\n\nrandom.normal\nGenerates random vectors from the standard normal distribution.\n\n\n\n\n\n\nDiagonalization methods\n\n\n\ndiagonalize.lanczos\nLanczos method for matrix tridiagonalization.\n\n\n\n\n\n\nMiscellenous functions\n\n\n\nget_include\nReturn the directory that contains the primate’s *.h header files.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#trace",
    "href": "reference/index.html#trace",
    "title": "Function reference",
    "section": "",
    "text": "Functions for estimating the trace of matrices and matrix functions.\n\n\n\ntrace.hutch\nEstimates the trace of a symmetric A or matrix function f(A) via a Girard-Hutchinson estimator.\n\n\ntrace.xtrace",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#random",
    "href": "reference/index.html#random",
    "title": "Function reference",
    "section": "",
    "text": "Randomized module\n\n\n\nrandom.rademacher\nGenerates random vectors from the rademacher distribution.\n\n\nrandom.normal\nGenerates random vectors from the standard normal distribution.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#diagonalize",
    "href": "reference/index.html#diagonalize",
    "title": "Function reference",
    "section": "",
    "text": "Diagonalization methods\n\n\n\ndiagonalize.lanczos\nLanczos method for matrix tridiagonalization.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/index.html#misc",
    "href": "reference/index.html#misc",
    "title": "Function reference",
    "section": "",
    "text": "Miscellenous functions\n\n\n\nget_include\nReturn the directory that contains the primate’s *.h header files.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/primate.trace.sl_gauss.html",
    "href": "reference/primate.trace.sl_gauss.html",
    "title": "sl_gauss",
    "section": "",
    "text": "trace.sl_gauss(A, n=150, deg=20, pdf='rademacher', rng='pcg', seed=-1, orth=0, num_threads=0)\nStochastic Gaussian quadrature approximation.\nComputes a set of sample nodes and weights for the degree-k orthogonal polynomial approximating the cumulative spectral measure of A. This function can be used to approximate the spectral density of A, or to approximate the spectral sum of any function applied to the spectrum of A.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nA\nndarray, sparray, or LinearOperator\nreal symmetric operator.\nrequired\n\n\nn\nint\nNumber of random vectors to sample for the quadrature estimate.\n150\n\n\ndeg\nint\nDegree of the quadrature approximation.\n20\n\n\nrng\n‘splitmix64’, ’xoshiro256**‘, ’pcg64’, ‘lcg64’, ‘mt64’\nRandom number generator to use (PCG64 by default).\n'splitmix64'\n\n\nseed\nint\nSeed to initialize the rng entropy source. Set seed &gt; -1 for reproducibility.\n-1\n\n\npdf\n‘rademacher’, ‘normal’\nChoice of zero-centered distribution to sample random vectors from.\n'rademacher'\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against when building the Krylov basis.\n0\n\n\nnum_threads\nint\nNumber of threads to use to parallelize the computation. Setting num_threads &lt; 1 to let OpenMP decide.\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nEstimate of the trace of the matrix function f(A).\n\n\n(dict, optional)\nIf ‘info = True’, additional information about the computation.",
    "crumbs": [
      "API Reference",
      "Trace",
      "SL Gauss"
    ]
  },
  {
    "objectID": "reference/trace.html",
    "href": "reference/trace.html",
    "title": "trace",
    "section": "",
    "text": "trace\n\n\n\n\n\nName\nDescription\n\n\n\n\nhutch\nEstimates the trace of a matrix A or matrix function f(A) via a Girard-Hutchinson estimator.\n\n\nxtrace"
  },
  {
    "objectID": "reference/trace.html#functions",
    "href": "reference/trace.html#functions",
    "title": "trace",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nhutch\nEstimates the trace of a matrix A or matrix function f(A) via a Girard-Hutchinson estimator.\n\n\nxtrace"
  },
  {
    "objectID": "basic/integration.html",
    "href": "basic/integration.html",
    "title": "Integration",
    "section": "",
    "text": "primate supports a variety of matrix-types of the box, including numpy ndarray’s, compressed sparse matrices (a lá SciPy), and LinearOperators—the latter enables the use of matrix free operators.\nOutside of the natively types above, the basic requirements for any operator A to be used with e.g. the Lanczos method in primate are:\n\nA method A.matvec(input: ndarray) -&gt; ndarray implementing v \\mapsto Av\nAn attribute A.shape -&gt; tuple[int, int] giving the output/input dimensions of A\n\n\n\nHere’s an example of a simple operator representing a Diagonal matrix, which inherits a .matvec() method by following the subclassing rules of SciPy’s LinearOperator:\nimport numpy as np \nfrom numpy.typing import ArrayLike\nfrom scipy.sparse.linalg import LinearOperator \n\nclass DiagonalOp(LinearOperator):\n  diag: np.ndarray = None\n  \n  def __init__(self, d: ArrayLike, dtype = None):\n    self.diag = np.array(d)\n    self.shape = (len(d), len(d))\n    self.dtype = np.dtype('float32') if dtype is None else dtype\n\n  def _matvec(self, x: ArrayLike) -&gt; np.ndarray:\n    out = self.diag * np.ravel(x)\n    return out.reshape(x.shape)",
    "crumbs": [
      "Basics",
      "Integration"
    ]
  },
  {
    "objectID": "theory/lanczos.html",
    "href": "theory/lanczos.html",
    "title": "The Lanczos method",
    "section": "",
    "text": "Whether for simplifying the representation of complicated systems, characterizing the asymptotic behavior of differential equations, or even just fitting polynomials to data via least-squares, decomposing linear operators has had significant use in many areas of sciences and engineering.\nCentral to operator theory is the spectral theorem, which provides conditions under which a linear operator A : \\mathbb{R}^n \\to \\mathbb{R}^n can be diagonalized in terms of its eigenvalues and eigenvectors:  A = U \\Lambda U^{-1}\nIn the case where A is symmetric, the eigen-decomposition is not only guarenteed to exist, but its canonical form may be obtained via orthogonal diagonalization. Such matrices are among the most commonly encountered matrices in applications.\nIn 1950, Cornelius Lanczos studied an alternative means of decomposition via tridiagonalization:   AQ = Q T \\quad \\Leftrightarrow \\quad Q^T A Q = T  The algorithm by which one produces such a T is known as the Lanczos method. Despite its age, it remains the standard algorithm1 both for computing eigensets and solving linear systems in the large-scale regime. Having intrinsic connections to the conjugate gradient method, the theory of orthogonal polynomials, and Gaussian quadrature, it is one of the most important numerical methods of all time—indeed, an IEEE guest editorial places it among the top 10 most influential algorithms of the 20th century.\nAs the Lanczos method lies at the heart of primate’s design, this post introduces it, with a focus on its motivating principles and computational details. For its API usage, see the lanczos page.",
    "crumbs": [
      "Theory",
      "The Lanczos Method"
    ]
  },
  {
    "objectID": "theory/lanczos.html#lanczos-on-a-bumper-sticker",
    "href": "theory/lanczos.html#lanczos-on-a-bumper-sticker",
    "title": "The Lanczos method",
    "section": "Lanczos on a bumper sticker",
    "text": "Lanczos on a bumper sticker\nGiven any non-zero v \\in \\mathbb{R}^n, Lanczos generates a Krylov subspace via successive powers of A:\n\n\\mathcal{K}(A, v) \\triangleq \\{ \\, A^{0} v, A^{1}v, A^{2}v, \\dots, A^{n}v \\, \\}\n\nThese vectors are independent, so orthogonalizing them not only yields an orthonormal basis for \\mathcal{K}(A, v) but also a change-of-basis matrix Q, allowing A to be represented by a new matrix T:\n\n\\begin{align*}\nK &= [\\, v \\mid Av \\mid A^2 v \\mid \\dots \\mid A^{n-1}v \\,] && \\\\\nQ &= [\\, q_1, q_2, \\dots, q \\,] \\gets \\mathrm{qr}(K) &&  \\\\\nT &= Q^T A Q &&\n\\end{align*}\n\nIt turns out that since A is symmetric, T is guaranteed to have a symmetric tridiagonal structure:\n\nT = \\mathrm{tridiag}\\Bigg(\n\\begin{array}{ccccccccc}\n& \\beta_2 & & \\beta_3 & & \\cdot & & \\beta_n & \\\\\n\\alpha_1 & & \\alpha_2 & & \\cdot & & \\cdot & & \\alpha_n \\\\\n& \\beta_2 & & \\beta_3 & & \\cdot & & \\beta_n &\n\\end{array}\n\\Bigg)\n\n\n\nSince \\mathrm{range}(Q) = \\mathcal{K}(A, v), the change-of-basis A \\mapsto Q^{-1} A Q is in fact a similarity transform, which are known to be equivalence relations on \\mathcal{S}^n—thus we can obtain \\Lambda by diagonalizing T:\n T = Y \\Lambda Y^T  \nAs T is quite structured, it can be easily diagonalized, thus we have effectively solved the eigenvalue problem. To quote the Lanczos introduction from Parlett, could anything be more simple?",
    "crumbs": [
      "Theory",
      "The Lanczos Method"
    ]
  },
  {
    "objectID": "theory/lanczos.html#the-lanczos-iteration",
    "href": "theory/lanczos.html#the-lanczos-iteration",
    "title": "The Lanczos method",
    "section": "The Lanczos Iteration",
    "text": "The Lanczos Iteration\nThe Lanczos method exposes the spectrum of A by successively projecting onto Krylov subspaces. That is, given a symmetric A \\in \\mathbb{R}^{n \\times n} with eigenvalues \\lambda_1 \\geq \\lambda_2 &gt; \\dots \\geq \\lambda_r &gt; 0 and a vector v \\in \\mathbb{R} \\setminus \\{0\\}, the order-j Krylov subspaces / Krylov matrices of the pair (A, v) are given by:\n\n\\mathcal{K}_j(A, v) := \\mathrm{span}\\{ v, Av, A^2 v, \\dots, A^{j-1}v \\}, \\quad \\quad K_j(A, v) = [ v \\mid Av \\mid A^2 v \\mid \\dots \\mid A^{j-1}]\n\nKrylov subspaces arise naturally from using the minimal polynomial of A to express A^{-1} in terms of powers of A: if A is nonsingular and its minimal polynomial has degree m, then A^{-1}v \\in K_m(A, v) and K_m(A, v) is an invariant subspace.\nThe spectral theorem implies that since A is symmetric, it is orthogonally diagonalizable: thus, \\Lambda(A) may be obtained by generating an orthonormal basis for \\mathcal{K}_n(A, v). To do this, the Lanczos method constructs successive QR factorizations of K_j(A,v) = Q_j R_j for each j = 1, 2, \\dots, n. Due to A’s symmetry and the orthogonality of Q_j, we have q_k^T A q_l = q_l^T A^T q_k = 0 for k &gt; l + 1, implying T_j = Q_j^T A Q_j has a tridiagonal structure:\n\\begin{equation}\n    T_j = \\begin{bmatrix}\n    \\alpha_1 & \\beta_2 & & & \\\\\n    \\beta_2 & \\alpha_2 & \\beta_3 & & \\\\\n     & \\beta_3 & \\alpha_3 & \\ddots & \\\\\n    & & \\ddots & \\ddots & \\beta_{j} \\\\\n    & & & \\beta_{j} & \\alpha_{j}\n    \\end{bmatrix}, \\; \\beta_j &gt; 0, \\; j = 1, 2, \\dots, n\n\\end{equation}\n\nGiven an initial pair (A, q_1) satisfying \\lVert q_1 \\rVert = 1, one can restrict and project A to its j-th Krylov subspace T_j via: \n\\begin{equation}\n    A Q_j = Q_j T_j + \\beta_{j+1} q_{j+1} e_{j}^T \\quad\\quad (\\beta_j &gt; 0)\n\\end{equation}\n where Q_j = [\\, q_1 \\mid q_2 \\mid \\dots \\mid q_j \\,] is an orthonormal set of vectors mutually orthogonal to q_{j+1}. Equating the j-th columns on each side of the above and rearranging the terms yields the famed three-term recurrence: \\begin{equation}\n     \\beta_{j} \\, q_{j+1} = A q_j - \\alpha_j \\, q_j - \\beta_{j\\text{-}1} \\, q_{j\\text{-}1}  \n\\end{equation}\n where \\alpha_j = q_j^T A q_j, \\beta_j = \\lVert r_j \\rVert_2, r_j = (A - \\alpha_j I)q_j - \\beta_{j\\text{-}1} q_j, and q_{j+1} = r_j / \\beta_j. The equation above is a variable-coefficient second-order linear difference equation, and such equations have unique solutions: if (q_{j\\text{-}1}, \\beta_j, q_j) are known, then (\\alpha_j, \\beta_{j+1}, q_{j+1}) are completely determined. This sequential process which iteratively builds T_j via this three-term recurrence is what is known as the Lanczos iteration.",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "theory/lanczos.html#uniqueness-of-t",
    "href": "theory/lanczos.html#uniqueness-of-t",
    "title": "The Lanczos method",
    "section": "Uniqueness of T",
    "text": "Uniqueness of T\nUnfortunately, unlike the spectral decomposition A = V \\Lambda V^T—which identifies a diagonalizable A with its spectrum \\Lambda(A) up to a change of basis A \\mapsto M^{-1} A M—there is no canonical choice of T_j due to the arbitrary choice of v. However, there is a connection between the iterates K_j(A,v) and the full tridiagonalization of A: if Q^T A Q = T is tridiagonal and Q= [\\, q_1 \\mid q_2 \\mid \\dots \\mid q_n \\,] is an n \\times n orthogonal matrix Q Q^T = I_n = [e_1, e_2, \\dots, e_n], then we have: \n\\begin{equation}\n    K_n(A, q_1) = Q Q^T K_n(A, q_1) = Q[ \\, e_1 \\mid T e_1 \\mid T^2 e_1 \\mid \\dots \\mid T^{n-1} e_1 \\, ]\n\\end{equation}\n is the QR factorization of K_n(A, q_1). Thus, tridiagonalizing A with respect to a unit-norm q_1 determines Q. Indeed, the Implicit Q Theorem asserts that if an upper Hessenburg matrix T \\in \\mathbb{R}^{n \\times n} has only positive elements on its first subdiagonal and there exists an orthogonal matrix Q such that Q^T A Q = T, then Q and T are uniquely determined by (A, q_1).",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "integration/slq_guide.html",
    "href": "integration/slq_guide.html",
    "title": "SLQ Trace guide",
    "section": "",
    "text": "This guide walks through the SLQ method implemented in primate, which can be specialized for different purposes."
  },
  {
    "objectID": "integration/slq_guide.html#slq-as-a-function-template",
    "href": "integration/slq_guide.html#slq-as-a-function-template",
    "title": "SLQ Trace guide",
    "section": "SLQ as a function template",
    "text": "SLQ as a function template\nBelow is the full signature of the SLQ function template:\n// Stochastic Lanczos quadrature method\ntemplate&lt; std::floating_point F, LinearOperator Matrix, ThreadSafeRBG RBG &gt;\nvoid slq (\n  const Matrix& A,                    // Any *LinearOperator*\n  const function&lt; F(int,F*,F*) &gt;& f,  // Generic function\n  const function&lt; bool(int) &gt;& stop,  // Early-stop function\n  const int nv,                       // Num. of sample vectors\n  const Distribution dist,            // Sample vector distribution\n  RBG& rng,                           // Random bit generator\n  const int lanczos_degree,           // Krylov subspace degree\n  const F lanczos_rtol,               // Lanczos residual tolerance\n  const int orth,                     // Add. vectors to orthogonalize\n  const int ncv,                      // Num. of Lanczos vectors\n  const int num_threads,              // # threads to allocate \n  const int seed                      // Seed for RNG \n)\nMany of the runtime arguments are documented in the lanczos or sl_trace docs; the compile-time (template) parameters are:\n\nThe floating point type (e.g. float, double, long double)\nThe operator type (e.g. Eigen::MatrixXf, torch::Tensor, LinOp)\nThe multi-threaded random number generator (e.g. ThreadedRNG64)\n\nNote any type combination satisfying these concepts (e.g. std::floating_point, LinearOperator) generates a function specialized of said types at compile-time—this is known as template instantiation."
  },
  {
    "objectID": "integration/slq_guide.html#generality-via-function-passing",
    "href": "integration/slq_guide.html#generality-via-function-passing",
    "title": "SLQ Trace guide",
    "section": "Generality via function passing",
    "text": "Generality via function passing\nGiven a valid set of parameters, the main body of the SLQ looks something like this:\n  bool stop_flag = false;\n  #pragma omp parallel shared(stop_flag)\n  {\n    // &lt; allocations for Q, alpha, beta, etc. &gt; \n    int tid = omp_get_thread_num(); // thread-id \n    \n    #pragma omp for\n    for (i = 0; i &lt; nv; ++i){\n      if (stop_flag){ continue; }\n      generate_isotropic&lt; F &gt;(...); // populates q\n      lanczos_recurrence&lt; F &gt;(...); // populates alpha + beta\n      lanczos_quadrature&lt; F &gt;(...); // populates nodes + weights\n      f(i, q, Q, nodes, weights);   // Run user-supplied function \n      #pragma omp critical\n      {\n        stop_flag = stop(i);        // Checks for early-stopping\n      }\n    } // end for\n  } // end parallel \nThere are two functions that can be used for generalizing SLQ for different purposes.\nThe first generic function f can read, save, or modify the information available from the iteration index i, the isotropic vector q, the Lanczos vectors Q, and/or the quadrature information nodes, weights. Note this function is run in the parallel section.\nThe second is a boolean-valued function stop which can be used to stop the iteration early, for example if convergence has been achieved according to some rule. Since this is run in the critical section, it is called sequentially."
  },
  {
    "objectID": "integration/slq_guide.html#using-slq-to-estimate-mathrmtrfa",
    "href": "integration/slq_guide.html#using-slq-to-estimate-mathrmtrfa",
    "title": "SLQ Trace guide",
    "section": "Using SLQ to estimate \\mathrm{tr}(f(A))",
    "text": "Using SLQ to estimate \\mathrm{tr}(f(A))\nThe SLQ method is often used to estimate the trace of an arbitrary matrix function:\n \\mathrm{tr}(f(A)), \\quad \\text{ where } f(A) = U f(\\Lambda) U^T \nIt’s has been shown1 that the information obtained by the Lanczos method is sufficient to obtained a Gaussian quadrature approximation of the empirical spectral measure of A. By sampling zero-mean vectors satisfying \\mathbb{E}[v v^T] = I, one can obtain estimates of the trace above: \\operatorname{tr}(f(A)) \\approx \\frac{n}{\\mathrm{n}_{\\mathrm{v}}} \\sum_{l=1}^{\\mathrm{n}_{\\mathrm{v}}}\\left(\\sum_{k=0}^m\\left(\\tau_k^{(l)}\\right)^2 f\\left(\\theta_k^{(l)}\\right)\\right)\nIt turns out averaging these trace estimates yields unbiased, Girard-Hutchinson estimator of the trace. To see why this estimator is unbiased, note that:  \\mathtt{tr}(A) = \\mathbb{E}[v^T A v] \\approx \\frac{1}{n_v}\\sum\\limits_{i=1}^{n_v} v_i^\\top A v_i \nThus, all we need to do to estimate the trace of a matrix function is multiply and sum the quadrature nodes and weights output by SLQ."
  },
  {
    "objectID": "integration/slq_guide.html#sl_trace-method",
    "href": "integration/slq_guide.html#sl_trace-method",
    "title": "SLQ Trace guide",
    "section": "sl_trace method",
    "text": "sl_trace method\nTo see how these formulas are actually implemented with the generic SLQ implementation, here’s an abbreviated form of the sl_trace function implemented by primate:\ntemplate&lt; std::floating_point F, LinearOperator Matrix, ThreadSafeRBG RBG &gt;\nvoid sl_trace(\n  const Matrix& A, const std::function&lt; F(F) &gt; sf, RBG& rbg, \n  const int nv, const int dist, const int engine_id, const int seed,\n  const int deg, const float lanczos_rtol, const int orth, const int ncv,\n  const F atol, const F rtol\n  F* estimates\n){  \n  using VectorF = Eigen::Array&lt; F, Dynamic, 1&gt;;\n\n  // Parameterize the trace function (runs in parallel)\n  auto trace_f = [&](int i, F* q, F* Q, F* nodes, F* weights){\n    Map&lt; VectorF &gt; nodes_v(nodes, deg, 1);     // no-op\n    Map&lt; VectorF &gt; weights_v(weights, deg, 1); // no-op\n    nodes_v.unaryExpr(sf);\n    estimates[i] = (nodes_v * weights_v).sum();\n  };\n  \n  // Convergence checking like scipy.integrate.quadrature\n  int n = 0;\n  F mu_est = 0.0, mu_pre = 0.0;\n  const auto early_stop = [&](int i) -&gt; bool {\n    ++n; // Number of estimates\n    mu_est = (1.0 / F(n)) * (estimates[i] + (n - 1) * mu_pre); \n    bool atol_check = abs(mu_est - mu_pre) &lt;= atol;\n    bool rtol_check = abs(mu_est - mu_pre) / mu_est &lt;= rtol; \n    mu_pre = mu_est; \n    return atol_check || rtol_check;\n  };\n\n  // Execute the stochastic Lanczos quadrature with the trace function \n  slq&lt; float &gt;(A, trace_f, early_stop, ...);\n}\nAs before, two functions are used to parameterize the slq method.\nThe first (trace_f) applies an arbitrary spectral function sf to the Rayleigh-Ritz values obtained by the Lanczos tridiagonalization of A(or equivalently, the nodes of the Gaussian quadrature). These are the \\theta’s in the pseudocode above. When multiplied by the weights of the quadrature, the corresponding sum forms an estimate of the trace of the matrix function.\nThe second function early_stop is used to check for convergence of the estimator. First, it uses the trace estimate x_n to update the sample mean \\mu_n via the formula:\n \\mu_n = n^{-1} [x_n + (n - 1)\\mu_{n-1}] \nThen, much in the same way the quadrature function from scipy.integrate approximates a definite integral, it checks for convergence using the absolute and relative tolerances supplied by the user. Returning true signals convergence, stopping the iteration early."
  },
  {
    "objectID": "integration/slq_guide.html#references",
    "href": "integration/slq_guide.html#references",
    "title": "SLQ Trace guide",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "theory/slq.html",
    "href": "theory/slq.html",
    "title": "SLQ Trace guide",
    "section": "",
    "text": "To clarify that that means, here’s an abstract presentation of the generic SLQ procedure:\n\n\n\\begin{algorithm} \\caption{Stochastic Lanczos Quadrature} \\begin{algorithmic} \\Input Symmetric operator ($A \\in \\mathbb{R}^{n \\times n}$) \\Require Number of queries ($n_v$), Degree of quadrature ($k$) \\Function{SLQ}{$A$, $n_v$, $k$} \\State $\\Gamma \\gets 0$ \\For{$j = 1, 2, \\dots, n_v$} \\State $v_i \\sim \\mathcal{D}$ where $\\mathcal{D}$ satisfies $\\mathbb{E}(v v^\\top) = I$ \\State $T^{(j)}(\\alpha, \\beta)$ $\\gets$ $\\mathrm{Lanczos}(A,v_j,k+1)$ \\State $[\\Theta, Y] \\gets \\mathrm{eigh\\_tridiag}(T^{(j)}(\\alpha, \\beta))$ \\State $\\tau_i \\gets \\langle e_1, y_i \\rangle$ \\State &lt; Do something with the node/weight pairs $(\\theta_i, \\tau_i^2)$ &gt; \\EndFor \\EndFunction \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "Theory",
      "SLQ"
    ]
  },
  {
    "objectID": "theory/lanczos.html#why-care",
    "href": "theory/lanczos.html#why-care",
    "title": "The Lanczos method",
    "section": "Why care?",
    "text": "Why care?\nComputing the eigen-decomposition A = U \\Lambda U^T for general symmetric A \\in \\mathbb{R}^{n \\times n} is essentially bounded above by \\Theta(n^\\omega) time and \\Theta(n^2) space, where \\omega \\approx is the matrix-multiplication constant. This translates to an effective \\Omega(n^3) time bound if we exclude the Strassen-model for matrix multiplication (since it is not practical anyways). However, if one can show that v \\mapsto Av \\approx O(n), then there is a simple way of obtaining \\Lambda(A) in O(n^2) time and O(n) space!",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "theory/lanczos.html#quadratic-time-and-linear-space-how",
    "href": "theory/lanczos.html#quadratic-time-and-linear-space-how",
    "title": "The Lanczos method",
    "section": "Quadratic time and linear space? How?",
    "text": "Quadratic time and linear space? How?\nUnless you know the tricks, its not obvious at all decompositions above takes just O(n^2) time and O(n) space to obtain. So… how does the complexity argument play out?\n\nThe cubic bound\nFirst, its important to establish that computing the eigen-decomposition A = U \\Lambda U^T for general symmetric A \\in \\mathbb{R}^{n \\times n} is essentially bounded by \\Theta(n^\\omega) time and \\Theta(n^2) space, where \\omega \\approx 2.37\\dots is the matrix-multiplication constant. Conceptually, if we exclude the Strassen-model for matrix multiplication (since it is not practical anyways), this translates to an effective \\Omega(n^3) time bound.\n\n\nSolving the right problem\nIn some applications, the eigenvectors are not needed all at once (or at all, even). One of the main draws to the Lanczos method is its efficiency: if one can perform v \\mapsto Av quickly—say, in \\approx O(n) time—then the Lanczos method can construct \\Lambda(A) in just O(n^2) time and O(n) space! Moreover, entire method is matrix free as the only input to the algorithm is a (fast) matrix-vector product v \\mapsto Av: one need not store A explicitly to do this for many special types of linear operators.\nIf you squint hard enough, you can deduce that since A Q_j = Q_j T_j + \\beta_{j+1} q_{j+1} e_{j}^T, every symmetric A \\in \\mathbb{R}^{n \\times n} expanded this way admits a three-term recurrence: \n\\begin{align*}\nA q_j &= \\beta_{j\\text{-}1} q_{j\\text{-}1} + \\alpha_j q_j + \\beta_j q_{j+1} \\\\\n\\Leftrightarrow \\beta_{j} \\, q_{j+1} &= A q_j - \\alpha_j \\, q_j - \\beta_{j\\text{-}1} \\, q_{j\\text{-}1}  \n\\end{align*}\n\nThe equation above is a variable-coefficient second-order linear difference equation, and it is known such equations have unique solutions: \n\\alpha_j = q_j^T A q_j, \\;\\; \\beta_j = \\lVert r_j \\rVert_2, \\;\\; q_{j+1} = r_j / \\beta_j\n\n\n\\text{where  } r_j = (A - \\alpha_j I)q_j - \\beta_{j\\text{-}1} q_j\n\nIn other words, if (q_{j\\text{-}1}, \\beta_j, q_j) are known, then (\\alpha_j, \\beta_{j+1}, q_{j+1}) are completely determined. This fact is fantastic from a computational point of view: no explicit call to the QR algorithm necessary2!\nNote that a symmetric tridiagonal matrix is fully characterized by its diagonal and subdiagonal terms, which requires just O(n) space. If we assume that v \\mapsto Av \\sim O(n), then the above procedure clearly takes at most O(n^2) time, since there are most n such vectors \\{q_i\\}_{i=1}^n to generate!\nMoreover, if we only need the eigen-values \\Lambda(A) ( and not their vectors U), then we may execute the recurrence keeping at most three vectors \\{q_{j-1}, q_{j}, q_{j+1}\\} in memory at any given time. Since each of these is O(n) is size, the claim of O(n) space is justified!\nThe description above is essentially the proof of the following Theorem:\n\nTheorem 1 (Parlett 1994, Simon 1984) Given a symmetric rank-r matrix A \\in \\mathbb{R}^{n \\times n} whose operator x \\mapsto A x requires O(\\eta) time and O(\\nu) space, the Lanczos iteration computes \\Lambda(A) = \\{ \\lambda_1, \\lambda_2, \\dots, \\lambda_r \\} in O(\\max\\{\\eta, n\\}\\cdot r) time and O(\\max\\{\\nu, n\\}) space, when computation is done in exact arithmetic",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "basic/install.html",
    "href": "basic/install.html",
    "title": "Installation",
    "section": "",
    "text": "primate is a standard PEP-517 package, and thus can be installed via pip:\npip install &lt; primate source directory &gt;\nCurrently the package must be built from source via cloning the repository. PYPI support is planned.\n\n\nPlatform support\nPlatform-specific wheels are currently built with cibuildwheel and uploaded to PyPI. These enable primate to be installed on supported architecture / CPython implementations without compilation. The currently supported platforms include:\nIf your platform isn’t on this list, feel free to make an issue requesting support for your platform.\n\n\nCompiling from source\nTo install the package from its source distribution, a C++20 compiler is required; the current builds are all built with some variant of clang. For platform- and compiler-specific settings, consult the build scripts and CI configuration files.\n\n\nC++ Installation\nprimate’s C++ interface is header-only, making it easy to compile your own extension modules. The simplest way to link these headers is to add primate as a dependency to your package and use the get_include() function to find the appropriate directory.\n\nsetuptoolsmeson-pythongit submodule\n\n\n# setup.py\nimport primate as pm\n...\nExtension('extension_name', ..., include_dirs=[pm.get_include()])\n...\n\n\n# meson.build\n...\nprimate_include_dirs = run_command(py, \n  ['-c', 'import primate as pm; print(pm.get_include())']\n).stdout().strip()\n...\n\n\nAssuming your headers are located in extern, from your git repository, you can use:\ngit submodule add https://github.com/peekxc/primate extern/primate\ngit submodule update --init\nFrom here, you can now include extern/primate/include into your C++ source files, or you can add this directory to the search path used other various build tools, such as CMake or Meson.",
    "crumbs": [
      "Basics",
      "Installation"
    ]
  },
  {
    "objectID": "theory/lanczos.html#surpassing-the-cubic-bound",
    "href": "theory/lanczos.html#surpassing-the-cubic-bound",
    "title": "The Lanczos method",
    "section": "Surpassing the cubic bound",
    "text": "Surpassing the cubic bound\nComputing the eigen-decomposition A = U \\Lambda U^T for general symmetric A \\in \\mathbb{R}^{n \\times n} is essentially bounded by \\Theta(n^\\omega) time and \\Theta(n^2) space, where \\omega \\approx 2.37\\dots is the matrix-multiplication constant. Conceptually, if we exclude the Strassen-model for matrix multiplication (since it is not practical anyways), this translates to an effective \\Omega(n^3) time bound. Not great!\nOne of the main draws to the Lanczos method is its efficiency: if one can perform v \\mapsto Av quickly—say, in \\approx O(n) time—then the Lanczos method can construct \\Lambda(A) in just O(n^2) time and O(n) space! Moreover, entire method is matrix free as the only input to the algorithm is a (fast) matrix-vector product v \\mapsto Av: one need not store A explicitly to do this for many special types of linear operators.",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Package overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMusco, Cameron, Christopher Musco, and Aaron Sidford. (2018) “Stability of the Lanczos method for matrix function approximation.”↩︎\nUbaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature.↩︎\nThis includes std::function’s, C-style function pointers, functors, and lambda expressions.↩︎",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "theory/lanczos.html#footnotes",
    "href": "theory/lanczos.html#footnotes",
    "title": "The Lanczos method",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA variant of the Lanczos method is actually at the heart scipy.sparse.linalg’s default eigsh solver (which is a port of ARPACK).↩︎\nThe spectral decomposition A = U \\Lambda U^T identifies a diagonalizable A with its spectrum \\Lambda(A) up to a change of basis A \\mapsto M^{-1} A M↩︎\nThe spectral decomposition A = U \\Lambda U^T identifies a diagonalizable A with its spectrum \\Lambda(A) up to a change of basis A \\mapsto M^{-1} A M↩︎\nFor general A \\in \\mathbb{R}^{n \\times n}, computing the spectral-decomposition is essentially bounded by the matrix-multiplication time: \\Theta(n^\\omega) time and \\Theta(n^2) space, where \\omega \\approx 2.37\\dots is the matrix multiplication constant. If we exclude the Strassen model for computation, we get effectively a \\Omega(n^3) time and \\Omega(n^2) space bound.↩︎",
    "crumbs": [
      "Theory",
      "The Lanczos Method"
    ]
  },
  {
    "objectID": "imate_compare.html#footnotes",
    "href": "imate_compare.html#footnotes",
    "title": "Comparison to imate",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBefore v0.2, much of primate’s code was essentially ported and refactored from imate. The code for v0.2+ has been re-written using the Eigen template C++ library.↩︎\nprimate does not provide native GPU-implemented Linear operators. However, there is nothing preventing one from using e.g. CUDA- or ROCm-based GPU-based tensor libraries to accelerate matrix-vector products. Indeed, primate was designed to work with essentially any operator matching the interface.↩︎\nSee imates documentation for the list of supported functions.↩︎",
    "crumbs": [
      "Basics",
      "Comparison to *imate*"
    ]
  },
  {
    "objectID": "theory/lanczos.html#pseudocode",
    "href": "theory/lanczos.html#pseudocode",
    "title": "The Lanczos method",
    "section": "Pseudocode",
    "text": "Pseudocode\nThere are several ways to implement the Lanczos method, some of which are “better” than others. Below is pseudocode equivalent to Paige’s A27 variant, which has been shown to have a variety of attractive properties.\nThere are many extensions that modify the Lanczos method to make it more robust, more computationally efficient, etc., though many of these have non-trivial implications on the space and time complexities.",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "advanced/slq_param.html",
    "href": "advanced/slq_param.html",
    "title": "Parameterizing SLQ",
    "section": "",
    "text": "This guide walks through how to parameterize the SLQ method implemented in primate on the C++ side to approximate some spectral quantity of interest.",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#slq-as-a-function-template",
    "href": "advanced/slq_param.html#slq-as-a-function-template",
    "title": "Parameterizing SLQ",
    "section": "SLQ as a function template",
    "text": "SLQ as a function template\nBelow is the full signature of the SLQ function template:\n// Stochastic Lanczos quadrature method\ntemplate&lt; std::floating_point F, LinearOperator Matrix, ThreadSafeRBG RBG &gt;\nvoid slq (\n  const Matrix& A,                    // Any *LinearOperator*\n  const function&lt; F(int,F*,F*) &gt;& f,  // Generic function\n  const function&lt; bool(int) &gt;& stop,  // Early-stop function\n  const int nv,                       // Num. of sample vectors\n  const Distribution dist,            // Sample vector distribution\n  RBG& rng,                           // Random bit generator\n  const int lanczos_degree,           // Krylov subspace degree\n  const F lanczos_rtol,               // Lanczos residual tolerance\n  const int orth,                     // Add. vectors to orthogonalize\n  const int ncv,                      // Num. of Lanczos vectors\n  const int num_threads,              // # threads to allocate \n  const int seed                      // Seed for RNG \n)\nMany of the runtime arguments are documented in the lanczos or sl_trace docs; the compile-time (template) parameters are:\n\nThe floating point type (e.g. float, double, long double)\nThe operator type (e.g. Eigen::MatrixXf, torch::Tensor, LinOp)\nThe multi-threaded random number generator (e.g. ThreadedRNG64)\n\nNote any type combination satisfying these concepts (e.g. std::floating_point, LinearOperator) generates a function specialized of said types at compile-time—this is known as template instantiation.",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#generality-via-function-passing",
    "href": "advanced/slq_param.html#generality-via-function-passing",
    "title": "Parameterizing SLQ",
    "section": "Generality via function passing",
    "text": "Generality via function passing\nGiven a valid set of parameters, the main body of the SLQ looks something like this:\n  bool stop_flag = false;\n  #pragma omp parallel shared(stop_flag)\n  {\n    // &lt; allocations for Q, alpha, beta, etc. &gt; \n    int tid = omp_get_thread_num(); // thread-id \n    \n    #pragma omp for\n    for (i = 0; i &lt; nv; ++i){\n      if (stop_flag){ continue; }\n      generate_isotropic&lt; F &gt;(...); // populates q\n      lanczos_recurrence&lt; F &gt;(...); // populates alpha + beta\n      lanczos_quadrature&lt; F &gt;(...); // populates nodes + weights\n      f(i, q, Q, nodes, weights);   // Run user-supplied function \n      #pragma omp critical\n      {\n        stop_flag = stop(i);        // Checks for early-stopping\n      }\n    } // end for\n  } // end parallel \nThere are two functions that can be used for generalizing SLQ for different purposes.\nThe first generic function f can read, save, or modify the information available from the iteration index i, the isotropic vector q, the Lanczos vectors Q, and/or the quadrature information nodes, weights. Note this function is run in the parallel section.\nThe second is a boolean-valued function stop which can be used to stop the iteration early, for example if convergence has been achieved according to some rule. Since this is run in the critical section, it is called sequentially.",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#using-slq-to-estimate-mathrmtrfa",
    "href": "advanced/slq_param.html#using-slq-to-estimate-mathrmtrfa",
    "title": "Parameterizing SLQ",
    "section": "Using SLQ to estimate \\mathrm{tr}(f(A))",
    "text": "Using SLQ to estimate \\mathrm{tr}(f(A))\nThe SLQ method is often used to estimate the trace of an arbitrary matrix function:\n \\mathrm{tr}(f(A)), \\quad \\text{ where } f(A) = U f(\\Lambda) U^T \nIt’s has been shown1 that the information obtained by the Lanczos method is sufficient to obtained a Gaussian quadrature approximation of the empirical spectral measure of A. By sampling zero-mean vectors satisfying \\mathbb{E}[v v^T] = I, one can obtain estimates of the trace above: \\operatorname{tr}(f(A)) \\approx \\frac{n}{\\mathrm{n}_{\\mathrm{v}}} \\sum_{l=1}^{\\mathrm{n}_{\\mathrm{v}}}\\left(\\sum_{k=0}^m\\left(\\tau_k^{(l)}\\right)^2 f\\left(\\theta_k^{(l)}\\right)\\right)\nIt turns out averaging these trace estimates yields unbiased, Girard-Hutchinson estimator of the trace. To see why this estimator is unbiased, note that:  \\mathtt{tr}(A) = \\mathbb{E}[v^T A v] \\approx \\frac{1}{n_v}\\sum\\limits_{i=1}^{n_v} v_i^\\top A v_i \nThus, all we need to do to estimate the trace of a matrix function is multiply and sum the quadrature nodes and weights output by SLQ.",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#sl_trace-method",
    "href": "advanced/slq_param.html#sl_trace-method",
    "title": "Parameterizing SLQ",
    "section": "sl_trace method",
    "text": "sl_trace method\nTo see how these formulas are actually implemented with the generic SLQ implementation, here’s an abbreviated form of the sl_trace function implemented by primate:\ntemplate&lt; std::floating_point F, LinearOperator Matrix, ThreadSafeRBG RBG &gt;\nvoid sl_trace(\n  const Matrix& A, const std::function&lt; F(F) &gt; sf, RBG& rbg, \n  const int nv, const int dist, const int engine_id, const int seed,\n  const int deg, const float lanczos_rtol, const int orth, const int ncv,\n  const F atol, const F rtol\n  F* estimates\n){  \n  using VectorF = Eigen::Array&lt; F, Dynamic, 1&gt;;\n\n  // Parameterize the trace function (runs in parallel)\n  auto trace_f = [&](int i, F* q, F* Q, F* nodes, F* weights){\n    Map&lt; VectorF &gt; nodes_v(nodes, deg, 1);     // no-op\n    Map&lt; VectorF &gt; weights_v(weights, deg, 1); // no-op\n    nodes_v.unaryExpr(sf);\n    estimates[i] = (nodes_v * weights_v).sum();\n  };\n  \n  // Convergence checking like scipy.integrate.quadrature\n  int n = 0;\n  F mu_est = 0.0, mu_pre = 0.0;\n  const auto early_stop = [&](int i) -&gt; bool {\n    ++n; // Number of estimates\n    mu_est = (1.0 / F(n)) * (estimates[i] + (n - 1) * mu_pre); \n    bool atol_check = abs(mu_est - mu_pre) &lt;= atol;\n    bool rtol_check = abs(mu_est - mu_pre) / mu_est &lt;= rtol; \n    mu_pre = mu_est; \n    return atol_check || rtol_check;\n  };\n\n  // Execute the stochastic Lanczos quadrature with the trace function \n  slq&lt; float &gt;(A, trace_f, early_stop, ...);\n}\nAs before, two functions are used to parameterize the slq method.\nThe first (trace_f) applies an arbitrary spectral function sf to the Rayleigh-Ritz values obtained by the Lanczos tridiagonalization of A(or equivalently, the nodes of the Gaussian quadrature). These are the \\theta’s in the pseudocode above. When multiplied by the weights of the quadrature, the corresponding sum forms an estimate of the trace of the matrix function.\nThe second function early_stop is used to check for convergence of the estimator. First, it uses the trace estimate x_n to update the sample mean \\mu_n via the formula:\n \\mu_n = n^{-1} [x_n + (n - 1)\\mu_{n-1}] \nThen, much in the same way the quadrature function from scipy.integrate approximates a definite integral, it checks for convergence using the absolute and relative tolerances supplied by the user. Returning true signals convergence, stopping the iteration early.",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#references",
    "href": "advanced/slq_param.html#references",
    "title": "Parameterizing SLQ",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "advanced/slq_param.html#footnotes",
    "href": "advanced/slq_param.html#footnotes",
    "title": "Parameterizing SLQ",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUbaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099.↩︎",
    "crumbs": [
      "Advanced",
      "Parameterizing SLQ"
    ]
  },
  {
    "objectID": "theory/lanczos.html#but-wait-isnt-t-arbitrary",
    "href": "theory/lanczos.html#but-wait-isnt-t-arbitrary",
    "title": "The Lanczos method",
    "section": "But wait, isn’t T arbitrary?",
    "text": "But wait, isn’t T arbitrary?\nUnfortunately, there is no canonical choice of T_j. Indeed, as T_n is a family with n - 1 degrees of freedom and v \\in \\mathbb{R}^n was chosen arbitrarily, there are infinitely many essentially distinct such decompositions. In contrast, the spectral decomposition A = U \\Lambda U^T identifies a diagonalizable A with its spectrum \\Lambda(A) up to a change of basis A \\mapsto M^{-1} A M.\nNot all hope is lost though. Notice that since Q is an orthogonal matrix, thus we have:\n Q Q^T = I_n = [e_1, e_2, \\dots, e_n] \nBy extension, given an initial pair (A, q_1) satisfying \\lVert q_1 \\rVert = 1, we have:\n\nK_n(A, q_1) = Q Q^T K_n(A, q_1) = Q[ \\, e_1 \\mid T e_1 \\mid T^2 e_1 \\mid \\dots \\mid T^{n-1} e_1 \\, ]\n\nThis is actually QR factorization! Indeed, the Implicit Q Theorem asserts that if an upper Hessenburg matrix T \\in \\mathbb{R}^{n \\times n} has only positive elements on its first subdiagonal and there exists an orthogonal matrix Q such that Q^T A Q = T, then Q and T are uniquely determined3 by (A, q_1).\nThus, tridiagonalizing A with respect to an arbitrary q_1 \\in \\mathbb{R}^n satisfying \\lVert q_1\\rVert = 1 determines Q.",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "advanced/cpp_integration.html",
    "href": "advanced/cpp_integration.html",
    "title": "C++ Integration",
    "section": "",
    "text": "primate’s generic API is enabled through function templates specialized using C++20 concepts. In other words, a function F requiring concept C will compile with any type T so long as T respects the constraints imposed by C. For example, generically, any type T respecting the LinearOperatorconcept shown below can be passed to the Lanczos method:\nIn english, an instance A of type T is said to support the LinearOperator concept if it has:\nshape() should return a pair (n,m) representing the sizes of the output and input vectors, respectively. Note in the matrix setting this corresponds to the number of rows and columns.",
    "crumbs": [
      "Advanced",
      "Usage from C++"
    ]
  },
  {
    "objectID": "advanced/cpp_integration.html#the-linearoperator-concept",
    "href": "advanced/cpp_integration.html#the-linearoperator-concept",
    "title": "C++ Integration",
    "section": "",
    "text": "primate’s generic API is enabled through C++20 concepts. Thus, the more exact statement is that any type respecting the LinearOperator concept shown below can be passed:\nusing FP = std::floating_point; \ntemplate &lt; typename T, FP float_t = typename T::value_type &gt;\nconcept LinearOperator = requires(T A, const float_t* v, float_t* o) {\n  { A.matvec(v, o) }; // o = A v\n  { A.shape() } -&gt; std::convertible_to&lt; std::pair&lt; size_t, size_t &gt; &gt;;\n};\nAn instance A of type T is said to support the LinearOperator concept if it has:\n\nA method Av \\mapsto o, with signature A.matvec(const float_t* v, float_t* o)\nA method yielding (\\mathrm{card}(o), \\mathrm{card}(v)), with signatureA.shape() -&gt; pair&lt; ... &gt;\n\nshape() should yield a pair (n,m) representing the sizes of the output and input vectors, respectively. This corresponds to the number of rows and columns in the matrix setting.",
    "crumbs": [
      "Advanced",
      "Usage from C++"
    ]
  },
  {
    "objectID": "advanced/cpp_integration.html#other-concepts",
    "href": "advanced/cpp_integration.html#other-concepts",
    "title": "C++ Integration",
    "section": "Other Concepts",
    "text": "Other Concepts\nDepending on the problem at hand, the supplied operator may need to meet other constraints. Here’s a short list additional operator concepts:\n\n\n\n\n\n\n\n\n\nConcept\nSupports\nSignature\nRequires\n\n\n\n\nLinearOperator\nA v \\mapsto o\nA.matvec(v, o)\nNA\n\n\nAdjointOperator\nA^T v \\mapsto o\nA.rmatvec(v, o)\nLinearOperator\n\n\nAdditiveOperator\no \\gets o + \\alpha Av\nA.matvec_add(v, alpha, o)\nLinearOperator\n\n\nAffineOperator\nSets t s.t. A + tB\nA.set_parameter(t)\nLinearOperator\n\n\nQuadOperator\nv^T A v\nA.quad(v)\nNA\n\n\n\nThe exported methods in primate only need the minimum constraints to be satisfied to compile: if you need access to the Lanczos method, then just supporting the LinearOperator concept is sufficient. On the other hand, adding support for other constraints can optimize the efficiency of certain methods; for example, the hutch method technically only requires a LinearOperator to do trace estimation (via matvec calls), but will also compile and prefer calling quad with a QuadOperator as input. In such a situaton, if your operator has an efficient quadratic form v \\mapsto v^T A v, then implementing quad may improve the performance of hutch.",
    "crumbs": [
      "Advanced",
      "Usage from C++"
    ]
  },
  {
    "objectID": "advanced/pybind11_integration.html",
    "href": "advanced/pybind11_integration.html",
    "title": "pybind11 Integration",
    "section": "",
    "text": "If you’re using pybind11, you can easily incorporate your own custom linear operator / matrix function pair using primates binding headers.\nTODO",
    "crumbs": [
      "Advanced",
      "Integrating with pybind11"
    ]
  },
  {
    "objectID": "theory/lanczos.html#complexities",
    "href": "theory/lanczos.html#complexities",
    "title": "The Lanczos method",
    "section": "Complexities",
    "text": "Complexities\nElegant as the Lanczos method may be, what does it net us? Well, for starters, the Lanczos method can drop the complexity of obtaining the spectral decomposition by order of magnitude.\n\nTheorem 1 (Parlett 1994, Simon 1984) Given a symmetric rank-r matrix A \\in \\mathbb{R}^{n \\times n} whose operator x \\mapsto A x requires O(\\eta) time and O(\\nu) space, the Lanczos method computes \\Lambda(A) in O(\\max\\{\\eta, n\\}\\cdot r) time and O(\\max\\{\\nu, n\\}) space, when computation is done in exact arithmetic\n\nFor general A \\in \\mathbb{R}^{n \\times n}, computing the spectral-decomposition is essentially bounded by the matrix-multiplication time: \\Theta(n^\\omega) time and \\Theta(n^2) space, where \\omega \\approx 2.37\\dots is the matrix multiplication constant. If we exclude the Strassen model for computation, we get effectively a \\Omega(n^3) time and \\Omega(n^2) space bound. In contrast, when both r = n and \\eta = \\nu = n, the Theorem above implies the spectral decomposition takes just O(n^2) time and O(n) space to obtain. Unless you know the tricks, this feels like a contradiction. So… how does the complexity argument play out?\n\nSolving the right problem\n\n\nIf you squint hard enough, you can deduce that since A Q_j = Q_j T_j + \\beta_{j+1} q_{j+1} e_{j}^T, every symmetric A \\in \\mathbb{R}^{n \\times n} expanded this way admits a three-term recurrence: \n\\begin{align*}\nA q_j &= \\beta_{j\\text{-}1} q_{j\\text{-}1} + \\alpha_j q_j + \\beta_j q_{j+1} \\\\\n\\Leftrightarrow \\beta_{j} \\, q_{j+1} &= A q_j - \\alpha_j \\, q_j - \\beta_{j\\text{-}1} \\, q_{j\\text{-}1}  \n\\end{align*}\n\nThe equation above is a variable-coefficient second-order linear difference equation, and it is known such equations have unique solutions, which are given by: \n\\alpha_j = q_j^T A q_j, \\;\\; \\beta_j = \\lVert r_j \\rVert_2, \\;\\; q_{j+1} = r_j / \\beta_j\n\n\n\\text{where  } r_j = (A - \\alpha_j I)q_j - \\beta_{j\\text{-}1} q_j\n\nIn other words, if (q_{j\\text{-}1}, \\beta_j, q_j) are known, then (\\alpha_j, \\beta_{j+1}, q_{j+1}) are completely determined. This fact is fantastic from a computational point of view: no explicit call to the QR algorithm necessary2!\nNote that a symmetric tridiagonal matrix is fully characterized by its diagonal and subdiagonal terms, which requires just O(n) space. If we assume that v \\mapsto Av \\sim O(n), then the above procedure clearly takes at most O(n^2) time, since there are most n such vectors \\{q_i\\}_{i=1}^n to generate!\nMoreover, if we only need the eigen-values \\Lambda(A) ( and not their vectors U), then we may execute the recurrence keeping at most three vectors \\{q_{j-1}, q_{j}, q_{j+1}\\} in memory at any given time. Since each of these is O(n) is size, the claim of O(n) space is justified!\n\n\nBut wait, isn’t T arbitrary?\nUnfortunately, there is no canonical choice of T_j. Indeed, as T_n is a family with n - 1 degrees of freedom and v \\in \\mathbb{R}^n was chosen arbitrarily, there are infinitely many essentially distinct such decompositions. In contrast, the spectral decomposition A = U \\Lambda U^T identifies a diagonalizable A with its spectrum \\Lambda(A) up to a change of basis A \\mapsto M^{-1} A M.\nNot all hope is lost though. Notice that since Q is an orthogonal matrix, thus we have:\n Q Q^T = I_n = [e_1, e_2, \\dots, e_n] \nBy extension, given an initial pair (A, q_1) satisfying \\lVert q_1 \\rVert = 1, we have:\n\nK_n(A, q_1) = Q Q^T K_n(A, q_1) = Q[ \\, e_1 \\mid T e_1 \\mid T^2 e_1 \\mid \\dots \\mid T^{n-1} e_1 \\, ]\n\nThis is actually QR factorization! Indeed, the Implicit Q Theorem asserts that if an upper Hessenburg matrix T \\in \\mathbb{R}^{n \\times n} has only positive elements on its first subdiagonal and there exists an orthogonal matrix Q such that Q^T A Q = T, then Q and T are uniquely determined3 by (A, q_1).",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "theory/lanczos.html#approximation",
    "href": "theory/lanczos.html#approximation",
    "title": "The Lanczos method",
    "section": "Approximation",
    "text": "Approximation\nTODO",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#examples",
    "href": "reference/diagonalize.lanczos.html#examples",
    "title": "diagonalize.lanczos",
    "section": "Examples",
    "text": "Examples\n  ::: {#af54fa09 .cell execution_count=1}\n  ``` {.python .cell-code}\n          print(\"hello\")\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  hello\n  ```\n  :::\n  :::",
    "crumbs": [
      "API Reference",
      "Diagonalize",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#returns",
    "href": "reference/diagonalize.lanczos.html#returns",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\ntuple\nA tuple (a,b) parameterizing the diagonal and off-diagonal of the tridiagonal matrix. If return_basis=True, the tuple (a,b), Q is returned, where Q represents an orthogonal basis for the degree-deg Krylov subspace.",
    "crumbs": [
      "API Reference",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#see-also",
    "href": "reference/diagonalize.lanczos.html#see-also",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "scipy.linalg.eigh_tridiagonal : Eigenvalue solver for real symmetric tridiagonal matrices. operator.matrix_function : Approximates the action of a matrix function via the Lanczos method.",
    "crumbs": [
      "API Reference",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#notes",
    "href": "reference/diagonalize.lanczos.html#notes",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "The Lanczos method iteratively builds a tridiagonal matrix T of a symmetric A via an orthogonal change-of-basis Q:  Q^T A Q  = T  Unlike other Lanczos implementations (e.g. SciPy’s eigsh), which includes e.g. sophisticated restarting, deflation, and selective-reorthogonalization steps, this method simply executes deg steps of the Lanczos method with the supplied v0 and returns the resulting tridiagonal matrix.\nDiagonalizing T via e.g. scipy.linalg.eigh_tridiagonal yields Rayleigh-Ritz approximations of the eigenvalues of A, though note no checking is performed for ‘ghost’ or already converged eigenvalues. To increase the accuracy of these eigenvalue approximation, try increasing orth and deg. Supplying either negative values or values larger than deg for orth will result in full re-orthogonalization, though note the number of matvecs scales linearly with deg and the number of inner-products scales quadratically with orth.",
    "crumbs": [
      "API Reference",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/diagonalize.lanczos.html#references",
    "href": "reference/diagonalize.lanczos.html#references",
    "title": "diagonalize.lanczos",
    "section": "",
    "text": "Paige, Christopher C. “Computational variants of the Lanczos method for the eigenproblem.” IMA Journal of Applied Mathematics 10.3 (1972): 373-381.",
    "crumbs": [
      "API Reference",
      "Lanczos"
    ]
  },
  {
    "objectID": "reference/primate.trace.hutch.html",
    "href": "reference/primate.trace.hutch.html",
    "title": "hutch",
    "section": "",
    "text": "trace.hutch(A, fun=None, maxiter=200, deg=20, atol=None, rtol=None, stop=['confidence', 'change'], ncv=2, orth=0, quad='fttr', confidence=0.95, pdf='rademacher', rng='pcg64', seed=-1, num_threads=0, verbose=False, info=False, plot=False, **kwargs)\nEstimates the trace of a matrix A or matrix function f(A) via a Girard-Hutchinson estimator.\nThis function uses up to maxiter random isotropic vectors to form an unbiased estimator of the trace of A. The estimator is obtained by averaging quadratic forms of A (or f(A)), rescaling as necessary.\n\n\nFor matrix functions, the Lanczos method up to degree deg is used to approximate the action of f(A). By default,\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nA\nndarray, sparray, or LinearOperator\nreal symmetric operator.\nrequired\n\n\nfun\nstr or typing.Callable\nreal-valued function defined on the spectrum of A.\n\"identity\"\n\n\nmaxiter\nint\nMaximum number of random vectors to sample for the trace estimate.\n10\n\n\ndeg\nint\nDegree of the quadrature approximation. Must be at least 1.\n20\n\n\natol\nfloat\nAbsolute tolerance to signal convergence for early-stopping. See notes.\nNone\n\n\nrtol\nfloat\nRelative tolerance to signal convergence for early-stopping. See notes.\n1e-2\n\n\nstop\nstr\nEarly-stopping criteria to test estimator convergence. See details.\n\"confidence\"\n\n\nncv\nint\nNumber of Lanczos vectors to allocate. Must be at least 2.\n2\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against. Must be less than ncv.\n0\n\n\nquad\nstr\nMethod used to obtain the weights of the Gaussian quadrature. See notes.\n'fttr'\n\n\nconfidence\nfloat\nConfidence level to consider estimator as converged. Only used when stop = “confidence”.\n0.95\n\n\npdf\n‘rademacher’, ‘normal’\nChoice of zero-centered distribution to sample random vectors from.\n'rademacher'\n\n\nrng\n‘splitmix64’, ’xoshiro256**‘, ’pcg64’, ‘lcg64’, ‘mt64’\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint\nSeed to initialize the rng entropy source. Set seed &gt; -1 for reproducibility.\n-1\n\n\nnum_threads\nint\nNumber of threads to use to parallelize the computation. Set to &lt;= 0 to let OpenMP decide.\n0\n\n\nplot\nbool\nIf true, plots the samples of the trace estimate along with their convergence characteristics.\nFalse\n\n\ninfo\nbool\nIf True, returns a dictionary containing all relevant information about the computation.\nFalse\n\n\nkwargs\ndict\nadditional key-values to parameterize the chosen function ‘fun’.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nfloat\nEstimate of the trace of A, if fun = \"identity\", otherwise estimates the trace of f(A).\n\n\n(dict, optional)\nIf ‘info = True’, additional information about the computation.\n\n\n\n\n\n\nlanczos : the lanczos algorithm.\n\n\n\n[1] Ubaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099."
  },
  {
    "objectID": "reference/primate.trace.hutch.html#notes",
    "href": "reference/primate.trace.hutch.html#notes",
    "title": "hutch",
    "section": "",
    "text": "For matrix functions, the Lanczos method up to degree deg is used to approximate the action of f(A). By default,"
  },
  {
    "objectID": "reference/primate.trace.hutch.html#parameters",
    "href": "reference/primate.trace.hutch.html#parameters",
    "title": "hutch",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nA\nndarray, sparray, or LinearOperator\nreal symmetric operator.\nrequired\n\n\nfun\nstr or typing.Callable\nreal-valued function defined on the spectrum of A.\n\"identity\"\n\n\nmaxiter\nint\nMaximum number of random vectors to sample for the trace estimate.\n10\n\n\ndeg\nint\nDegree of the quadrature approximation. Must be at least 1.\n20\n\n\natol\nfloat\nAbsolute tolerance to signal convergence for early-stopping. See notes.\nNone\n\n\nrtol\nfloat\nRelative tolerance to signal convergence for early-stopping. See notes.\n1e-2\n\n\nstop\nstr\nEarly-stopping criteria to test estimator convergence. See details.\n\"confidence\"\n\n\nncv\nint\nNumber of Lanczos vectors to allocate. Must be at least 2.\n2\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against. Must be less than ncv.\n0\n\n\nquad\nstr\nMethod used to obtain the weights of the Gaussian quadrature. See notes.\n'fttr'\n\n\nconfidence\nfloat\nConfidence level to consider estimator as converged. Only used when stop = “confidence”.\n0.95\n\n\npdf\n‘rademacher’, ‘normal’\nChoice of zero-centered distribution to sample random vectors from.\n'rademacher'\n\n\nrng\n‘splitmix64’, ’xoshiro256**‘, ’pcg64’, ‘lcg64’, ‘mt64’\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint\nSeed to initialize the rng entropy source. Set seed &gt; -1 for reproducibility.\n-1\n\n\nnum_threads\nint\nNumber of threads to use to parallelize the computation. Set to &lt;= 0 to let OpenMP decide.\n0\n\n\nplot\nbool\nIf true, plots the samples of the trace estimate along with their convergence characteristics.\nFalse\n\n\ninfo\nbool\nIf True, returns a dictionary containing all relevant information about the computation.\nFalse\n\n\nkwargs\ndict\nadditional key-values to parameterize the chosen function ‘fun’.\n{}"
  },
  {
    "objectID": "reference/primate.trace.hutch.html#returns",
    "href": "reference/primate.trace.hutch.html#returns",
    "title": "hutch",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nfloat\nEstimate of the trace of A, if fun = \"identity\", otherwise estimates the trace of f(A).\n\n\n(dict, optional)\nIf ‘info = True’, additional information about the computation."
  },
  {
    "objectID": "reference/primate.trace.hutch.html#see-also",
    "href": "reference/primate.trace.hutch.html#see-also",
    "title": "hutch",
    "section": "",
    "text": "lanczos : the lanczos algorithm."
  },
  {
    "objectID": "reference/primate.trace.hutch.html#reference",
    "href": "reference/primate.trace.hutch.html#reference",
    "title": "hutch",
    "section": "",
    "text": "[1] Ubaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099."
  },
  {
    "objectID": "reference/primate.trace.sl_gauss.html#parameters",
    "href": "reference/primate.trace.sl_gauss.html#parameters",
    "title": "sl_gauss",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nA\nndarray, sparray, or LinearOperator\nreal symmetric operator.\nrequired\n\n\nn\nint\nNumber of random vectors to sample for the quadrature estimate.\n150\n\n\ndeg\nint\nDegree of the quadrature approximation.\n20\n\n\nrng\n‘splitmix64’, ’xoshiro256**‘, ’pcg64’, ‘lcg64’, ‘mt64’\nRandom number generator to use (PCG64 by default).\n'splitmix64'\n\n\nseed\nint\nSeed to initialize the rng entropy source. Set seed &gt; -1 for reproducibility.\n-1\n\n\npdf\n‘rademacher’, ‘normal’\nChoice of zero-centered distribution to sample random vectors from.\n'rademacher'\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against when building the Krylov basis.\n0\n\n\nnum_threads\nint\nNumber of threads to use to parallelize the computation. Setting num_threads &lt; 1 to let OpenMP decide.\n0",
    "crumbs": [
      "API Reference",
      "Trace",
      "SL Gauss"
    ]
  },
  {
    "objectID": "reference/primate.trace.sl_gauss.html#returns",
    "href": "reference/primate.trace.sl_gauss.html#returns",
    "title": "sl_gauss",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\nfloat\nEstimate of the trace of the matrix function f(A).\n\n\n(dict, optional)\nIf ‘info = True’, additional information about the computation.",
    "crumbs": [
      "API Reference",
      "Trace",
      "SL Gauss"
    ]
  },
  {
    "objectID": "theory/lanczos.html#the-iteration-part",
    "href": "theory/lanczos.html#the-iteration-part",
    "title": "The Lanczos method",
    "section": "The “iteration” part",
    "text": "The “iteration” part\nLanczos originally referred to his algorithm as the method of minimized iterations, and indeed nowadays it is often called an iterative method. Where’s the iterative component?\nIf you squint hard enough, you can deduce that for every j \\in [1, n): A Q_j = Q_j T_j + \\beta_{j+1} q_{j+1} e_{j}^T Equating the j-th columns on each side of the equation and rearranging yields a three-term recurrence: \n\\begin{align*}\nA q_j &= \\beta_{j\\text{-}1} q_{j\\text{-}1} + \\alpha_j q_j + \\beta_j q_{j+1} \\\\\n\\Leftrightarrow \\beta_{j} \\, q_{j+1} &= A q_j - \\alpha_j \\, q_j - \\beta_{j\\text{-}1} \\, q_{j\\text{-}1}  \n\\end{align*}\n\nThe equation above is a variable-coefficient second-order linear difference equation, and it is known such equations have unique solutions; they are given below: \n\\alpha_j = q_j^T A q_j, \\;\\; \\beta_j = \\lVert r_j \\rVert_2, \\;\\; q_{j+1} = r_j / \\beta_j\n\n\n\\text{where  } r_j = (A - \\alpha_j I)q_j - \\beta_{j\\text{-}1} q_j\n\nIn other words, if (q_{j\\text{-}1}, \\beta_j, q_j) are known, then (\\alpha_j, \\beta_{j+1}, q_{j+1}) are completely determined. In theory, this means we can iteratively generate both Q and T using just a couple vectors at a time—no need to explicitly call to the QR algorithm as shown above. Pretty nifty, eh!",
    "crumbs": [
      "Theory",
      "The Lanczos Method"
    ]
  },
  {
    "objectID": "theory/lanczos.html#complexity-analysis",
    "href": "theory/lanczos.html#complexity-analysis",
    "title": "The Lanczos method",
    "section": "Complexity analysis",
    "text": "Complexity analysis\nElegant and as theoretically founded as the Lanczos method may be, is it efficient in practice?\nLet’s start by establishing a baseline on its complexity:\n\nTheorem 1 (Parlett 1994) Given a symmetric rank-r matrix A \\in \\mathbb{R}^{n \\times n} whose operator x \\mapsto A x requires O(\\eta) time and O(\\nu) space, the Lanczos method computes \\Lambda(A) in O(\\max\\{\\eta, n\\}\\cdot r) time and O(\\max\\{\\nu, n\\}) space, when computation is done in exact arithmetic\n\nAs its clear from the theorem, if we specialize it such that r = n and \\eta = \\nu = n, then the Lanczos method requires just O(n^2) time and O(n) space to execute. In other words, the Lanczos method drops both the time and space complexity4 of obtaining spectral information by order of magnitude over similar eigen-algorithms that decompose A directly.\nTo see why this is true, note that a symmetric tridiagonal matrix is fully characterized by its diagonal and subdiagonal terms, which requires just O(n) space. If we assume that v \\mapsto Av \\sim O(n), then carrying out the recurrence clearly takes at most O(n^2) time, since there are most n such vectors \\{q_i\\}_{i=1}^n to generate!\nNow, if we need to store all of Y or Q explicitly, we clearly need O(n^2) space to do so. However, if we only need the eigen-values \\Lambda(A) (and not their eigen-vectors U), then we may execute the recurrence keeping at most three vectors \\{q_{j-1}, q_{j}, q_{j+1}\\} in memory at any given time. Since each of these is O(n) is size, the claim of O(n) space is justified!",
    "crumbs": [
      "Theory",
      "The Lanczos Method"
    ]
  },
  {
    "objectID": "theory/lanczos.html#rayleigh-ritz-approximations",
    "href": "theory/lanczos.html#rayleigh-ritz-approximations",
    "title": "The Lanczos method",
    "section": "Rayleigh-Ritz approximations",
    "text": "Rayleigh-Ritz approximations\nSuppose instead of constructing the full T \\in \\mathbb{R}^{n \\times n}, we stop at the j^{\\text{th}} iteration, where 1 \\leq j &lt; n.\n\nT_j = \\mathrm{tridiag}\\Bigg(\n\\begin{array}{ccccccccc}\n& \\beta_2 & & \\beta_3 & & \\cdot & & \\beta_j & \\\\\n\\alpha_1 & & \\alpha_2 & & \\cdot & & \\cdot & & \\alpha_j \\\\\n& \\beta_2 & & \\beta_3 & & \\cdot & & \\beta_j &\n\\end{array}\n\\Bigg)\n\nIt is natural to assume that the eigenvalues of T_j approximate some of eigenvalues of \\Lambda(A). This intuition not only turns out to be true, but in fact the eigenvalues of T_j are known to be optimal approximations of \\Lambda(A) under many appealing notions of optimality. Thus, if we need only to approximate the spectrum of A, we may potentially do so using just j &lt;&lt; n iterations; indeed, this is the hallmark of the iterative approach to obtaining eigenvalues:",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "theory/lanczos.html#wait-isnt-t-arbitrary",
    "href": "theory/lanczos.html#wait-isnt-t-arbitrary",
    "title": "The Lanczos method",
    "section": "Wait, isn’t T arbitrary?",
    "text": "Wait, isn’t T arbitrary?\nUnfortunately—and unlike the spectral decomposition2—there is no canonical choice of T. Indeed, as T is a family with n - 1 degrees of freedom and v \\in \\mathbb{R}^n was chosen arbitrarily, there are infinitely many essentially distinct such decompositions.\nNot all hope is lost though, as it turns out that T is actually fully characterized by v. To see this, notice that since Q is an orthogonal matrix, we have:\n Q Q^T = I_n = [e_1, e_2, \\dots, e_n] \nBy extension, given an initial pair (A, q_1) satisfying \\lVert q_1 \\rVert = 1, the following holds:\n\nK_n(A, q_1) = Q Q^T K_n(A, q_1) = Q[ \\, e_1 \\mid T e_1 \\mid T^2 e_1 \\mid \\dots \\mid T^{n-1} e_1 \\, ]\n\n…this is actually a QR factorization, which is essentially unique! Indeed, the Implicit Q Theorem asserts that if an upper Hessenburg matrix T \\in \\mathbb{R}^{n \\times n} has only positive elements on its first subdiagonal and there exists an orthogonal matrix Q such that Q^T A Q = T, then Q and T are uniquely determined3 by (A, q_1).",
    "crumbs": [
      "Theory",
      "The Lanczos Method"
    ]
  },
  {
    "objectID": "theory/lanczos.html#finite-precision",
    "href": "theory/lanczos.html#finite-precision",
    "title": "The Lanczos method",
    "section": "Finite-precision",
    "text": "Finite-precision\nThough elegant as the Lanczos method is, the complexity statements and much of the theory holds only in exact arith",
    "crumbs": [
      "Theory",
      "Lanczos"
    ]
  },
  {
    "objectID": "basic/integration.html#python-usage",
    "href": "basic/integration.html#python-usage",
    "title": "Integration",
    "section": "",
    "text": "primate supports a variety of matrix-types of the box, including numpy ndarray’s, compressed sparse matrices (a lá SciPy), and LinearOperators—the latter enables the use of matrix free operators.\nOutside of the natively types above, the basic requirements for any operator A to be used with e.g. the Lanczos method in primate are:\n\nA method A.matvec(input: ndarray) -&gt; ndarray implementing v \\mapsto Av\nAn attribute A.shape -&gt; tuple[int, int] giving the output/input dimensions of A\n\n\n\nHere’s an example of a simple operator representing a Diagonal matrix, which inherits a .matvec() method by following the subclassing rules of SciPy’s LinearOperator:\nimport numpy as np \nfrom numpy.typing import ArrayLike\nfrom scipy.sparse.linalg import LinearOperator \n\nclass DiagonalOp(LinearOperator):\n  diag: np.ndarray = None\n  \n  def __init__(self, d: ArrayLike, dtype = None):\n    self.diag = np.array(d)\n    self.shape = (len(d), len(d))\n    self.dtype = np.dtype('float32') if dtype is None else dtype\n\n  def _matvec(self, x: ArrayLike) -&gt; np.ndarray:\n    out = self.diag * np.ravel(x)\n    return out.reshape(x.shape)",
    "crumbs": [
      "Basics",
      "Integration"
    ]
  },
  {
    "objectID": "basic/integration.html#c-usage",
    "href": "basic/integration.html#c-usage",
    "title": "Integration",
    "section": "C++ usage",
    "text": "C++ usage\nSimilarly, to get started calling any matrix-free function provided by primate on the C++ side, such hutch or lanczos, simply pass any type with .shape() and .matvec() member functions:\nclass LinOp {\n  int nrow, ncol;\n  \n  LinOp(int nr, int nc) : nrow(nr), ncol(nc) {}\n  \n  void matvec(const float* input, float* output) const {\n    ... // implementation details \n  }\n\n  void shape() const { return std::make_pair(nrow, ncol); }\n}\nIt’s up to you to ensure shape() yields the correct size; primate will supply vectors to input of size .shape().second (number of columns) and guarantees the pointer to the output will be at least shape().first (number of rows), no more.\nTo read more about how semantics extend to the C++ side as well via C++20 concepts—see the C++ integration guide. If you’re using pybind11 and you want to extend primate’s Python API to work natively with linear operator implemented in C++, see the pybind11 integration guide.",
    "crumbs": [
      "Basics",
      "Integration"
    ]
  },
  {
    "objectID": "basic/usage.html",
    "href": "basic/usage.html",
    "title": "primate usage - quickstart",
    "section": "",
    "text": "Below is a quick introduction to primate. For more introductory material, theor\nTo do trace estimation, use functions in the trace module:\n\nimport primate.trace as TR\nfrom primate.random import symmetric\nA = symmetric(150)  ## random positive-definite matrix \n\nprint(f\"Actual trace: {A.trace():6f}\")        ## Actual trace\nprint(f\"Girard-Hutch: {TR.hutch(A):6f}\")      ## Monte-carlo tyoe estimator\nprint(f\"XTrace:       {TR.xtrace(A):6f}\")     ## Epperly's algorithm\n\nActual trace: 75.697397\nGirard-Hutch: 75.981329\nXTrace:       75.697398\n\n\nFor matrix functions, you can either construct a LinearOperator directly via the matrix_function API, or supply a string to the parameter fun describing the spectral function to apply. For example, one might compute the log-determinant as follows:\n\nfrom primate.operator import matrix_function\nM = matrix_function(A, fun=\"log\")\n\new = np.linalg.eigvalsh(A)\nprint(f\"logdet(A):  {np.sum(np.log(ew)):6f}\")\nprint(f\"GR approx:  {TR.hutch(M):6f}\")\nprint(f\"XTrace:     {TR.xtrace(M):6f}\")\n\n## Equivalently could've used: \n## M = matrix_function(A, fun=np.log)\n\nlogdet(A):  -148.321844\nGR approx:  -147.250366\nXTrace:     -148.315937\n\n\nNote in the above example you can supply to fun either string describing a built-in spectral function or an arbitrary Callable. The former is preferred when possible, as function evaluations will generally be faster and hutch can also be parallelized. Multi-threaded execution of e.g. hutch with arbitrary functions is not currently allowed due to the GIL, though there are options available, see the integration docs for more details.\nFor ‘plain’ operators, XTrace should recover the exact trace (up to roundoff error). For matrix functions f(A), there will be some inherent inaccuracy as the underlying matrix-vector multiplication is approximated with the Lanczos method.\nIn general, the amount of accuracy depends both on the Lanczos parameters and the type of matrix function. Spectral functions that are difficult or impossible to approximate via low-degree polynomials, for example, may suffer more from inaccuracy issues than otherwise. For example, consider the example below that computes that rank:\n\n## Make a rank-deficient operator\new = np.sort(ew)\new[:30] = 0.0\nA = symmetric(150, ew = ew, pd = False)\nM = matrix_function(A, fun=np.sign)\n\nprint(f\"Rank:       {np.linalg.matrix_rank(A)}\")\nprint(f\"GR approx:  {TR.hutch(M)}\")\nprint(f\"XTrace:     {TR.xtrace(M)}\")\n\nRank:       120\nGR approx:  144.8700408935547\nXTrace:     143.97018151807674\n\n\nThis is not so much a fault of hutch or xtrace as much as it is the choice of approximation and Lanczos parameters. The sign function has a discontinuity at 0, is not smooth, and is difficult to approximate with low-degree polynomials. One workaround to handle this issue is relax the sign function with a low-degree “soft-sign” function:  \\mathcal{S}_\\lambda(x) = \\sum\\limits_{i=0}^q \\left( x(1 - x^2)^i \\prod_{j=1}^i \\frac{2j - 1}{2j} \\right)\nVisually, the soft-sign function looks like this:\n\nfrom primate.special import soft_sign, figure_fun\nshow(figure_fun(\"smoothstep\"))\n\n\n  \n\n\n\n\n\nIt’s been shown that there is a low degree polynomial p^\\ast that uniformly approximates \\mathcal{S}_\\lambda up to a small error on the interval [-1,1]. Since matrix_function uses a low degree Krylov subspace to approximate the action v \\mapsto f(A)v, one way to improve the accuracy of rank estimation is to replace \\mathrm{sign} \\mapsto \\mathcal{S}_{\\lambda} for some choice of q \\in \\mathbb{Z}_+ (this function is available in primate under the name soft_sign):\n\nfrom primate.special import soft_sign\nfor q in range(0, 50, 5):\n  M = matrix_function(A, fun=soft_sign(q=q))\n  print(f\"XTrace S(A) for q={q}: {TR.xtrace(M):6f}\")\n\nXTrace S(A) for q=0: 72.776775\nXTrace S(A) for q=5: 109.325087\nXTrace S(A) for q=10: 114.843157\nXTrace S(A) for q=15: 116.952146\nXTrace S(A) for q=20: 118.027760\nXTrace S(A) for q=25: 118.657212\nXTrace S(A) for q=30: 119.055340\nXTrace S(A) for q=35: 119.319911\nXTrace S(A) for q=40: 119.501839\nXTrace S(A) for q=45: 119.630120\n\n\nIf the type of operator A is known to typically have a large spectral gap, another option is to compute the numerical rank by thresholding values above some fixed value \\lambda_{\\text{min}}. This is equivalent to applying the following spectral function:\n S_{\\lambda_{\\text{min}}}(x) =\n\\begin{cases}\n1 & \\text{ if } x \\geq \\lambda_{\\text{min}} \\\\\n0 & \\text{ otherwise }\n\\end{cases}\n\nIn the above example, the optimal cutoff \\lambda_{\\text{min}} is given by the smallest non-zero eigenvalue. Since the trace estimators all stochastic to some degree, we set the cutoff to slightly less than this value:\n\nlambda_min = min(ew[ew != 0.0])\nprint(f\"Smallest non-zero eigenvalue: {lambda_min:.6f}\")\n\nstep = lambda x: 1 if x &gt; (lambda_min * 0.90) else 0\nM = matrix_function(A, fun=step, deg=50)\nprint(f\"XTrace S_t(A) for t={lambda_min*0.90:.4f}: {TR.xtrace(M):6f}\")\n\nSmallest non-zero eigenvalue: 0.191916\nXTrace S_t(A) for t=0.1727: 120.000000\n\n\nIndeed, this works! Of course, here we’ve used the fact that we know the optimal cutoff value, but this can also be estimated with the lanczos method itself.\n\nfrom primate.diagonalize import lanczos\nfrom scipy.linalg import eigvalsh_tridiagonal\na,b = lanczos(A)\nrr = eigvalsh_tridiagonal(a,b) # Rayleigh-Ritz values\ntol = 10 * np.finfo(A.dtype).resolution\nprint(f\"Approx. cutoff: {np.min(rr[rr &gt; tol]):.6f}\")\n\nApprox. cutoff: 0.191916"
  },
  {
    "objectID": "theory/intro.html",
    "href": "theory/intro.html",
    "title": "Introduction to trace estimation with primate",
    "section": "",
    "text": "primate contains a variety of methods for estimating quantities from matrix functions. One popular such quantity is the trace of f(A), for some generic f: [a,b] \\to \\mathbb{R} defined on the spectrum of A:  \\mathrm{tr}(f(A)) \\triangleq \\mathrm{tr}(U f(\\Lambda) U^{\\intercal}), \\quad \\quad f : [a,b] \\to \\mathbb{R}\nMany quantities of common interest can be expressed in as traces of matrix functions with the right spectral function specialization. A few such specialization include:\n\\begin{align*}\nf &= \\mathrm{id} \\quad &\\Longleftrightarrow& \\quad &\\mathrm{tr}(A) \\\\\nf &= f^{-1} \\quad &\\Longleftrightarrow& \\quad &\\mathrm{tr}(A^{-1}) \\\\\nf &= \\log \\quad &\\Longleftrightarrow& \\quad  &\\mathrm{logdet}(A) \\\\\nf &= \\mathrm{exp} \\quad &\\Longleftrightarrow& \\quad &\\mathrm{exp}(A) \\\\\nf &= \\mathrm{sign} \\quad &\\Longleftrightarrow& \\quad &\\mathrm{rank}(A)  \\\\\n&\\vdots \\quad &\\hphantom{\\Longleftrightarrow}& \\quad & \\vdots &\n\\end{align*}\nIn this introduction, the basics of randomized trace estimation are introduced, with a focus on matrix-free algorithms how the readily extend to estimating the traces of matrix functions.",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "theory/intro.html#basics-trace-estimation",
    "href": "theory/intro.html#basics-trace-estimation",
    "title": "Introduction to trace estimation with primate",
    "section": "Basics: Trace estimation",
    "text": "Basics: Trace estimation\n\nSuppose we wanted to compute the trace of some matrix A. By the very definition of the trace, we have a variety of means to do it: \\mathrm{tr}(A) \\triangleq \\sum\\limits_{i=1}^n A_{ii} = \\sum\\limits_{i=1}^n \\lambda_i = \\sum\\limits_{i=1}^n e_i^T A e_i \nwhere, in the right-most sum, we’re using e_i to represent the ith identity vector:  A_{ii} = e_i^T A e_i, \\quad e_i = [0, \\dots, 0, \\underbrace{1}_{i}, 0, \\dots, 0] \nLet’s see some code. First, we start with a simple (random) positive-definite matrix A \\in \\mathbb{R}^{n \\times n}.\n\nfrom primate.random import symmetric\nA = symmetric(150, pd = True)\n\nBy default, symmetric normalizes A such that the eigenvalues are uniformly distributed in the interval [0, 1]. For reference, the spectrum of A looks as follows:\n\n\n\n  \n\n\n\n\n\nNow, using numpy, we can verify all of these trace definitions are indeed equivalent:\n\n\nimport numpy as np\neye = lambda i: np.ravel(np.eye(1, 150, k=i))\nprint(f\"Direct: {np.sum(A.diagonal()):.8f}\")\nprint(f\"Eigen:  {np.sum(np.linalg.eigvalsh(A)):.8f}\")\nprint(f\"Matvec: {sum([eye(i) @ A @ eye(i) for i in range(150)]):.8f}\")\n\nDirect: 75.69739746\nEigen:  75.69739746\nMatvec: 75.69739746\n\n\nWhile (1) trivially yields \\mathrm{tr}(A) in (optimal) O(n) time, there exist situations where the diagonal entries of A are not available—particularly in the large-scale regime. In contrast, though both (2) and (3) are less efficient, they are nonetheless viable altenrnatives to obtain \\mathrm{tr}(A).",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "theory/intro.html#the-implicit-trace-estimation-problem",
    "href": "theory/intro.html#the-implicit-trace-estimation-problem",
    "title": "Introduction to trace estimation with primate",
    "section": "The implicit trace estimation problem",
    "text": "The implicit trace estimation problem\nThe implicit trace estimation problem can be stated as follows:\n\nGiven access to a square matrix A \\in \\mathbb{R}^{n \\times n} via its matrix-vector product operator x \\mapsto Ax, estimate its trace \\mathrm{tr}(A) = \\sum\\limits_{i=1}^n A_{ii}\n\nObserve that although method (3) fits squarely in this category, its pretty inefficient from a computational standpoint—after all, most of the entries of e_i are zero, thus most inner product computations do not contribute to the trace estimate at all.\nOne idea, attributed to A. Girard, is to replace e_i with random zero-mean vectors, e.g. random sign vectors v \\in \\{-1, +1\\}^{n} or random normal vectors v \\sim \\mathcal{N}(\\mu=0, \\sigma=1):\n\\mathtt{tr}(A) \\approx \\frac{1}{n_v}\\sum\\limits_{i=1}^{n_v} v_i^\\top A v_i \nIt was shown more formally later by M.F. Huchinson that if these random vectors v \\in \\mathbb{R}^n satisfy \\mathbb{E}[vv^T] = I then this approach indeed forms an unbiased estimator of \\mathrm{tr}(A). To see this, its sufficient to combine the linearity of expectation, the cyclic-property of the trace function, and the aforementioned isotropy conditions: \\mathtt{tr}(A) = \\mathtt{tr}(A I) = \\mathtt{tr}(A \\mathbb{E}[v v^T]) = \\mathbb{E}[\\mathtt{tr}(Avv^T)] = \\mathbb{E}[\\mathtt{tr}(v^T A v)] = \\mathbb{E}[v^T A v]\nNaturally we expect the approximation to gradually improve as more vectors are sampled, converging as n_v \\to \\infty. Let’s see if we can check this: we start by collecting a few sample quadratic forms\n\ndef isotropic(n):\n  yield from [np.random.choice([-1,+1], size=A.shape[1]) for i in range(n)]\nestimates = np.array([v @ A @ v for v in isotropic(500)])\n\nPlotting the estimator formed by averaging the first k samples along with it’s error, we get the following figures for k = 50 and k = 500, respectively:\n\n\n/var/folders/0l/b3dbb2_d2bb4y3wbbfk0wt_80000gn/T/ipykernel_34141/588126951.py:32: UserWarning:\n\n\nYou are attempting to set `plot.legend.label_text_font_size` on a plot that has zero legends added, this will have no effect.\n\nBefore legend properties can be set, you must add a Legend explicitly, or call a glyph method with a legend parameter set.\n\n\n/var/folders/0l/b3dbb2_d2bb4y3wbbfk0wt_80000gn/T/ipykernel_34141/588126951.py:33: UserWarning:\n\n\nYou are attempting to set `plot.legend.label_height` on a plot that has zero legends added, this will have no effect.\n\nBefore legend properties can be set, you must add a Legend explicitly, or call a glyph method with a legend parameter set.\n\n\n/var/folders/0l/b3dbb2_d2bb4y3wbbfk0wt_80000gn/T/ipykernel_34141/588126951.py:34: UserWarning:\n\n\nYou are attempting to set `plot.legend.glyph_height` on a plot that has zero legends added, this will have no effect.\n\nBefore legend properties can be set, you must add a Legend explicitly, or call a glyph method with a legend parameter set.\n\n\n/var/folders/0l/b3dbb2_d2bb4y3wbbfk0wt_80000gn/T/ipykernel_34141/588126951.py:35: UserWarning:\n\n\nYou are attempting to set `plot.legend.spacing` on a plot that has zero legends added, this will have no effect.\n\nBefore legend properties can be set, you must add a Legend explicitly, or call a glyph method with a legend parameter set.\n\n\n/var/folders/0l/b3dbb2_d2bb4y3wbbfk0wt_80000gn/T/ipykernel_34141/588126951.py:37: UserWarning:\n\n\nYou are attempting to set `plot.legend.orientation` on a plot that has zero legends added, this will have no effect.\n\nBefore legend properties can be set, you must add a Legend explicitly, or call a glyph method with a legend parameter set.\n\n\n/var/folders/0l/b3dbb2_d2bb4y3wbbfk0wt_80000gn/T/ipykernel_34141/588126951.py:38: UserWarning:\n\n\nYou are attempting to set `plot.legend.background_fill_alpha` on a plot that has zero legends added, this will have no effect.\n\nBefore legend properties can be set, you must add a Legend explicitly, or call a glyph method with a legend parameter set.\n\n\n\n\n\n  \n\n\n\n\n\nSure enough, the estimator quickly gets within an error rate of about 1-5% away from the actual trace, getting generally closer as more samples are collected. On the other hand, improvement is not gaurenteed, and securing additional precision beyond &lt; 1% becomes increasingly difficult due to the variance of the sample estimates.",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "theory/intro.html#the-statistical-side",
    "href": "theory/intro.html#the-statistical-side",
    "title": "Introduction to trace estimation with primate",
    "section": "The statistical side",
    "text": "The statistical side\nThis forms an unbiased estimator of \\mathrm{tr}(A); to see this, its sufficient to combine the linearity of expectation, the cyclic-property of the trace function, and the aforementioned isotropy conditions \\mathtt{tr}(A) = \\mathtt{tr}(A I) = \\mathtt{tr}(A \\mathbb{E}[v v^T]) = \\mathbb{E}[\\mathtt{tr}(Avv^T)] = \\mathbb{E}[\\mathtt{tr}(v^T A v)] = \\mathbb{E}[v^T A v]",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "theory/intro.html#ex-logdet-computation",
    "href": "theory/intro.html#ex-logdet-computation",
    "title": "Introduction to trace estimation with primate",
    "section": "Ex: Logdet computation",
    "text": "Ex: Logdet computation\nThe basic way to approximate the trace of a matrix function is to simply set fun to the name the spectral function. For example, to approximate the log-determinant:\n\nfrom primate.trace import hutch\n\n## Log-determinant\nlogdet_test = hutch(A, fun=\"log\", maxiter=25)\nlogdet_true = np.sum(np.log(np.linalg.eigvalsh(A)))\n\nprint(f\"Logdet (exact):  {logdet_true}\")\nprint(f\"Logdet (approx): {logdet_test}\")\n\nLogdet (exact):  -148.3218440234385\nLogdet (approx): -150.71530622226584\n\n\nEven using n / 6 matvecs, we get a decent approximation. But how good is this estimate?\nTo get a slightly better idea, you can set verbose=True:\n\nest = hutch(A, fun=\"log\", maxiter=25, verbose=True)\n\nGirard-Hutchinson estimator (fun=log, deg=20, quad=fttr)\nEst: -145.469 +/- 7.37 (95% CI), CV: 3%, Evals: 25 [R]\n\n\nThe first line of the statement contains fixed about the estimator, including the type of estimator (Girard-Hutchinson), the function applied to the spectrum (log), the degree of the Krylov expansion (20), and the quadrature method used (fttr).\nThe second line prints the runtime information about the samples, such as the final trace estimate (``) 2. Its margin of error 2. The coefficient of variation (aka the relative std. deviation) 3. The number of samples used + their distribution prefix (‘R’ for rademacher)\n\nest = hutch(A, fun=\"log\", maxiter=100, plot=True)",
    "crumbs": [
      "Theory",
      "Introduction"
    ]
  },
  {
    "objectID": "reference/primate.operator.matrix_function.html",
    "href": "reference/primate.operator.matrix_function.html",
    "title": "matrix_function",
    "section": "",
    "text": "operator.matrix_function(A, fun='identity', deg=20, rtol=None, orth=0, **kwargs)\nConstructs an operator approximating the action v \\mapsto f(A)v\nThis function constructs an operator that uses the Lanczos quadrature method to approximate the action of matrix-vector multiplication with the matrix function f(A) for any choice of fun.\n\n\nA : ndarray, sparray, or LinearOperator real symmetric operator. fun : str or Callable, default=“identity” real-valued function defined on the spectrum of A. deg : int, default=20 Degree of the Krylov expansion. rtol : float, default=1e-8 Relative tolerance to consider two Lanczos vectors are numerically orthogonal. orth: int, default=0 Number of additional Lanczos vectors to orthogonalize against when building the Krylov basis. kwargs : dict, optional additional key-values to parameterize the chosen function ‘fun’.\n\n\n\noperator : LinearOperator Operator approximating the action of fun on the spectrum of A\n\n\n\nThe matrix-function approximation is implemented via Lanczos quadrature, which combines the Lanczos method with either: 1. The Golub-Welsch algorithm (GW), or 2. The Forward Three Term Recurrence algorithm (FTTR) for computing the deg-point Gaussian quadrature rule of a symmetric tridiagonal / Jacobi matrix T.\nThe GW computation uses implicit symmetric QR steps with Wilkinson shift to compute the full eigen-decomposition of T, while the FTTR algorithm uses the explicit expression for orthogonal polynomials to compute only the weights of the quadrature. The former uses O(\\mathrm{deg}^2) time and space and is highly accurate, while the latter uses O(\\mathrm{deg}^2) time and O(1) space at the cost of some accuracy. If deg is large, the fttr method should be preferred."
  },
  {
    "objectID": "reference/primate.operator.matrix_function.html#parameters",
    "href": "reference/primate.operator.matrix_function.html#parameters",
    "title": "matrix_function",
    "section": "",
    "text": "A : ndarray, sparray, or LinearOperator real symmetric operator. fun : str or Callable, default=“identity” real-valued function defined on the spectrum of A. deg : int, default=20 Degree of the Krylov expansion. rtol : float, default=1e-8 Relative tolerance to consider two Lanczos vectors are numerically orthogonal. orth: int, default=0 Number of additional Lanczos vectors to orthogonalize against when building the Krylov basis. kwargs : dict, optional additional key-values to parameterize the chosen function ‘fun’."
  },
  {
    "objectID": "reference/primate.operator.matrix_function.html#returns",
    "href": "reference/primate.operator.matrix_function.html#returns",
    "title": "matrix_function",
    "section": "",
    "text": "operator : LinearOperator Operator approximating the action of fun on the spectrum of A"
  },
  {
    "objectID": "reference/primate.operator.matrix_function.html#notes",
    "href": "reference/primate.operator.matrix_function.html#notes",
    "title": "matrix_function",
    "section": "",
    "text": "The matrix-function approximation is implemented via Lanczos quadrature, which combines the Lanczos method with either: 1. The Golub-Welsch algorithm (GW), or 2. The Forward Three Term Recurrence algorithm (FTTR) for computing the deg-point Gaussian quadrature rule of a symmetric tridiagonal / Jacobi matrix T.\nThe GW computation uses implicit symmetric QR steps with Wilkinson shift to compute the full eigen-decomposition of T, while the FTTR algorithm uses the explicit expression for orthogonal polynomials to compute only the weights of the quadrature. The former uses O(\\mathrm{deg}^2) time and space and is highly accurate, while the latter uses O(\\mathrm{deg}^2) time and O(1) space at the cost of some accuracy. If deg is large, the fttr method should be preferred."
  },
  {
    "objectID": "reference/operator.html",
    "href": "reference/operator.html",
    "title": "operator",
    "section": "",
    "text": "operator\n\n\n\n\n\nName\nDescription\n\n\n\n\nmatrix_function\nConstructs an operator approximating the action v \\mapsto f(A)v"
  },
  {
    "objectID": "reference/operator.html#functions",
    "href": "reference/operator.html#functions",
    "title": "operator",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nmatrix_function\nConstructs an operator approximating the action v \\mapsto f(A)v"
  },
  {
    "objectID": "reference/primate.trace.xtrace.html",
    "href": "reference/primate.trace.xtrace.html",
    "title": "xtrace",
    "section": "",
    "text": "xtrace\ntrace.xtrace(A, nv='auto', pdf='sphere', atol=0.1, rtol=1e-06, cond_tol=100000000.0, verbose=0, info=False)"
  },
  {
    "objectID": "reference/index.html#operators",
    "href": "reference/index.html#operators",
    "title": "Function reference",
    "section": "",
    "text": "Matrix operators\n\n\n\noperator.matrix_function\nConstructs a LinearOperator approximating the action of the matrix function fun(A).",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "reference/random.normal.html#returns",
    "href": "reference/random.normal.html#returns",
    "title": "random.normal",
    "section": "",
    "text": "np.narray Randomly generated matrix of shape size with entries in { -1, 1 }.",
    "crumbs": [
      "API Reference",
      "Random",
      "Normal"
    ]
  },
  {
    "objectID": "reference/random.rademacher.html#returns",
    "href": "reference/random.rademacher.html#returns",
    "title": "random.rademacher",
    "section": "",
    "text": "np.narray Randomly generated matrix of shape size with entries in { -1, 1 }.",
    "crumbs": [
      "API Reference",
      "Random",
      "Rademacher"
    ]
  },
  {
    "objectID": "reference/operator.matrix_function.html",
    "href": "reference/operator.matrix_function.html",
    "title": "operator.matrix_function",
    "section": "",
    "text": "operator.matrix_function(A, fun='identity', deg=20, rtol=None, orth=0, **kwargs)\nConstructs a LinearOperator approximating the action of the matrix function fun(A).\nThis function uses the Lanczos quadrature method to approximate the action of matrix function:  v \\mapsto f(A)v, \\quad f(A) = U f(\\lambda) U^T  The resulting operator may be used in conjunction with other trace estimation methods, such as the hutch or xtrace estimators.\nThe weights of the quadrature may be computed using either the Golub-Welsch (GW) or Forward Three Term Recurrence algorithms (FTTR) (see the quad parameter). For a description of the other parameters, see the Lanczos function.\n\n\n\n\n\n\nNote\n\n\n\nTo compute the weights of the quadrature, the GW computation uses implicit symmetric QR steps with Wilkinson shifts, while the FTTR algorithm uses the explicit expression for orthogonal polynomials. While both require O(\\mathrm{deg}^2) time to execute, the former requires O(\\mathrm{deg}^2) space but is highly accurate, while the latter uses only O(1) space at the cost of stability. If deg is large and accuracy is not a priority, the fttr method is preferred.\n\n\n\n\nA : ndarray, sparray, or LinearOperator real symmetric operator. fun : str or Callable, default=“identity” real-valued function defined on the spectrum of A. deg : int, default=20 Degree of the Krylov expansion. rtol : float, default=1e-8 Relative tolerance to consider two Lanczos vectors are numerically orthogonal. orth: int, default=0 Number of additional Lanczos vectors to orthogonalize against when building the Krylov basis. kwargs : dict, optional additional key-values to parameterize the chosen function ‘fun’.\n\n\n\noperator : LinearOperator Operator approximating the action of fun on the spectrum of A",
    "crumbs": [
      "API Reference",
      "Matrix Function"
    ]
  },
  {
    "objectID": "reference/operator.matrix_function.html#parameters",
    "href": "reference/operator.matrix_function.html#parameters",
    "title": "operator.matrix_function",
    "section": "",
    "text": "A : ndarray, sparray, or LinearOperator real symmetric operator. fun : str or Callable, default=“identity” real-valued function defined on the spectrum of A. deg : int, default=20 Degree of the Krylov expansion. rtol : float, default=1e-8 Relative tolerance to consider two Lanczos vectors are numerically orthogonal. orth: int, default=0 Number of additional Lanczos vectors to orthogonalize against when building the Krylov basis. kwargs : dict, optional additional key-values to parameterize the chosen function ‘fun’.",
    "crumbs": [
      "API Reference",
      "Matrix Function"
    ]
  },
  {
    "objectID": "reference/operator.matrix_function.html#returns",
    "href": "reference/operator.matrix_function.html#returns",
    "title": "operator.matrix_function",
    "section": "",
    "text": "operator : LinearOperator Operator approximating the action of fun on the spectrum of A",
    "crumbs": [
      "API Reference",
      "Matrix Function"
    ]
  },
  {
    "objectID": "reference/operator.matrix_function.html#notes",
    "href": "reference/operator.matrix_function.html#notes",
    "title": "operator.matrix_function",
    "section": "",
    "text": "The matrix-function approximation is implemented via Lanczos quadrature, which combines the Lanczos method with either: 1. The Golub-Welsch algorithm (GW), or 2. The Forward Three Term Recurrence algorithm (FTTR) for computing the deg-point Gaussian quadrature rule of a symmetric tridiagonal / Jacobi matrix T.\nThe GW computation uses implicit symmetric QR steps with Wilkinson shift to compute the full eigen-decomposition of T, while the FTTR algorithm uses the explicit expression for orthogonal polynomials to compute only the weights of the quadrature. The former uses O(\\mathrm{deg}^2) time and space and is highly accurate, while the latter uses O(\\mathrm{deg}^2) time and O(1) space at the cost of some accuracy. If deg is large, the fttr method should be preferred."
  },
  {
    "objectID": "reference/trace.xtrace.html",
    "href": "reference/trace.xtrace.html",
    "title": "trace.xtrace",
    "section": "",
    "text": "trace.xtrace\ntrace.xtrace(A, nv='auto', pdf='sphere', atol=0.1, rtol=1e-06, cond_tol=100000000.0, verbose=0, info=False)",
    "crumbs": [
      "API Reference",
      "XTrace"
    ]
  },
  {
    "objectID": "reference/trace.hutch.html",
    "href": "reference/trace.hutch.html",
    "title": "trace.hutch",
    "section": "",
    "text": "trace.hutch(A, fun=None, maxiter=200, deg=20, atol=None, rtol=None, stop=['confidence', 'change'], ncv=2, orth=0, quad='fttr', confidence=0.95, pdf='rademacher', rng='pcg64', seed=-1, num_threads=0, verbose=False, info=False, plot=False, **kwargs)\nEstimates the trace of a symmetric A or matrix function f(A) via a Girard-Hutchinson estimator.\nThis function uses up to maxiter random isotropic vectors to estimate of the trace of f(A), where: \\mathrm{tr}(f(A)) = \\mathrm{tr}(U f(\\Lambda) U^T) = \\sum\\limits_{i=1}^n f(\\lambda_i)  The estimator is obtained by averaging quadratic forms v \\mapsto v^T f(A)v, rescaling as necessary. This estimator may be used to quickly approximate of a variety of quantities, such as the trace inverse, the log-determinant, the numerical rank, etc. See the online documentation for more details.\n\n\n\n\n\n\nNote\n\n\n\nConvergence behavior is controlled by the stop parameter: “confidence” uses the central limit theorem to generate confidence intervals on the fly, which may be used in conjunction with atol and rtol to upper-bound the error of the approximation. Alternatively, when stop = “change”, the estimator is considered converged when the error between the last two iterates is less than atol (or rtol, respectively), similar to the behavior of scipy.integrate.quadrature.\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nA\nndarray, sparray, or LinearOperator\nreal symmetric operator.\nrequired\n\n\nfun\nstr or typing.Callable\nreal-valued function defined on the spectrum of A.\n\"identity\"\n\n\nmaxiter\nint\nMaximum number of random vectors to sample for the trace estimate.\n10\n\n\ndeg\nint\nDegree of the quadrature approximation. Must be at least 1.\n20\n\n\natol\nfloat\nAbsolute tolerance to signal convergence for early-stopping. See notes.\nNone\n\n\nrtol\nfloat\nRelative tolerance to signal convergence for early-stopping. See notes.\n1e-2\n\n\nstop\nstr\nEarly-stopping criteria to test estimator convergence. See details.\n\"confidence\"\n\n\nncv\nint\nNumber of Lanczos vectors to allocate. Must be at least 2.\n2\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against. Must be less than ncv.\n0\n\n\nquad\nstr\nMethod used to obtain the weights of the Gaussian quadrature. See notes.\n'fttr'\n\n\nconfidence\nfloat\nConfidence level to consider estimator as converged. Only used when stop = “confidence”.\n0.95\n\n\npdf\n‘rademacher’, ‘normal’\nChoice of zero-centered distribution to sample random vectors from.\n'rademacher'\n\n\nrng\n‘splitmix64’, ’xoshiro256**‘, ’pcg64’, ‘mt64’\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint\nSeed to initialize the rng entropy source. Set seed &gt; -1 for reproducibility.\n-1\n\n\nnum_threads\nint\nNumber of threads to use to parallelize the computation. Set to &lt;= 0 to let OpenMP decide.\n0\n\n\nplot\nbool\nIf true, plots the samples of the trace estimate along with their convergence characteristics.\nFalse\n\n\ninfo\nbool\nIf True, returns a dictionary containing all relevant information about the computation.\nFalse\n\n\nkwargs\ndict\nadditional key-values to parameterize the chosen function ‘fun’.\n{}\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Union[float, tuple]\nEstimate the trace of f(A). If ‘info = True’, additional information about the computation is also returned.\n\n\n\n\n\n\nlanczos : the lanczos tridiagonalization algorithm.\n\n\n\n\nUbaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099.",
    "crumbs": [
      "API Reference",
      "Hutch"
    ]
  },
  {
    "objectID": "reference/trace.hutch.html#notes",
    "href": "reference/trace.hutch.html#notes",
    "title": "trace.hutch",
    "section": "",
    "text": "For matrix functions (when fun is not None), the Lanczos method is used to approximate the action of f(A), and this function yields estimates of the quantity: \\mathrm{tr}(f(A)) = \\mathrm{tr}(U f(\\Lambda) U^T) This allows for quick approximating of a variety of quantities, such as the trace inverse, the log-determinant, the numerical rank, etc. See the online documentation for more details.",
    "crumbs": [
      "API Reference",
      "Hutch"
    ]
  },
  {
    "objectID": "reference/trace.hutch.html#parameters",
    "href": "reference/trace.hutch.html#parameters",
    "title": "trace.hutch",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nA\nndarray, sparray, or LinearOperator\nreal symmetric operator.\nrequired\n\n\nfun\nstr or typing.Callable\nreal-valued function defined on the spectrum of A.\n\"identity\"\n\n\nmaxiter\nint\nMaximum number of random vectors to sample for the trace estimate.\n10\n\n\ndeg\nint\nDegree of the quadrature approximation. Must be at least 1.\n20\n\n\natol\nfloat\nAbsolute tolerance to signal convergence for early-stopping. See notes.\nNone\n\n\nrtol\nfloat\nRelative tolerance to signal convergence for early-stopping. See notes.\n1e-2\n\n\nstop\nstr\nEarly-stopping criteria to test estimator convergence. See details.\n\"confidence\"\n\n\nncv\nint\nNumber of Lanczos vectors to allocate. Must be at least 2.\n2\n\n\north\nint\nNumber of additional Lanczos vectors to orthogonalize against. Must be less than ncv.\n0\n\n\nquad\nstr\nMethod used to obtain the weights of the Gaussian quadrature. See notes.\n'fttr'\n\n\nconfidence\nfloat\nConfidence level to consider estimator as converged. Only used when stop = “confidence”.\n0.95\n\n\npdf\n‘rademacher’, ‘normal’\nChoice of zero-centered distribution to sample random vectors from.\n'rademacher'\n\n\nrng\n‘splitmix64’, ’xoshiro256**‘, ’pcg64’, ‘mt64’\nRandom number generator to use.\n'splitmix64'\n\n\nseed\nint\nSeed to initialize the rng entropy source. Set seed &gt; -1 for reproducibility.\n-1\n\n\nnum_threads\nint\nNumber of threads to use to parallelize the computation. Set to &lt;= 0 to let OpenMP decide.\n0\n\n\nplot\nbool\nIf true, plots the samples of the trace estimate along with their convergence characteristics.\nFalse\n\n\ninfo\nbool\nIf True, returns a dictionary containing all relevant information about the computation.\nFalse\n\n\nkwargs\ndict\nadditional key-values to parameterize the chosen function ‘fun’.\n{}",
    "crumbs": [
      "API Reference",
      "Hutch"
    ]
  },
  {
    "objectID": "reference/trace.hutch.html#returns",
    "href": "reference/trace.hutch.html#returns",
    "title": "trace.hutch",
    "section": "",
    "text": "Type\nDescription\n\n\n\n\ntyping.Union[float, tuple]\nEstimate the trace of f(A). If ‘info = True’, additional information about the computation is also returned.",
    "crumbs": [
      "API Reference",
      "Hutch"
    ]
  },
  {
    "objectID": "reference/trace.hutch.html#see-also",
    "href": "reference/trace.hutch.html#see-also",
    "title": "trace.hutch",
    "section": "",
    "text": "lanczos : the lanczos tridiagonalization algorithm.",
    "crumbs": [
      "API Reference",
      "Hutch"
    ]
  },
  {
    "objectID": "reference/trace.hutch.html#reference",
    "href": "reference/trace.hutch.html#reference",
    "title": "trace.hutch",
    "section": "",
    "text": "Ubaru, S., Chen, J., & Saad, Y. (2017). Fast estimation of tr(f(A)) via stochastic Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099.",
    "crumbs": [
      "API Reference",
      "Hutch"
    ]
  }
]